{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"BLIS \u2014 Blackbox Inference Simulator","text":"<p>A discrete-event simulator for LLM inference serving systems. BLIS models multi-instance clusters with configurable admission control, request routing, KV-cache dynamics (including tiered GPU+CPU offloading), scheduling policies, and token generation \u2014 all driven by trained performance coefficients or analytical roofline estimates.</p> <p>The simulator is CPU-only, deterministic, and designed for capacity planning, policy optimization research, and performance prediction across model/GPU/TP configurations without requiring real GPUs.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Build\ngit clone https://github.com/inference-sim/inference-sim.git\ncd inference-sim\ngo build -o simulation_worker main.go\n\n# Run with default model\n./simulation_worker run --model meta-llama/llama-3.1-8b-instruct\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Discrete-event simulation for prefill, decode, and request scheduling</li> <li>Deterministic execution \u2014 same seed produces byte-identical output across runs (INV-6)</li> <li>KV-cache modeling with prefix caching and tiered GPU+CPU offload</li> <li>Chunked prefill and preemption-aware batch formation</li> <li>Two latency estimation modes: blackbox (data-driven) and roofline (analytical)</li> <li>Multi-instance cluster simulation with shared-clock event loop</li> <li>Pluggable routing policies: round-robin, least-loaded, weighted-scoring, prefix-affinity</li> <li>Admission control, priority policies, and instance schedulers</li> <li>ServeGen-informed workload generation with multi-client traffic classes</li> <li>Decision tracing and counterfactual analysis with top-k regret computation</li> <li>Fitness evaluation with weighted multi-objective scoring</li> </ul>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>Request Arrival \u2192 Admission \u2192 Routing \u2192 WaitQueue \u2192 Batch Formation \u2192 Step Execution \u2192 Completion\n                                            \u2193              \u2193\n                                      KV Allocation   Latency Estimation\n</code></pre> <p>Admission and Routing apply in cluster mode (multi-instance). Single-instance mode skips directly to WaitQueue.</p> <p>For detailed architecture documentation, see Cluster Architecture and Core Engine.</p>"},{"location":"#documentation-guide","title":"Documentation Guide","text":"Section What You'll Find Design System architecture, core engine, concepts glossary, configuration reference Standards Antipattern rules (R1-R20), system invariants (INV-1-8), engineering principles Process PR workflow, design document process, macro planning, hypothesis experiments, convergence protocol Templates Design guidelines, macro/micro plan templates, hypothesis template Extension Recipes Step-by-step guides for adding policies, scorers, KV tiers, and more Contributing Engineering standards, development workflow, and getting started"},{"location":"#reading-order-for-newcomers","title":"Reading Order for Newcomers","text":"<ol> <li>Concepts &amp; Glossary \u2014 learn BLIS-specific terminology</li> <li>Core Engine \u2014 understand the DES architecture and single-instance simulation</li> <li>Cluster Architecture \u2014 understand multi-instance orchestration</li> <li>Configuration Reference \u2014 when running experiments</li> <li>Extension Recipes \u2014 when adding new policies or features</li> </ol>"},{"location":"#supported-models","title":"Supported Models","text":"<p>All models below have pre-trained alpha/beta coefficients in <code>defaults.yaml</code> for blackbox mode. Models with a HuggingFace <code>config.json</code> in <code>model_configs/</code> additionally support roofline mode.</p>"},{"location":"#dense-models","title":"Dense Models","text":"Model Sizes Meta LLaMA 3.1 8B Meta LLaMA 3.3 70B IBM Granite 3.1 8B CodeLlama 34B Microsoft Phi-4 14B Mistral Small (2501) 24B Mistral Small 3.1 (2503) 24B NVIDIA LLaMA 3.1 Nemotron 70B OpenAI GPT-OSS 20B, 120B Qwen 2.5 7B SmolLM3 3B"},{"location":"#moe-models","title":"MoE Models","text":"Model Architecture LLaMA 4 Maverick 17B, 128 experts LLaMA 4 Scout 17B, 16 experts Mixtral 8x7B"},{"location":"#quantized-variants","title":"Quantized Variants","text":"<p>Red Hat AI (<code>redhatai/</code>) provides FP8, W4A16, and W8A8 quantized variants for many of the above models, including LLaMA 3.1/3.3/4, Mistral Small 3.1, Phi-4, and Qwen 2.5. See <code>defaults.yaml</code> for the full list.</p>"},{"location":"#roofline-only-models","title":"Roofline-Only Models","text":"<p>Any model with a HuggingFace <code>config.json</code> can use roofline mode via <code>--model-config-folder</code>. Pre-packaged configs exist for additional architectures (Qwen 2.5 1.5B/3B, Qwen 3 14B, LLaMA 2 7B/70B) in <code>model_configs/</code>.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the Apache License, Version 2.0. See LICENSE for details.</p>"},{"location":"contributing/","title":"Contributing to BLIS","text":"<p>This guide covers the engineering standards that keep BLIS (Blackbox Inference Simulator) correct and maintainable.</p>"},{"location":"contributing/#quick-start","title":"Quick Start","text":"<pre><code># Build\ngo build -o simulation_worker main.go\n\n# Test\ngo test ./...\n\n# Install linter (one-time setup)\ngo install github.com/golangci/golangci-lint/v2/cmd/golangci-lint@v2.9.0\n\n# Lint\ngolangci-lint run ./...\n</code></pre> <p>All three must pass before submitting a PR. CI uses golangci-lint v2.9.0 (see <code>.github/workflows/ci.yml</code>).</p> <pre><code># Local docs preview (requires Python + mkdocs-material)\npip install mkdocs-material==9.7.3\nsed 's|](|](|g' CONTRIBUTING.md &gt; docs/contributing.md\nmkdocs serve\n</code></pre>"},{"location":"contributing/#your-first-contribution","title":"Your First Contribution","text":"<p>This walkthrough adds a trivial admission policy \u2014 the lightest extension type (~3 files). Follow it step-by-step to learn the patterns, then apply them to your own contribution.</p> <p>What we'll build: A <code>CountingAdmit</code> admission policy that admits the first N requests and rejects the rest. We'll use test-driven development, starting with a test for the feature we want to implement.</p>"},{"location":"contributing/#step-1-create-a-branch","title":"Step 1: Create a branch","text":"<pre><code>git checkout -b feature/counting-admit\n</code></pre>"},{"location":"contributing/#step-2-write-the-failing-test","title":"Step 2: Write the failing test","text":"<p>Add a test to <code>sim/admission_test.go</code>:</p> <pre><code>func TestCountingAdmit_RejectsAfterLimit(t *testing.T) {\n    // GIVEN a CountingAdmit policy with limit=2\n    policy := &amp;CountingAdmit{Limit: 2}\n    req := &amp;Request{ID: \"test\", InputTokens: make([]int, 3)}\n    state := &amp;RouterState{Clock: 0}\n\n    // WHEN 3 requests arrive\n    r1, _ := policy.Admit(req, state)\n    r2, _ := policy.Admit(req, state)\n    r3, reason := policy.Admit(req, state)\n\n    // THEN the first 2 are admitted and the 3rd is rejected\n    if !r1 {\n        t.Error(\"first request should be admitted\")\n    }\n    if !r2 {\n        t.Error(\"second request should be admitted\")\n    }\n    if r3 {\n        t.Errorf(\"third request should be rejected, got reason: %s\", reason)\n    }\n}\n</code></pre> <p>Run: <code>go test ./sim/... -run TestCountingAdmit -v</code> Expected: FAIL (type <code>CountingAdmit</code> does not exist yet)</p>"},{"location":"contributing/#step-3-implement-the-policy","title":"Step 3: Implement the policy","text":"<p>In <code>sim/admission.go</code>, add after the existing policies:</p> <pre><code>// CountingAdmit admits the first Limit requests, then rejects all subsequent ones.\ntype CountingAdmit struct {\n    Limit int\n    count int\n}\n\nfunc (c *CountingAdmit) Admit(_ *Request, _ *RouterState) (bool, string) {\n    c.count++\n    if c.count &lt;= c.Limit {\n        return true, \"\"\n    }\n    return false, \"counting-admit limit exceeded\"\n}\n</code></pre>"},{"location":"contributing/#step-4-register-in-the-factory","title":"Step 4: Register in the factory","text":"<p>Two files need changes:</p> <p>In <code>sim/bundle.go</code>, add <code>\"counting-admit\"</code> to the <code>validAdmissionPolicies</code> map:</p> <pre><code>validAdmissionPolicies = map[string]bool{\"\": true, \"always-admit\": true, \"token-bucket\": true, \"reject-all\": true, \"counting-admit\": true}\n</code></pre> <p>In <code>sim/admission.go</code>, add a case to the <code>NewAdmissionPolicy</code> factory switch:</p> <pre><code>case \"counting-admit\":\n    return &amp;CountingAdmit{Limit: 100} // hardcoded for tutorial simplicity\n</code></pre> <p>Note: In a real policy, you would wire the limit through the factory parameters (e.g., <code>Limit: int(capacity)</code>) or via <code>PolicyBundle</code> YAML config. Hardcoded defaults would fail code review \u2014 see how <code>token-bucket</code> uses <code>capacity</code> and <code>refillRate</code>.</p>"},{"location":"contributing/#step-5-verify-tests-pass","title":"Step 5: Verify tests pass","text":"<pre><code>go test ./sim/... -run TestCountingAdmit -v   # Your new test\ngo test ./...                                    # All tests still pass\ngolangci-lint run ./...                          # No lint issues\n</code></pre>"},{"location":"contributing/#step-6-commit-and-open-a-pr","title":"Step 6: Commit and open a PR","text":"<pre><code>git add sim/admission.go sim/admission_test.go sim/bundle.go\ngit commit -m \"feat(sim): add counting-admit admission policy\n\n- Admits first N requests, rejects the rest\n- Registered in factory with default limit=100\"\ngit push -u origin feature/counting-admit\ngh pr create --title \"feat: add counting-admit admission policy\" --body \"My first BLIS contribution!\"\n</code></pre> <p>That's it! You've added a complete, tested, registered policy. Real contributions follow the same pattern \u2014 just with more contracts and a formal implementation plan.</p> <p>Important: This example is for learning only. Do not submit this as a real PR \u2014 <code>CountingAdmit</code> is a toy policy with no practical use. For your actual first contribution, check open issues for tasks labeled <code>good first issue</code>.</p>"},{"location":"contributing/#contributing-with-claude-code","title":"Contributing with Claude Code","text":"<p>Canonical source: <code>docs/process/pr-workflow.md</code>. If this section diverges, pr-workflow.md is authoritative.</p> <p>BLIS development workflows are orchestrated through Claude Code skills \u2014 structured sequences that handle worktree creation, plan generation, multi-perspective review with convergence enforcement, and PR creation. Contributors with Claude Code get the full automated pipeline. Contributors without it follow the manual path below and still go through the same quality gates (maintainers run the automated reviews on submitted PRs).</p> <p>Prerequisites: Claude Code installed with project skills available (<code>convergence-review</code>, <code>hypothesis-experiment</code>) and general Claude Code skills (<code>writing-plans</code>, <code>executing-plans</code>, <code>commit-push-pr</code>). See <code>docs/process/pr-workflow.md</code> for the full skill table. Before your first contribution, read <code>docs/templates/design-guidelines.md</code> \u2014 it covers module architecture, extension types, and DES foundations.</p>"},{"location":"contributing/#choosing-your-journey","title":"Choosing Your Journey","text":"You want to... Journey Starts with Fix a bug or make a small change Bug Fix / Small Change A GitHub issue or observed bug Add a new policy, scorer, or extension New Policy or Extension An existing interface to implement Build a new feature or subsystem New Feature (Idea to PR) An idea or requirement Validate simulator behavior Hypothesis Experiment A behavioral prediction <p>For hypothesis experiments, see Running or Contributing Hypothesis Experiments below. With Claude Code, the <code>hypothesis-experiment</code> skill orchestrates the full Steps 0\u201310 workflow.</p>"},{"location":"contributing/#bug-fix-small-change","title":"Bug Fix / Small Change","text":"<p>The lightest path. For bug fixes, docs updates, and single-PR changes that don't introduce new module boundaries.</p> <ol> <li>Create worktree \u2014 <code>/superpowers:using-git-worktrees fix-&lt;name&gt;</code></li> <li>Write micro plan \u2014 <code>/superpowers:writing-plans</code> using <code>@docs/templates/micro-plan.md</code></li> <li>Review plan \u2014 <code>/pr-review-toolkit:review-pr</code> then <code>/convergence-review pr-plan &lt;plan-path&gt;</code></li> <li>Human approval \u2014 review contracts and tasks, approve to proceed</li> <li>Implement \u2014 <code>/superpowers:executing-plans @&lt;plan-path&gt;</code></li> <li>Review code \u2014 <code>/pr-review-toolkit:review-pr</code> then <code>/convergence-review pr-code</code></li> <li>Self-audit + commit \u2014 deliberate critical thinking, then <code>/commit-commands:commit-push-pr</code></li> </ol> <p>Full process: <code>docs/process/pr-workflow.md</code></p>"},{"location":"contributing/#new-policy-or-extension","title":"New Policy or Extension","text":"<p>For adding a routing policy, admission policy, scorer, scheduler, priority policy, or tier composition \u2014 anything behind an existing interface.</p> <ol> <li>Identify extension type \u2014 see Adding New Components below</li> <li>Create worktree \u2014 <code>/superpowers:using-git-worktrees &lt;extension-name&gt;</code></li> <li>Write micro plan \u2014 <code>/superpowers:writing-plans</code> using <code>@docs/templates/micro-plan.md</code> and <code>@docs/extension-recipes.md</code></li> <li>Follow steps 3\u20137 from Bug Fix (review \u2192 approve \u2192 implement \u2192 review \u2192 commit)</li> </ol> <p>No design doc needed for policy templates. For tier compositions, a design doc is recommended \u2014 see the extension type table in Adding New Components. Full process: <code>docs/process/pr-workflow.md</code></p>"},{"location":"contributing/#new-feature-idea-to-pr","title":"New Feature (Idea to PR)","text":"<p>The full pipeline for features that introduce new module boundaries, new interfaces, or span multiple PRs.</p> <p>Phase 1 \u2014 Idea to Design: 1. Explore approaches \u2014 discuss design options with Claude, settle on an approach 2. Write design doc \u2014 following <code>docs/templates/design-guidelines.md</code> 3. Review design \u2014 <code>/convergence-review design &lt;path&gt;</code> (8 perspectives) 4. Human approval \u2014 review design doc before planning begins</p> <p>Full process: <code>docs/process/design.md</code></p> <p>Phase 2 \u2014 Design to Macro Plan (skip if single-PR):</p> <ol> <li>Write macro plan \u2014 decompose into PRs following <code>docs/templates/macro-plan.md</code></li> <li>Review macro plan \u2014 <code>/convergence-review macro-plan &lt;path&gt;</code> (8 perspectives)</li> <li>Human approval \u2014 review PR decomposition and module contracts</li> </ol> <p>Full process: <code>docs/process/macro-plan.md</code></p> <p>Phase 3 \u2014 Plan to PR (repeat for each PR):</p> <ol> <li>Follow the Bug Fix journey (steps 1\u20137) using the macro plan section or design doc as the source document</li> </ol> <p>Each phase produces an artifact that feeds the next. Human approval gates between phases prevent wasted work.</p>"},{"location":"contributing/#without-claude-code","title":"Without Claude Code","text":"<p>If you are not using Claude Code, here is the simplified workflow:</p> <ol> <li>Branch \u2014 <code>git checkout -b feature/my-change</code></li> <li>Plan \u2014 write an implementation plan following <code>docs/templates/micro-plan.md</code>. Include behavioral contracts (GIVEN/WHEN/THEN) and a task breakdown. Post the plan as a PR draft or issue comment for review.</li> <li>Implement \u2014 follow TDD: write a failing test, implement the minimal code to pass it, run <code>go test ./...</code>, run <code>golangci-lint run ./...</code>, commit. Repeat for each contract.</li> <li>Self-review \u2014 check the Antipattern Checklist below. Run <code>go build ./... &amp;&amp; go test ./... &amp;&amp; golangci-lint run ./...</code> one final time.</li> <li>PR \u2014 push your branch and open a PR. Maintainers will run the automated review protocols (convergence-review with 10 perspectives).</li> </ol> <p>The automated review tools (convergence-review, pr-review-toolkit) are run by maintainers \u2014 you do not need Claude Code installed. Your PR will go through the same quality gates regardless of tooling.</p> <p>For design docs and macro plans: follow the same templates (<code>docs/templates/design-guidelines.md</code>, <code>docs/templates/macro-plan.md</code>) and submit for review. Maintainers will run convergence review.</p> <p>Full process: <code>docs/process/pr-workflow.md</code> (the same workflow applies regardless of tooling)</p>"},{"location":"contributing/#engineering-principles","title":"Engineering Principles","text":"<p>See <code>docs/standards/principles.md</code> for the full principles guide covering: separation of concerns, interface design, configuration design, canonical constructors, output channel separation, error handling boundaries, and BDD/TDD development.</p> <p>Key points for new contributors: - <code>sim/</code> is a library \u2014 never call <code>os.Exit</code> or <code>logrus.Fatalf</code>. Return errors. Only <code>cmd/</code> may terminate. - Write behavioral contracts (GIVEN/WHEN/THEN) before tests. Test observable behavior, not internal structure. - If your PR touches request lifecycle, KV cache, or metrics, add or extend invariant tests (see <code>docs/standards/invariants.md</code>).</p>"},{"location":"contributing/#antipattern-checklist","title":"Antipattern Checklist","text":"<p>20 rules, each tracing to a real bug. See <code>docs/standards/rules.md</code> for full details.</p> <p>Before submitting a PR, verify:</p> <ul> <li> R1: No silent <code>continue</code>/<code>return</code> dropping data</li> <li> R2: Map keys sorted before float accumulation or ordered output</li> <li> R3: Every new CLI flag validated (zero, negative, NaN, Inf)</li> <li> R4: All struct construction sites audited for new fields</li> <li> R5: Resource allocation loops handle mid-loop failure with rollback</li> <li> R6: No <code>logrus.Fatalf</code> or <code>os.Exit</code> in <code>sim/</code> packages</li> <li> R7: Invariant tests alongside any golden tests</li> <li> R8: No exported mutable maps</li> <li> R9: <code>*float64</code> for YAML fields where zero is valid</li> <li> R10: YAML strict parsing (<code>KnownFields(true)</code>)</li> <li> R11: Division by runtime-derived denominators guarded</li> <li> R12: Golden dataset regenerated if output changed</li> <li> R13: New interfaces work for 2+ implementations</li> <li> R14: No method spans multiple module responsibilities</li> <li> R15: Stale PR references resolved</li> <li> R16: Config params grouped by module</li> <li> R17: Routing scorer signals documented for freshness tier</li> <li> R18: CLI flag values not silently overwritten by defaults.yaml</li> <li> R19: Unbounded retry/requeue loops have circuit breakers</li> <li> R20: Detectors and analyzers handle degenerate inputs (empty, skewed, zero)</li> </ul>"},{"location":"contributing/#adding-new-components","title":"Adding New Components","text":"<p>BLIS has four extension types. Identify which type your change is, then follow the corresponding recipe. See <code>docs/templates/design-guidelines.md</code> Section 5 for full details.</p> Extension Type What It Is Design Doc Required? Example Policy Template New algorithm behind an existing interface No New routing algorithm Subsystem Module New module with its own interface and events Yes AutoScaler, P/D disaggregation Backend Swap Alternative implementation of internal module Yes (covers both phases) SGLang latency model Tier Composition Wrapper layering behavior on existing module Recommended NVMe KV tier"},{"location":"contributing/#adding-a-new-model-to-defaultsyaml","title":"Adding a New Model to defaults.yaml","text":"<p>When adding a new model configuration:</p> <ol> <li>Add an entry to the <code>defaults:</code> section with <code>GPU</code>, <code>tensor_parallelism</code>, and <code>vllm_version</code></li> <li>Add an <code>hf_repo</code> field mapping the BLIS model name (lowercase) to the case-sensitive HuggingFace repository path (e.g., <code>hf_repo: meta-llama/Llama-3.1-8B-Instruct</code>). This enables <code>--roofline</code> auto-fetch. Models without real HuggingFace repos (e.g., synthetic benchmarks) may omit <code>hf_repo</code> \u2014 document why with a YAML comment.</li> <li>If trained coefficients exist, add a corresponding entry to the <code>models:</code> list</li> </ol>"},{"location":"contributing/#policy-template-lightest-3-files","title":"Policy Template (lightest \u2014 ~3 files)","text":"<ol> <li>Implement the interface in the corresponding file (<code>sim/admission.go</code>, <code>sim/routing.go</code>, <code>sim/priority.go</code>, <code>sim/scheduler.go</code>)</li> <li>Register in <code>sim/bundle.go</code> (valid names map + <code>IsValid*</code> function)</li> <li>Add <code>case</code> to factory function</li> <li>Add behavioral tests (<code>TestMyPolicy_Scenario_Behavior</code>)</li> <li>Update CLAUDE.md and README</li> </ol>"},{"location":"contributing/#subsystem-module-heaviest-new-interface-integration","title":"Subsystem Module (heaviest \u2014 new interface + integration)","text":"<p>Requires a design doc defining the module contract (observes / controls / owns / invariants / events / extension friction). See design guidelines Section 5.3.</p> <ol> <li>Write design doc with module contract, event integration, state ownership, failure modes, default behavior</li> <li>Create implementation plan via <code>docs/templates/micro-plan.md</code></li> <li>Implement interface + default implementation + factory</li> <li>Integrate into cluster event pipeline</li> <li>Add CLI flags with full validation</li> <li>Add behavioral tests + invariant tests</li> <li>Update CLAUDE.md, README, and design guidelines module map if needed</li> </ol>"},{"location":"contributing/#backend-swap-two-phases-extract-interface-then-add-alternative","title":"Backend Swap (two phases \u2014 extract interface, then add alternative)","text":"<p>Phase A (refactoring): Extract interface from hardcoded logic, verify existing tests pass unchanged. Phase B (extension): Implement new backend behind extracted interface, add configuration to select between backends.</p> <p>See design guidelines Section 5.4 for the full two-phase recipe.</p>"},{"location":"contributing/#tier-composition-delegation-pattern-4-files","title":"Tier Composition (delegation pattern \u2014 ~4 files)","text":"<ol> <li>Implement the same interface as the inner module (Liskov substitution)</li> <li>Compose existing tiers using delegation pattern</li> <li>Update factory with validation</li> <li>Add CLI flags with validation (zero, negative, NaN/Inf guards)</li> <li>Aggregate metrics from all tiers</li> <li>Add conservation invariant tests</li> </ol>"},{"location":"contributing/#new-trace-record-type","title":"New Trace Record Type","text":"<ol> <li>Define record struct in <code>sim/trace/record.go</code> (pure data, no <code>sim/</code> dependency)</li> <li>Add slice field to <code>SimulationTrace</code></li> <li>Add recording method</li> <li>Hook into cluster event pipeline (<code>if cs.trace != nil</code>)</li> <li>Update <code>Summarize()</code> aggregation</li> <li>Add behavioral tests</li> </ol>"},{"location":"contributing/#running-or-contributing-hypothesis-experiments","title":"Running or Contributing Hypothesis Experiments","text":"<p>Canonical source: <code>docs/process/hypothesis.md</code>. If this section diverges, hypothesis.md is authoritative.</p> <p>BLIS uses hypothesis-driven experimentation to validate system behavior, surface bugs, and document design tradeoffs. Experiments are organized into 6 families (workload/arrival, scheduler invariants, performance-regime, structural model, robustness, cross-policy comparative).</p> <p>To run existing experiments: <pre><code>cd hypotheses/h13-determinism\n./run.sh\n</code></pre> See <code>hypotheses/README.md</code> for the full list and coverage gaps.</p> <p>To propose a new hypothesis: File a GitHub issue using the \"Hypothesis Proposal\" template. Include: the hypothesis sentence, family, diagnostic value, and rough experiment design.</p> <p>To implement and run a new experiment: Follow <code>docs/process/hypothesis.md</code> for the full process (Steps 0-10). Key phases: 1. Create worktree, classify hypothesis, design experiment 2. Design Review (5 perspectives) \u2192 convergence \u2192 human approval 3. Implement <code>run.sh</code> and <code>analyze.py</code> using shared harness (<code>hypotheses/lib/</code>) 4. Code Review (5 perspectives) \u2192 convergence 5. Run experiments, document FINDINGS.md 6. FINDINGS Review (10 perspectives) \u2192 convergence 7. Self-audit (6 dimensions), verification gate, commit and PR</p> <p>Review protocol: Three review gates at different lifecycle stages, each using the universal convergence protocol (zero CRITICAL + zero IMPORTANT from all reviewers). External contributors without AI review infrastructure should submit their artifacts via PR \u2014 maintainers will run the review protocols. Only standard-library Python packages are needed (json, math, re, sys, pathlib).</p> Document Purpose <code>hypotheses/README.md</code> Existing experiments, coverage gaps <code>docs/process/hypothesis.md</code> Full process (Steps 0-10, three review gates) <code>docs/process/convergence.md</code> Universal Convergence Protocol (used by all review gates) <code>docs/standards/experiments.md</code> Rigor requirements (families, types, VV&amp;UQ, RCV rules) <code>docs/templates/hypothesis.md</code> FINDINGS.md template"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Composition over inheritance</li> <li>Timestamp-based event ordering via min-heap</li> <li>Partitioned RNG per subsystem for deterministic isolation</li> <li>BDD-style test naming: <code>TestType_Scenario_Behavior</code></li> <li>Conventional commits: <code>feat(scope)</code>, <code>fix(scope)</code>, <code>refactor(scope)</code>, <code>test(scope)</code>, <code>docs(scope)</code></li> </ul>"},{"location":"contributing/#key-references","title":"Key References","text":"Document What It Covers When to Read <code>CLAUDE.md</code> Code architecture, file organization, CLI flags, compact rule/invariant tables Always \u2014 authoritative for current codebase state <code>docs/standards/rules.md</code> 20 antipattern rules with evidence, checks, enforcement When reviewing or writing code <code>docs/standards/invariants.md</code> 8 system invariants (INV-1 through INV-8) with verification strategies When touching request lifecycle, KV cache, or metrics <code>docs/standards/experiments.md</code> Experiment taxonomy, rigor requirements, findings classification When running hypothesis experiments <code>docs/process/pr-workflow.md</code> End-to-end PR lifecycle (worktree \u2192 plan \u2192 review \u2192 implement \u2192 audit \u2192 PR) Before starting any PR <code>docs/design/</code> System architecture, core engine, concepts glossary, configuration reference When learning how BLIS works before contributing <code>docs/templates/design-guidelines.md</code> DES foundations, module architecture, extension framework Before designing a new feature or extending BLIS <code>docs/templates/micro-plan.md</code> Template for single-PR implementation plans When creating any PR implementation plan <code>docs/templates/macro-plan.md</code> Template for multi-PR feature expansions When planning a large feature with multiple PRs"},{"location":"extension-recipes/","title":"BLIS Extension Recipes","text":"<p>Step-by-step guides for extending BLIS. Each recipe lists the exact files to touch, the order, and examples to follow.</p>"},{"location":"extension-recipes/#adding-new-policy-templates","title":"Adding New Policy Templates","text":"<p>To add a new policy template (e.g., a new routing algorithm):</p> <ol> <li>Implement the interface in the corresponding file:</li> <li><code>AdmissionPolicy</code> \u2192 <code>sim/admission.go</code> (cluster-level: receives <code>*RouterState</code> with snapshots + clock)</li> <li><code>RoutingPolicy</code> \u2192 <code>sim/routing.go</code> (cluster-level: receives <code>*RouterState</code> with snapshots + clock)</li> <li><code>PriorityPolicy</code> \u2192 <code>sim/priority.go</code> (instance-level: receives <code>req</code> + <code>clock</code> only)</li> <li><code>InstanceScheduler</code> \u2192 <code>sim/scheduler.go</code> (instance-level: receives <code>requests</code> + <code>clock</code> only)</li> <li> <p>Note: <code>RouterState</code> is a bridge type in <code>sim/</code> to avoid import cycles \u2014 see <code>sim/router_state.go</code></p> </li> <li> <p>Register in two places (both required):</p> </li> <li>Add policy name to valid names map in <code>sim/bundle.go</code> (e.g., <code>validRoutingPolicies</code>) and corresponding <code>IsValid*</code> function</li> <li>Add <code>case</code> to factory function in the same policy file (e.g., <code>NewRoutingPolicy</code> in <code>sim/routing.go</code>)</li> <li> <p>CLI error messages auto-derive from <code>ValidAdmissionPolicyNames()</code> etc. \u2014 no manual update needed</p> </li> <li> <p>Add tests following BDD naming: <code>TestMyPolicy_Scenario_Behavior</code></p> </li> <li>Test observable behavior, not internal structure</li> <li>Include empty-snapshots panic test for routing policies (defensive programming convention)</li> <li> <p>Use <code>&amp;RouterState{Snapshots: snapshots, Clock: clock}</code> in test setup</p> </li> <li> <p>Update documentation: CLAUDE.md file organization, README policy lists</p> </li> </ol> <p>Important: For load-based routing, use <code>snap.EffectiveLoad()</code> \u2014 never compute <code>QueueDepth + BatchSize + PendingRequests</code> inline. This ensures all routing policies use the same formula.</p> <p>Examples: - See <code>RejectAll</code> in <code>sim/admission.go</code> for a simple admission template (constant return) - See <code>PrefixAffinity</code> in <code>sim/routing.go</code> for a stateful routing policy with LeastLoaded fallback. Known limitation (#259): hashes the full input sequence, not just the prefix \u2014 degrades to LeastLoaded for prefix-sharing workloads. PR18's prefix-affinity scorer with hierarchical block hashing addresses this.</p>"},{"location":"extension-recipes/#adding-new-scorers-weighted-routing","title":"Adding New Scorers (Weighted Routing)","text":"<p>To add a new scoring dimension for the <code>weighted</code> routing policy (e.g., predicted-latency):</p> <ol> <li>Implement the scorer function in <code>sim/routing_scorers.go</code> (stateless) or a new file (stateful) \u2014 a <code>scorerFunc</code> that takes <code>(*Request, []RoutingSnapshot)</code> and returns <code>map[string]float64</code> with scores in [0,1] per instance. Stateful scorers also return an <code>observerFunc</code> called after each routing decision.</li> <li>Register the scorer in <code>sim/routing_scorers.go</code>: add to <code>validScorerNames</code> map + <code>newScorerWithObserver</code> factory switch</li> <li>Add behavioral tests \u2014 monotonicity, boundary values, INV-1/INV-2 conformance</li> <li>Extension friction: 2 touch points (implementation + registration in <code>newScorerWithObserver</code>). Stateful scorers (like prefix-affinity) may use a separate file (e.g., <code>sim/routing_prefix_scorer.go</code>) but the registration point is the same <code>newScorerWithObserver</code> switch in <code>sim/routing_scorers.go</code>.</li> <li>Stateful scorers return an <code>observerFunc</code> alongside the <code>scorerFunc</code> from <code>newScorerWithObserver</code>. The <code>observerFunc</code> signature is <code>func(req *Request, targetInstance string)</code> and is called after each routing decision to update scorer state. The scorer and observer share state via closure.</li> </ol> <p>Examples: - See <code>scoreLoadBalance</code> in <code>sim/routing_scorers.go</code> for a simple stateless scorer - See <code>scoreQueueDepth</code> for a scorer with edge case handling (uniform load) - See <code>newPrefixAffinityScorer</code> in <code>sim/routing_prefix_scorer.go</code> for a stateful scorer with observer and router-side cache</p>"},{"location":"extension-recipes/#extending-kv-cache-tiers","title":"Extending KV Cache Tiers","text":"<p>To add a new KV tier (e.g., NVMe offloading for 3-tier GPU+CPU+NVMe):</p> <ol> <li>Implement the <code>KVStore</code> interface in <code>sim/kv/</code> (11 methods: allocate, get cached, release, capacity queries, metrics, <code>SetClock</code>, <code>ConsumePendingTransferLatency</code>)</li> <li>Compose existing tiers \u2014 e.g., wrap <code>TieredKVCache</code> (GPU+CPU) with NVMe logic, following the same delegation pattern</li> <li>Update <code>NewKVStore</code> factory in <code>sim/kv/register.go</code> to instantiate your tier based on <code>KVCacheConfig</code> fields (add new fields to <code>KVCacheConfig</code> in <code>sim/config.go</code>)</li> <li>Add CLI flags in <code>cmd/root.go</code> for new parameters (e.g., <code>--kv-nvme-blocks</code>) and wire them into the <code>KVCacheConfig</code> sub-config</li> <li>Aggregate metrics \u2014 combine hit/miss/thrashing counters from all tiers; see <code>TieredKVCache.CacheHitRate()</code> for the 2-tier pattern</li> <li>Add behavioral tests in <code>sim/kv/*_test.go</code></li> <li>Preserve rollback semantics \u2014 <code>KVCacheState.AllocateKVBlocks</code> is transactional: on mid-loop failure, <code>rollbackAllocation()</code> undoes all mutations (UsedBlockCnt, CacheMisses, CacheHits, RefCount, InUse, free list, HashToBlock, RequestMap). If your tier adds mutations beyond what delegation to <code>gpu.AllocateKVBlocks()</code> handles, you must roll those back too. See <code>cachedBlockMutation</code> and <code>newBlockMutation</code> types in <code>sim/kv/cache.go</code>.</li> <li><code>GetCachedBlocks</code> is a pure query \u2014 it returns cached block IDs without side effects. <code>CacheHits</code> are counted by <code>AllocateKVBlocks</code> when cached blocks are committed to an allocation (and rolled back on failure). This was fixed in the Phase 3 hardening PR; the previous implementation incremented CacheHits in GetCachedBlocks, causing double-counting in tiered mode.</li> </ol> <p>Examples: - See <code>TieredKVCache</code> in <code>sim/kv/tiered.go</code> for 2-tier GPU+CPU composition - See <code>KVCacheState</code> in <code>sim/kv/cache.go</code> for single-tier baseline (also implements <code>KVStore</code>) - See <code>docs/plans/archive/pr12-architectural-predesign.md</code> for the design decisions behind the tiered architecture</p>"},{"location":"extension-recipes/#adding-new-trace-record-types","title":"Adding New Trace Record Types","text":"<p>To add a new trace record type (e.g., <code>ScaleRecord</code> for autoscaling events):</p> <ol> <li>Define the record struct in <code>sim/trace/record.go</code> (pure data, no <code>sim/</code> dependency)</li> <li>Add a slice field to <code>SimulationTrace</code> in <code>sim/trace/trace.go</code> (e.g., <code>Scales []ScaleRecord</code>)</li> <li>Add a recording method to <code>SimulationTrace</code> (e.g., <code>RecordScale(ScaleRecord)</code>)</li> <li>Hook recording into the cluster event pipeline in <code>sim/cluster/cluster_event.go</code> (guard with <code>if cs.trace != nil</code> for zero-overhead default)</li> <li>Update <code>Summarize()</code> in <code>sim/trace/summary.go</code> to aggregate the new record type</li> <li>Add behavioral tests in <code>sim/trace/*_test.go</code></li> </ol> <p>Examples: - See <code>AdmissionRecord</code> in <code>sim/trace/record.go</code> for a simple record - See <code>RoutingRecord</code> with <code>CandidateScore</code> for a record with nested counterfactual data - See <code>computeCounterfactual()</code> in <code>sim/cluster/counterfactual.go</code> for derived computation that lives in <code>sim/cluster/</code> (not <code>sim/trace/</code>) because it needs <code>sim.RoutingSnapshot</code></p>"},{"location":"extension-recipes/#adding-new-latency-model-backends","title":"Adding New Latency Model Backends","text":"<p>To add a new latency estimation backend (e.g., SGLang RadixAttention, TensorRT-LLM, neural surrogate):</p> <ol> <li>Implement the <code>LatencyModel</code> interface in <code>sim/latency/latency.go</code> (or a new file in <code>sim/latency/</code> for complex models) \u2014 5 methods:</li> <li><code>StepTime(batch []*Request) int64</code> \u2014 estimate batch step duration from request states</li> <li><code>QueueingTime(req *Request) int64</code> \u2014 estimate arrival-to-queue delay</li> <li><code>OutputTokenProcessingTime() int64</code> \u2014 per-token post-processing overhead</li> <li><code>SchedulingProcessingTime() int64</code> \u2014 scheduling overhead per request</li> <li><code>PreemptionProcessingTime() int64</code> \u2014 preemption overhead per eviction</li> <li>Register in <code>NewLatencyModel</code> factory in <code>sim/latency/latency.go</code>: add a branch based on <code>ModelHardwareConfig</code> fields (e.g., a new string field or boolean in <code>sim/config.go</code>). The factory signature is <code>NewLatencyModel(LatencyCoeffs, ModelHardwareConfig)</code>.</li> <li>Add behavioral tests in <code>sim/latency/</code> \u2014 monotonicity (more tokens \u2192 longer step time), positive output, boundary cases (empty batch)</li> <li>Extension friction: 2 touch points (implementation + factory branch)</li> </ol> <p>Examples: - See <code>BlackboxLatencyModel</code> in <code>sim/latency/latency.go</code> for a simple stateless model (alpha/beta regression) - See <code>RooflineLatencyModel</code> in <code>sim/latency/latency.go</code> for a model that uses hardware config (FLOPs/bandwidth)</p>"},{"location":"extension-recipes/#adding-new-batch-formation-strategies","title":"Adding New Batch Formation Strategies","text":"<p>To add a new batch formation strategy (e.g., disaggregated prefill/decode, speculative decoding, continuous batching without preemption):</p> <ol> <li>Implement the <code>BatchFormation</code> interface in <code>sim/batch_formation.go</code> (or a new file for complex strategies) \u2014 1 method:</li> <li><code>FormBatch(ctx BatchContext) BatchResult</code> \u2014 compose the running batch for the next step</li> <li>The implementation receives <code>BatchContext</code> with: RunningBatch, WaitQ, KVCache, token budget, batch size limit, chunked prefill threshold, simulation time, step count, and ComputedTokens map</li> <li>The implementation MUST update <code>ctx.ComputedTokens[req.ID]</code> for each request that receives new tokens (Phase 2 of <code>Step()</code> reads this map to advance <code>ProgressIndex</code>)</li> <li>The implementation may mutate <code>WaitQ</code> (dequeue/prepend) and <code>KVCache</code> (allocate/release) during batch formation</li> <li>The implementation MUST NOT schedule events or record metrics \u2014 return decisions in <code>BatchResult</code>, the Simulator applies them</li> <li>Register in <code>NewBatchFormation</code> factory in <code>sim/batch_formation.go</code>: add a selection branch. The factory signature is <code>NewBatchFormation(LatencyModel)</code> \u2014 a future PR will add a strategy selection parameter (e.g., a string field in <code>PolicyConfig</code> or <code>BatchConfig</code>)</li> <li>Add behavioral tests \u2014 token budget enforcement, batch size limits, KV conservation, preemption behavior (if applicable), FCFS ordering</li> <li>Extension friction: 2 touch points (implementation + factory registration)</li> </ol> <p>Note: Currently only <code>VLLMBatchFormation</code> exists. Adding a second strategy will also require: (a) a <code>BatchFormation string</code> field in <code>PolicyConfig</code> or <code>BatchConfig</code> (in <code>sim/config.go</code>), (b) a CLI flag in <code>cmd/root.go</code>, (c) validation in <code>sim/bundle.go</code>, (d) selection logic in <code>NewBatchFormation</code>.</p> <p>Examples: - See <code>VLLMBatchFormation</code> in <code>sim/batch_formation.go</code> for the vLLM FCFS + chunked-prefill + preemption strategy - See <code>preemptForTokens</code> for the KV allocation + eviction loop pattern</p>"},{"location":"extension-recipes/#adding-new-per-request-metric-fields","title":"Adding New Per-Request Metric Fields","text":"<p>To add a new field to per-request JSON output (appears in <code>--results-path</code> output):</p> <ol> <li>Add field to <code>Request</code> in <code>sim/request.go</code> (runtime state, zero-value safe). When constructing <code>Request</code> structs, use <code>RequestState</code> typed constants (<code>StateQueued</code>, <code>StateRunning</code>, <code>StateCompleted</code>) \u2014 never bare strings.</li> <li>Add field to <code>RequestMetrics</code> in <code>sim/metrics_utils.go</code> (JSON output struct, use <code>omitempty</code> for backward compatibility)</li> <li>Update <code>NewRequestMetrics()</code> constructor in <code>sim/metrics_utils.go</code> to propagate the new field from <code>Request</code> to <code>RequestMetrics</code></li> <li>Set the field at the appropriate event (e.g., <code>RoutingDecisionEvent</code> for cluster-level, or completion for computed metrics)</li> <li>Add behavioral tests covering multi-instance, single-instance, and standalone boundaries</li> </ol> <p>Examples: - See <code>HandledBy</code> (#181) \u2014 set by <code>RoutingDecisionEvent</code>, zero-value when used outside cluster pipeline (suppressed from JSON via <code>omitempty</code>) - See <code>SLOClass</code>/<code>TenantID</code> (PR10) \u2014 set during workload generation, propagated at injection</p>"},{"location":"design/","title":"BLIS Design Documentation","text":"<p>This directory contains the public-facing design documentation for BLIS (Blackbox Inference Simulator). These pages explain BLIS's architecture, concepts, and configuration at the level needed to understand the system without reading source code.</p>"},{"location":"design/#design-pages","title":"Design Pages","text":""},{"location":"design/#architecture-internals","title":"Architecture &amp; Internals","text":"Page Description Cluster Architecture Multi-instance simulation: admission, routing, scorer composition, snapshot freshness, shared-clock event loop Core Engine Single-instance DES engine: event queue, Step() phases, request lifecycle, batch formation, KV cache, latency models Concepts &amp; Glossary Definitions of BLIS-specific terminology Configuration Reference All CLI flags, sub-config types, defaults.yaml behavior, workload modes Roofline Estimation Analytical GPU step time estimation without training data"},{"location":"design/#for-contributors","title":"For Contributors","text":"Page Description Extension Recipes Step-by-step guides for adding policies, scorers, KV tiers, trace records, and metrics Engineering Standards Antipattern rules (R1-R20), system invariants (INV-1 through INV-8), BDD/TDD principles Process Workflows PR workflow, design document process, hypothesis experiment protocol"},{"location":"design/#diagrams","title":"Diagrams","text":"Diagram Description End-to-end cluster pipeline: request arrival through metrics output Request state machine: states, transitions, and metric recording points DES event loop: min-heap queue, clock advancement, Step() decomposition Weighted scorer composition: per-scorer normalization, weight multiplication, argmax selection"},{"location":"design/#reading-order","title":"Reading Order","text":"<p>For newcomers to BLIS:</p> <ol> <li>Start with Concepts &amp; Glossary to learn BLIS-specific terminology</li> <li>Read Core Engine to understand the DES architecture and single-instance simulation</li> <li>Read Cluster Architecture to understand multi-instance orchestration</li> <li>Consult Configuration Reference when running experiments</li> <li>See Extension Recipes when adding new policies or features</li> </ol>"},{"location":"design/architecture/","title":"Cluster Architecture","text":"<p>This page describes how BLIS simulates multi-instance inference serving clusters. For single-instance simulation internals, see Core Engine.</p> <p>Canonical sources: Signal freshness (INV-7) is defined in <code>docs/standards/invariants.md</code>. If signal freshness descriptions here diverge, <code>invariants.md</code> is authoritative.</p>"},{"location":"design/architecture/#overview","title":"Overview","text":"<p>A BLIS cluster consists of N independent inference instances orchestrated by a shared-clock event loop. Each incoming request passes through a three-stage pipeline \u2014 admission, routing, and per-instance processing \u2014 before metrics are aggregated across all instances.</p> <pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  Requests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  Admission   \u2502\u2500\u2500rejected\u2500\u2500\u25b6 (counted)\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502 admitted\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Routing   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502 target instance selected\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u25bc            \u25bc            \u25bc\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502Instance 0\u2502 \u2502Instance 1\u2502 \u2502Instance 2\u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502            \u2502            \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Metrics   \u2502\u2500\u2500\u25b6 stdout (JSON)\n                    \u2502 Aggregation \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p></p>"},{"location":"design/architecture/#shared-clock-event-loop","title":"Shared-Clock Event Loop","text":"<p>The cluster simulator maintains a single global clock shared across all instances. At each iteration, it compares the earliest cluster-level event (request arrival, admission decision, routing decision) against the earliest per-instance event (step completion, queueing), and processes whichever is earlier.</p> <p>Ordering rules: - Cluster events at time T are processed before instance events at time T (cluster-first priority) - When multiple instances have events at the same time, the instance with the lowest index goes first - Within a single instance, events are ordered by timestamp only</p> <p>The simulation terminates when the clock exceeds the configured horizon or no events remain.</p>"},{"location":"design/architecture/#admission-pipeline","title":"Admission Pipeline","text":"<p>Admission is the first gate in the online routing pipeline. Every incoming request is evaluated by the admission policy before being passed to routing.</p>"},{"location":"design/architecture/#built-in-admission-policies","title":"Built-in Admission Policies","text":"Policy Behavior <code>always-admit</code> Accept all requests (default) <code>token-bucket</code> Rate-limiting via a token bucket with configurable capacity and refill rate <code>reject-all</code> Reject all requests (for pathological testing) <p>Token bucket rate-limits by consuming tokens proportional to input length: each request consumes tokens equal to its input token count, tokens refill at a constant rate, and requests are rejected when the bucket has insufficient tokens. Capacity and refill rate are configured via <code>--token-bucket-capacity</code> and <code>--token-bucket-refill-rate</code>.</p> <p>Rejected requests are counted in the output metrics but do not enter the routing pipeline. To add a new admission policy, see Extension Recipes. See Configuration Reference for flag details.</p>"},{"location":"design/architecture/#routing-pipeline","title":"Routing Pipeline","text":"<p>Routing selects which instance receives an admitted request. The routing policy sees a <code>RouterState</code> containing routing snapshots for all instances and the current simulation clock.</p>"},{"location":"design/architecture/#simple-routing-policies","title":"Simple Routing Policies","text":"Policy Selection Rule <code>round-robin</code> Cyclic instance assignment <code>least-loaded</code> Instance with minimum effective load <code>prefix-affinity</code> Deterministic prefix hash mapping with least-loaded fallback <code>always-busiest</code> Instance with maximum load (for pathological testing) <p>Effective load is defined as <code>QueueDepth + BatchSize + PendingRequests</code>, where <code>PendingRequests</code> counts requests that have been routed to an instance but not yet enqueued (the queueing event hasn't fired yet). This prevents routing pile-on at high arrival rates.</p>"},{"location":"design/architecture/#weighted-scoring-policy","title":"Weighted Scoring Policy","text":"<p>The <code>weighted</code> routing policy composes multiple scorers into a single routing decision. This is the recommended policy for production-like simulations and matches the architecture of real-world inference routers like llm-d's Endpoint Picker.</p> <p>The routing decision follows this pipeline:</p> <ol> <li>Score: Each scorer produces a per-instance score in [0, 1]</li> <li>Clamp: Scores are clamped to [0, 1] (scorers should already produce values in this range)</li> <li>Weight: Each score is multiplied by its configured weight</li> <li>Sum: Weighted scores are summed across scorers for each instance</li> <li>Select: The instance with the highest total score is chosen (argmax)</li> </ol> <p>Default weights: <code>prefix-affinity:3, queue-depth:2, kv-utilization:2</code> (llm-d parity). Note: weights are normalized to sum to 1.0 before scoring, so only weight ratios matter \u2014 <code>prefix-affinity:3,queue-depth:2</code> is identical to <code>prefix-affinity:30,queue-depth:20</code>. To add a new scorer, see Extension Recipes.</p> <p></p>"},{"location":"design/architecture/#scorer-composition","title":"Scorer Composition","text":"<p>Scorers are the building blocks of the weighted routing policy. Each scorer evaluates one signal dimension across all instances.</p>"},{"location":"design/architecture/#built-in-scorers","title":"Built-in Scorers","text":"Scorer Signal Score Computation Notes <code>prefix-affinity</code> Prefix cache overlap Proportion of request's block hashes found in instance's cache index Stateful: updates cache index after routing via observer <code>queue-depth</code> Instance load Min-max normalization of effective load (lower load = higher score) Stateless <code>kv-utilization</code> Memory pressure <code>1 - KVUtilization</code> (lower utilization = higher score) Stateless <code>load-balance</code> Instance load <code>1 / (1 + EffectiveLoad)</code> (decreasing function of load) Stateless"},{"location":"design/architecture/#stateful-vs-stateless-scorers","title":"Stateful vs. Stateless Scorers","text":"<p>Most scorers are stateless \u2014 they compute scores purely from the current routing snapshot. The <code>prefix-affinity</code> scorer is stateful: after a routing decision, an observer callback updates the router-side prefix cache index with the routed request's block hashes. This enables the scorer to track which prefixes are cached at which instance without querying the actual per-instance KV caches.</p>"},{"location":"design/architecture/#router-side-prefix-cache-index","title":"Router-Side Prefix Cache Index","text":"<p>The prefix-affinity scorer maintains a lightweight approximate cache of per-instance block hash history. This is separate from the actual per-instance KV cache and serves as a routing-time estimate of cache hit probability.</p> <p>Key properties: - Per-instance LRU with bounded capacity (default: 10,000 blocks) - Hierarchical block hashing: each block's hash chains with the prior block's hash for semantic prefix matching - Updated synchronously after each routing decision (Tier 1 freshness) - Score = proportion of request's block hashes found in the instance's cache index</p>"},{"location":"design/architecture/#signal-freshness","title":"Signal Freshness","text":"<p>Routing decisions depend on instance state signals with different freshness guarantees. Understanding freshness tiers is important for interpreting simulation results under high load.</p>"},{"location":"design/architecture/#freshness-tiers","title":"Freshness Tiers","text":"Tier Signals Update Mechanism Staleness Tier 1 (synchronous, always fresh) PendingRequests, prefix cache index Updated immediately after routing decision None Tier 2 (synchronous, point-in-time) QueueDepth, BatchSize Read from current instance state during routing Current within the tick Tier 3 (periodic, stale) KVUtilization, FreeKVBlocks, CacheHitRate Refreshed after configurable <code>SnapshotRefreshInterval</code> Potentially stale across multiple steps <p>The <code>--snapshot-refresh-interval</code> flag controls how frequently Tier 3 signals are re-read from instances. Setting it to 0 (default) makes all signals synchronous. Non-zero values introduce realistic staleness that affects routing quality under load.</p>"},{"location":"design/architecture/#counterfactual-regret","title":"Counterfactual Regret","text":"<p>When decision tracing is enabled (<code>--trace-level decisions</code>), BLIS computes counterfactual regret for each routing decision. This measures how much better an alternative routing choice could have been.</p> <p>Computation: 1. Score all candidate instances using the routing policy's scoring function 2. Rank candidates by score (descending) 3. Compute <code>regret = best_score - chosen_score</code> (clamped to &gt;= 0) 4. Record the top-k candidates with their scores and instance state</p> <p>Interpretation: - For the <code>weighted</code> policy, regret is typically zero because the chosen instance IS the highest-scored candidate - For <code>round-robin</code>, regret is non-zero because the policy ignores load signals - Higher regret does not necessarily imply worse performance \u2014 round-robin can achieve lower tail latency than least-loaded through perfect distribution uniformity</p> <p>Configure counterfactual analysis via <code>--counterfactual-k</code> (number of candidates to record per decision).</p>"},{"location":"design/architecture/#metrics-aggregation","title":"Metrics Aggregation","text":"<p>After simulation completes, per-instance metrics are aggregated into a unified cluster result:</p> Metric Category Aggregation TTFT, E2E Combined across all instances. JSON output: mean, p90, p95, p99. Internal <code>Distribution</code>: also p50, min, max (used by fitness evaluation). ITL: mean, p90, p95, p99 in JSON output. Throughput Total output tokens / simulation time; total requests / simulation time Request counts Sum of completed, queued, running, preempted, dropped across instances Per-SLO-class Separate distributions per SLO class (for multi-tenant analysis) Fairness Jain Fairness Index across tenant throughputs"},{"location":"design/architecture/#fitness-evaluation","title":"Fitness Evaluation","text":"<p>When <code>--fitness-weights</code> are configured, BLIS computes a single fitness score from the aggregated metrics. This enables automated policy comparison:</p> <ul> <li>Latency metrics (TTFT, E2E) are normalized via <code>1/(1 + value/1000)</code> where <code>value</code> is in ticks (microseconds) and 1000 ticks = 1ms is the reference point (lower latency = higher score). For example, TTFT of 50,000 ticks (50ms) maps to <code>1/(1+50) = 0.0196</code>.</li> <li>Throughput metrics are normalized via <code>value/(value + reference)</code> where <code>referenceRPS = 100.0</code> and <code>referenceTPS = 10000.0</code> (higher throughput = higher score)</li> <li>Normalized scores are multiplied by their configured weights and summed</li> <li>Higher fitness = better performance</li> </ul> <p>Note: the normalization compresses raw metric differences significantly. A 38% TTFT improvement might map to only an 8% fitness score difference. Always examine raw metrics alongside fitness scores.</p>"},{"location":"design/architecture/#instance-isolation","title":"Instance Isolation","text":"<p>Each instance in the cluster is a fully independent single-instance simulator with its own: - Event queue and simulation state - Wait queue and running batch - KV cache (block allocation, prefix caching, LRU eviction) - Latency model - Scheduling and priority policies</p> <p>Instances share the global clock but have no direct communication. All inter-instance coordination happens through the routing layer (via routing snapshots). This matches the architecture of real inference serving clusters where instances are independent processes.</p>"},{"location":"design/architecture/#online-routing-pipeline-walkthrough","title":"Online Routing Pipeline Walkthrough","text":"<p>A complete request lifecycle through the cluster pipeline:</p> <ol> <li>Generation: The workload generator creates a request with arrival time, input tokens, and output tokens</li> <li>ClusterArrivalEvent: Scheduled at the request's arrival time</li> <li>AdmissionDecisionEvent: Admission policy evaluates the request</li> <li>If rejected: request counted, pipeline ends</li> <li>If admitted: proceed to routing (with optional <code>--admission-latency</code> delay)</li> <li>RoutingDecisionEvent: Routing policy selects target instance</li> <li>PendingRequests for target instance incremented</li> <li>Request injected into target instance's wait queue (with optional <code>--routing-latency</code> delay)</li> <li>QueuedEvent: Fired by target instance when request enters its queue</li> <li>PendingRequests decremented (request is now fully absorbed)</li> <li>If no StepEvent exists, one is scheduled (work-conserving)</li> <li>Per-instance processing: Request follows the single-instance lifecycle (see Core Engine)</li> <li>Completion: Request metrics (TTFT, E2E, ITL) recorded at instance level, aggregated at cluster level</li> </ol>"},{"location":"design/concepts/","title":"Concepts &amp; Glossary","text":"<p>This page defines terminology used throughout BLIS documentation. Terms are listed alphabetically with cross-references to the relevant design pages.</p>"},{"location":"design/concepts/#admission-policy","title":"Admission Policy","text":"<p>A cluster-level gate that decides whether an incoming request enters the routing pipeline or is rejected. Built-in policies: <code>always-admit</code> (accept all), <code>token-bucket</code> (rate-limiting), <code>reject-all</code> (testing only). See Cluster Architecture.</p>"},{"location":"design/concepts/#alpha-coefficients","title":"Alpha Coefficients","text":"<p>Three regression coefficients <code>[alpha0, alpha1, alpha2]</code> that model non-GPU overhead per request. <code>alpha0 + alpha1 * input_length</code> estimates queueing delay (tokenization, API serialization); <code>alpha2</code> estimates output token processing time. These overheads are added to per-request metrics but do not block the simulation clock. See Core Engine: Latency Models.</p>"},{"location":"design/concepts/#batch-formation","title":"Batch Formation","text":"<p>The process of selecting which requests from the wait queue join the running batch for the next step. BLIS implements vLLM-style continuous batching with chunked prefill and preemption. See Core Engine: Batch Formation.</p>"},{"location":"design/concepts/#beta-coefficients","title":"Beta Coefficients","text":"<p>Three regression coefficients <code>[beta0, beta1, beta2]</code> that predict GPU step time: <code>beta0 + beta1 * cache_miss_tokens + beta2 * decode_tokens</code>. Trained offline via Bayesian optimization against real vLLM measurements. See Core Engine: Latency Models.</p>"},{"location":"design/concepts/#block-kv-block","title":"Block (KV Block)","text":"<p>The unit of KV cache allocation. Each block holds a fixed number of tokens (default: 16). Requests are allocated blocks proportional to their token count. Blocks are reference-counted and can be shared across requests via prefix caching. See Core Engine: KV Cache.</p>"},{"location":"design/concepts/#chunked-prefill","title":"Chunked Prefill","text":"<p>A vLLM optimization where long prefill sequences are split into chunks that fit within the per-step token budget (<code>max-num-scheduled-tokens</code>). Controlled by <code>--long-prefill-token-threshold</code>. See Core Engine: Batch Formation.</p>"},{"location":"design/concepts/#continuous-batching","title":"Continuous Batching","text":"<p>The serving strategy where new requests can join the running batch between decode steps, rather than waiting for the entire batch to complete. BLIS models this by re-evaluating the batch composition at every step. See Core Engine: Batch Formation.</p>"},{"location":"design/concepts/#counterfactual-regret","title":"Counterfactual Regret","text":"<p>A trace-level metric that measures how much better an alternative routing decision could have been. For each routing decision, BLIS scores all candidate instances and computes <code>regret = best_score - chosen_score</code>. Useful for offline analysis of routing policy quality. See Cluster Architecture: Counterfactual Regret.</p>"},{"location":"design/concepts/#decode-phase","title":"Decode Phase","text":"<p>The token generation phase where the model produces output tokens one at a time (or in parallel within a step). Each decode token uses the KV cache from all prior tokens. Decode steps are typically memory-bandwidth-bound. Contrast with Prefill Phase.</p>"},{"location":"design/concepts/#discrete-event-simulation-des","title":"Discrete Event Simulation (DES)","text":"<p>A simulation paradigm where the system state changes only at discrete event times. BLIS maintains a priority queue of timestamped events and advances the simulation clock by jumping between events, rather than stepping through fixed time intervals. See Core Engine: Event Queue.</p>"},{"location":"design/concepts/#e2e-end-to-end-latency","title":"E2E (End-to-End Latency)","text":"<p>Total time from request arrival to final token completion. Computed as <code>TTFT + sum(ITLs)</code>, where each ITL includes step time plus output processing overhead (alpha2). See Core Engine: Metrics.</p>"},{"location":"design/concepts/#effective-load","title":"Effective Load","text":"<p>A routing signal computed as <code>QueueDepth + BatchSize + PendingRequests</code>. Represents the total work visible at an instance, including requests that have been routed but not yet enqueued. Used by least-loaded routing and load-balance scoring. See Cluster Architecture: Routing Pipeline.</p>"},{"location":"design/concepts/#fitness-score","title":"Fitness Score","text":"<p>A single numeric value summarizing multi-objective simulation performance. Computed as a weighted combination of configurable metrics (TTFT percentiles, E2E percentiles, throughput). Latency metrics normalized via <code>1/(1 + value/1000)</code>; throughput metrics via <code>value/(value + reference)</code>. See Configuration Reference.</p>"},{"location":"design/concepts/#horizon","title":"Horizon","text":"<p>The simulation time limit in ticks (microseconds). The simulation stops when the clock exceeds the horizon or all requests complete, whichever comes first. See Configuration Reference.</p>"},{"location":"design/concepts/#itl-inter-token-latency","title":"ITL (Inter-Token Latency)","text":"<p>The observed time between consecutive decode steps for a single request. ITL varies with batch composition changes between steps. Mean ITL is reported as TPOT (Time Per Output Token).</p>"},{"location":"design/concepts/#kv-cache","title":"KV Cache","text":"<p>GPU memory organized as blocks that store key-value tensors computed during attention. BLIS simulates block allocation, prefix sharing, LRU eviction, and optional CPU offloading without actual GPU memory. See Core Engine: KV Cache.</p>"},{"location":"design/concepts/#latency-model","title":"Latency Model","text":"<p>The component that predicts GPU execution time for a batch step. Two modes: Blackbox (trained regression coefficients) and Roofline (analytical FLOPs/bandwidth estimation). See Core Engine: Latency Models and Roofline Estimation.</p>"},{"location":"design/concepts/#pending-requests","title":"Pending Requests","text":"<p>Requests that have been routed to an instance but not yet enqueued (the queueing event hasn't fired). Tracked per-instance to prevent routing pile-on at high arrival rates. Decremented when the <code>QueuedEvent</code> fires. See Cluster Architecture: Routing Pipeline.</p>"},{"location":"design/concepts/#policy-bundle","title":"Policy Bundle","text":"<p>A YAML configuration file (<code>--policy-config</code>) that specifies admission, routing, priority, and scheduling policies in one place. CLI flags override bundle values when explicitly set. See Configuration Reference.</p>"},{"location":"design/concepts/#preemption","title":"Preemption","text":"<p>KV cache eviction under memory pressure. When the batch formation algorithm cannot allocate blocks for a continuing request, it evicts requests from the batch tail, frees their blocks, and re-enqueues them at the front of the wait queue. See Core Engine: Batch Formation.</p>"},{"location":"design/concepts/#prefill-phase","title":"Prefill Phase","text":"<p>The initial processing phase where the model computes attention over all input tokens. Prefill is compute-bound for large inputs. After prefill completes, the request transitions to decode. TTFT is recorded at this boundary.</p>"},{"location":"design/concepts/#prefix-caching","title":"Prefix Caching","text":"<p>Reuse of KV blocks across requests that share a common input prefix. BLIS uses hierarchical block hashing: each block's hash chains with the prior block's hash, enabling semantic prefix matching. Shared blocks are reference-counted and exempt from eviction while in use. See Core Engine: KV Cache.</p>"},{"location":"design/concepts/#prefix-affinity-scoring","title":"Prefix-Affinity Scoring","text":"<p>A routing scorer that directs requests to instances likely to have their prefix cached. Uses a lightweight router-side cache index (not the actual per-instance KV cache) to estimate cache hit probability per instance. Score range [0, 1]. See Cluster Architecture: Scorer Composition.</p>"},{"location":"design/concepts/#priority-policy","title":"Priority Policy","text":"<p>A per-instance policy that assigns a numeric priority score to each request before batch formation. Used by priority-aware schedulers to reorder the wait queue. Built-in policies: <code>constant</code>, <code>slo-based</code>, <code>inverted-slo</code> (testing only). Note: despite its name, <code>slo-based</code> currently uses only request age (favoring older requests), not per-request SLO metadata. See Core Engine: Scheduling.</p>"},{"location":"design/concepts/#roofline-model","title":"Roofline Model","text":"<p>An analytical latency estimation technique that predicts step time as <code>max(FLOPs / peak_compute, bytes / peak_bandwidth)</code>. Requires only the model's HuggingFace <code>config.json</code> and GPU hardware specs. No training data needed. See Roofline Estimation.</p>"},{"location":"design/concepts/#routing-policy","title":"Routing Policy","text":"<p>A cluster-level policy that selects which instance receives an admitted request. Simple policies (round-robin, least-loaded) use fixed rules. The weighted scoring policy composes multiple scorers with configurable weights. See Cluster Architecture: Routing Pipeline.</p>"},{"location":"design/concepts/#routing-snapshot","title":"Routing Snapshot","text":"<p>A point-in-time view of instance state used for routing decisions. Contains queue depth, batch size, KV utilization, cache hit rate, and pending request count. Signals have different freshness tiers depending on how they're collected. See Cluster Architecture: Signal Freshness.</p>"},{"location":"design/concepts/#scorer","title":"Scorer","text":"<p>A component in the weighted scoring pipeline that produces a per-instance score in [0, 1] for a specific signal dimension. Built-in scorers: <code>prefix-affinity</code>, <code>queue-depth</code>, <code>kv-utilization</code>, <code>load-balance</code>. Scores are multiplied by weights and summed. See Cluster Architecture: Scorer Composition.</p>"},{"location":"design/concepts/#seed","title":"Seed","text":"<p>The random seed for deterministic simulation. Same seed produces byte-identical stdout across runs (INV-6). BLIS uses partitioned RNG to isolate randomness across subsystems. See Configuration Reference.</p>"},{"location":"design/concepts/#step","title":"Step","text":"<p>A single iteration of the inference engine. Each step processes one batch: prefill tokens for new/continuing requests and decode tokens for generating output. Step time is predicted by the latency model. See Core Engine: Step Phases.</p>"},{"location":"design/concepts/#tick","title":"Tick","text":"<p>The fundamental time unit in BLIS, representing one microsecond. All timestamps, durations, and latencies are measured in ticks. The simulation clock advances in ticks.</p>"},{"location":"design/concepts/#tiered-kv-cache","title":"Tiered KV Cache","text":"<p>An extension of the KV cache with GPU and CPU tiers. When GPU utilization exceeds a threshold, blocks are offloaded to CPU memory. On cache miss, blocks can be reloaded from CPU with a transfer latency penalty. See Core Engine: KV Cache.</p>"},{"location":"design/concepts/#ttft-time-to-first-token","title":"TTFT (Time To First Token)","text":"<p>Time from request arrival to completion of the prefill phase (first output token ready). Includes queueing delay, prefill step times, and output processing overhead (alpha2). A key latency SLO metric for interactive applications. See Core Engine: Metrics.</p>"},{"location":"design/concepts/#workload-specification","title":"Workload Specification","text":"<p>A YAML file (<code>--workload-spec</code>) defining multi-client workloads with per-client arrival distributions, token length distributions, prefix groups, and SLO classes. Supports <code>poisson</code>, <code>gamma</code>, and <code>weibull</code> arrival processes and <code>gaussian</code>, <code>exponential</code>, <code>pareto_lognormal</code>, <code>constant</code>, and <code>empirical</code> token distributions. See Configuration Reference.</p>"},{"location":"design/concepts/#work-conserving","title":"Work-Conserving","text":"<p>The property that the simulator never idles while requests are waiting. After every step completion, if the wait queue is non-empty, a new <code>StepEvent</code> is scheduled immediately (INV-8). See Core Engine: Event Queue.</p>"},{"location":"design/configuration/","title":"Configuration Reference","text":"<p>This page documents all CLI flags, configuration files, and their interactions. For architectural context on what these settings control, see Cluster Architecture and Core Engine.</p>"},{"location":"design/configuration/#configuration-precedence","title":"Configuration Precedence","text":"<p>BLIS uses a layered configuration system where more specific sources override more general ones:</p> <pre><code>CLI flags (highest priority \u2014 explicit user input)\n    \u2193 overrides\nYAML files (policy-config, workload-spec, defaults.yaml)\n    \u2193 overrides\nHardcoded defaults (lowest priority)\n</code></pre> <p>CLI flags only override YAML values when explicitly set. BLIS checks whether each flag was provided by the user (not just whether it has a non-default value), so default flag values do not accidentally override YAML configuration.</p>"},{"location":"design/configuration/#simulation-control","title":"Simulation Control","text":"<p>Top-level settings that control the simulation run.</p> Flag Type Default Description <code>--seed</code> int64 42 Random seed for deterministic simulation. Same seed produces byte-identical stdout. <code>--horizon</code> int64 MaxInt64 Simulation time limit in ticks (microseconds). Simulation stops when clock exceeds horizon or all requests complete. <code>--log</code> string \"warn\" Log verbosity: trace, debug, info, warn, error, fatal, panic. Logs go to stderr. <code>--results-path</code> string \"\" File path to save per-request results JSON. Empty = stdout only."},{"location":"design/configuration/#kv-cache-configuration","title":"KV Cache Configuration","text":"<p>Controls GPU and CPU memory simulation for key-value cache blocks. Maps to <code>KVCacheConfig</code>.</p> Flag Type Default Description <code>--total-kv-blocks</code> int64 1000000* Total GPU-tier KV blocks. <code>--block-size-in-tokens</code> int64 16 Tokens per KV block. <code>--kv-cpu-blocks</code> int64 0 CPU-tier blocks. 0 disables tiered caching. <code>--kv-offload-threshold</code> float64 0.9 GPU utilization fraction above which blocks are offloaded to CPU. Range [0, 1]. <code>--kv-transfer-bandwidth</code> float64 100.0 GPU-CPU transfer rate in blocks/tick. Required &gt; 0 when CPU blocks &gt; 0. <code>--kv-transfer-base-latency</code> int64 0 Fixed per-transfer latency in ticks. <p>* The CLI default is 1,000,000 but <code>defaults.yaml</code> overrides this per model when coefficients are loaded. For example, <code>llama-3.1-8b/H100/TP=2</code> uses 132,139 blocks. The override only applies if the user did not explicitly set <code>--total-kv-blocks</code>.</p>"},{"location":"design/configuration/#batch-formation","title":"Batch Formation","text":"<p>Controls how requests are selected for the running batch. Maps to <code>BatchConfig</code>.</p> Flag Type Default Description <code>--max-num-running-reqs</code> int64 256 Maximum requests in the running batch simultaneously. <code>--max-num-scheduled-tokens</code> int64 2048 Maximum total new tokens across all running requests per step (token budget). <code>--long-prefill-token-threshold</code> int64 0 Prefill length threshold for chunked prefill. 0 = disabled (all prefill in one step)."},{"location":"design/configuration/#latency-model","title":"Latency Model","text":""},{"location":"design/configuration/#regression-coefficients","title":"Regression Coefficients","text":"<p>Trained coefficients for the blackbox latency model. Maps to <code>LatencyCoeffs</code>.</p> Flag Type Default Description <code>--alpha-coeffs</code> float64 slice [0, 0, 0] Alpha coefficients [alpha0, alpha1, alpha2]. Models non-GPU overhead. <code>--beta-coeffs</code> float64 slice [0, 0, 0] Beta coefficients [beta0, beta1, beta2]. Models GPU step time. <p>When both alpha and beta coefficients are all zeros, BLIS automatically loads pre-trained coefficients from <code>defaults.yaml</code> based on the model, GPU, and TP configuration.</p>"},{"location":"design/configuration/#model-and-hardware-selection","title":"Model and Hardware Selection","text":"<p>Maps to <code>ModelHardwareConfig</code>.</p> Flag Type Default Description <code>--model</code> string (required) LLM model name (e.g., <code>meta-llama/llama-3.1-8b-instruct</code>). <code>--hardware</code> string \"\" GPU type (e.g., <code>H100</code>, <code>A100</code>). If empty, loaded from <code>defaults.yaml</code>. <code>--tp</code> int 0 Tensor parallelism degree. If 0, loaded from <code>defaults.yaml</code>. <code>--vllm-version</code> string \"\" vLLM version string. If empty, loaded from <code>defaults.yaml</code>."},{"location":"design/configuration/#roofline-mode","title":"Roofline Mode","text":"<p>For analytical step time estimation without trained coefficients.</p> Flag Type Default Description <code>--roofline</code> bool false Enable roofline mode with auto-fetch. Requires <code>--hardware</code> and <code>--tp</code>. Auto-resolves model config from <code>model_configs/</code> or HuggingFace, and hardware config from bundled <code>hardware_config.json</code>. Set <code>HF_TOKEN</code> env var for gated models. <code>--model-config-folder</code> string \"\" Path to folder containing HuggingFace <code>config.json</code>. Overrides <code>--roofline</code> auto-resolution. <code>--hardware-config</code> string \"\" Path to <code>hardware_config.json</code> with GPU specifications. Overrides <code>--roofline</code> auto-resolution. <p>See Roofline Estimation for details on the analytical model.</p>"},{"location":"design/configuration/#latency-mode-selection","title":"Latency Mode Selection","text":"<p>The latency model mode is selected based on available configuration:</p> <ol> <li>Blackbox mode (default): If coefficients are provided via CLI flags or loaded from <code>defaults.yaml</code></li> <li>Explicit roofline mode: If <code>--roofline</code> is set with <code>--hardware</code> and <code>--tp</code>. Model config is auto-resolved: <code>model_configs/</code> (local) \u2192 HuggingFace fetch \u2192 error. Alpha coefficients and <code>total_kv_blocks</code> are loaded from <code>defaults.yaml</code> when available. Beta coefficients are replaced by analytical roofline computation.</li> <li>Implicit roofline mode: If all coefficients are zero and all four of <code>--model-config-folder</code>, <code>--hardware-config</code>, <code>--hardware</code>, and <code>--tp</code> are provided</li> <li>Error: If no coefficients can be resolved and roofline inputs are incomplete</li> </ol>"},{"location":"design/configuration/#cluster-configuration","title":"Cluster Configuration","text":"<p>With <code>--num-instances 1</code> (the default), BLIS runs a single-instance simulation \u2014 requests go directly to the wait queue with no admission or routing layer. With <code>--num-instances N</code> (N &gt; 1), the cluster simulation activates: requests pass through the admission and routing pipeline before reaching per-instance wait queues. See Cluster Architecture for the multi-instance pipeline and Core Engine for single-instance internals.</p> Flag Type Default Description <code>--num-instances</code> int 1 Number of inference instances. 1 = single-instance mode; &gt; 1 = cluster mode with admission and routing."},{"location":"design/configuration/#admission-policy","title":"Admission Policy","text":"<p>Controls which requests enter the routing pipeline. See Cluster Architecture: Admission.</p> Flag Type Default Description <code>--admission-policy</code> string \"always-admit\" Policy name: <code>always-admit</code>, <code>token-bucket</code>, <code>reject-all</code>. <code>--admission-latency</code> int64 0 Admission decision latency in microseconds. <code>--token-bucket-capacity</code> float64 10000 Token bucket maximum capacity. Required &gt; 0 when using <code>token-bucket</code>. <code>--token-bucket-refill-rate</code> float64 1000 Token bucket refill rate in tokens/second. Required &gt; 0 when using <code>token-bucket</code>."},{"location":"design/configuration/#routing-policy","title":"Routing Policy","text":"<p>Controls how admitted requests are assigned to instances. See Cluster Architecture: Routing.</p> Flag Type Default Description <code>--routing-policy</code> string \"round-robin\" Policy name: <code>round-robin</code>, <code>least-loaded</code>, <code>weighted</code>, <code>prefix-affinity</code>, <code>always-busiest</code>. <code>--routing-latency</code> int64 0 Routing decision latency in microseconds. <code>--routing-scorers</code> string \"\" Scorer configuration for <code>weighted</code> policy. Format: <code>name:weight,name:weight,...</code> <code>--snapshot-refresh-interval</code> int64 0 KV utilization snapshot refresh interval in microseconds. 0 = immediate refresh."},{"location":"design/configuration/#scorer-configuration","title":"Scorer Configuration","text":"<p>When using <code>--routing-policy weighted</code>, the <code>--routing-scorers</code> flag configures which scorers are used and their relative weights:</p> <pre><code>--routing-scorers \"prefix-affinity:3,queue-depth:2,kv-utilization:2\"\n</code></pre> <p>Available scorers: <code>prefix-affinity</code>, <code>queue-depth</code>, <code>kv-utilization</code>, <code>load-balance</code>.</p> <p>Default (when <code>--routing-scorers</code> is empty): <code>prefix-affinity:3, queue-depth:2, kv-utilization:2</code> (llm-d parity).</p> <p>See Cluster Architecture: Scorer Composition for details on each scorer.</p>"},{"location":"design/configuration/#scheduling-and-priority","title":"Scheduling and Priority","text":"<p>Per-instance policies that control request ordering within the wait queue. Maps to <code>PolicyConfig</code>.</p> Flag Type Default Description <code>--scheduler</code> string \"fcfs\" Scheduler: <code>fcfs</code>, <code>priority-fcfs</code>, <code>sjf</code>, <code>reverse-priority</code>. <code>--priority-policy</code> string \"constant\" Priority policy: <code>constant</code>, <code>slo-based</code>, <code>inverted-slo</code>. <p>See Core Engine: Scheduling for policy details.</p>"},{"location":"design/configuration/#workload-configuration","title":"Workload Configuration","text":""},{"location":"design/configuration/#workload-modes","title":"Workload Modes","text":"<p>BLIS supports four workload specification modes, in order of precedence:</p> Mode Trigger Description Workload-spec YAML <code>--workload-spec &lt;path&gt;</code> Multi-client workload with per-client distributions. Highest priority. CLI distribution <code>--workload distribution</code> (default) Single-client Gaussian distribution controlled by CLI flags. Preset <code>--workload &lt;name&gt;</code> Named preset from <code>defaults.yaml</code> (chatbot, summarization, etc.). CSV traces <code>--workload traces</code> Replay recorded traces from a CSV file."},{"location":"design/configuration/#distribution-mode-flags","title":"Distribution Mode Flags","text":"<p>Used when <code>--workload distribution</code> (the default) and no <code>--workload-spec</code> is set.</p> Flag Type Default Description <code>--rate</code> float64 1.0 Request arrival rate in requests/second. <code>--num-requests</code> int 100 Total number of requests to generate. <code>--prompt-tokens</code> int 512 Mean prompt (input) token count. <code>--prompt-tokens-stdev</code> int 256 Standard deviation of prompt tokens. <code>--prompt-tokens-min</code> int 2 Minimum prompt token count. <code>--prompt-tokens-max</code> int 7000 Maximum prompt token count. <code>--output-tokens</code> int 512 Mean output token count. <code>--output-tokens-stdev</code> int 256 Standard deviation of output tokens. <code>--output-tokens-min</code> int 2 Minimum output token count. <code>--output-tokens-max</code> int 7000 Maximum output token count. <code>--prefix-tokens</code> int 0 Prefix token count for prefix caching simulation. Additive to prompt tokens."},{"location":"design/configuration/#workload-spec-yaml","title":"Workload-Spec YAML","text":"<p>The <code>--workload-spec</code> flag loads a YAML file defining multi-client workloads:</p> <pre><code>aggregate_rate: 100       # Total arrival rate in requests/second\nnum_requests: 1000\nseed: 42\nhorizon: 1000000000       # Ticks (microseconds)\n\nclients:\n  - id: \"interactive\"\n    rate_fraction: 0.6    # 60% of aggregate rate\n    prefix_group: \"chat\"\n    prefix_length: 512\n    arrival:\n      process: \"poisson\"\n    input_distribution:\n      type: \"gaussian\"\n      params:\n        mean: 256\n        std_dev: 128\n        min: 2\n        max: 4096\n    output_distribution:\n      type: \"exponential\"\n      params:\n        mean: 128\n\n  - id: \"batch\"\n    rate_fraction: 0.4\n    arrival:\n      process: \"gamma\"\n      cv: 2.0\n    input_distribution:\n      type: \"gaussian\"\n      params:\n        mean: 1024\n        std_dev: 512\n        min: 2\n        max: 7000\n    output_distribution:\n      type: \"gaussian\"\n      params:\n        mean: 512\n        std_dev: 256\n        min: 2\n        max: 7000\n</code></pre> <p>Supported arrival processes: <code>poisson</code>, <code>gamma</code> (with <code>cv</code> parameter), <code>weibull</code> (with <code>cv</code> parameter).</p> <p>Supported token distributions: <code>gaussian</code>, <code>exponential</code>, <code>pareto_lognormal</code>, <code>constant</code>, <code>empirical</code>.</p> <p>When <code>--workload-spec</code> is set, CLI <code>--seed</code>, <code>--horizon</code>, and <code>--num-requests</code> still override the YAML values if explicitly provided.</p>"},{"location":"design/configuration/#trace-files","title":"Trace Files","text":"Flag Type Default Description <code>--workload-spec</code> string \"\" Path to workload-spec YAML. <code>--workload-traces-filepath</code> string \"\" Path to CSV trace file (required when <code>--workload traces</code>). <code>--defaults-filepath</code> string \"defaults.yaml\" Path to <code>defaults.yaml</code>."},{"location":"design/configuration/#policy-bundle","title":"Policy Bundle","text":"<p>The <code>--policy-config</code> flag loads admission, routing, priority, and scheduling configuration from a single YAML file:</p> <pre><code>admission:\n  policy: \"always-admit\"\n  token_bucket_capacity: 10000.0\n  token_bucket_refill_rate: 1000.0\n\nrouting:\n  policy: \"weighted\"\n  scorers:\n    - name: \"prefix-affinity\"\n      weight: 3.0\n    - name: \"queue-depth\"\n      weight: 2.0\n    - name: \"kv-utilization\"\n      weight: 2.0\n\npriority:\n  policy: \"constant\"\n\nscheduler: \"fcfs\"\n</code></pre> <p>CLI flags override policy bundle values when explicitly set. For example, <code>--routing-policy least-loaded</code> overrides the bundle's <code>routing.policy</code> setting.</p>"},{"location":"design/configuration/#decision-tracing","title":"Decision Tracing","text":"Flag Type Default Description <code>--trace-level</code> string \"none\" Trace verbosity: <code>none</code> or <code>decisions</code>. <code>--counterfactual-k</code> int 0 Number of counterfactual candidates per routing decision. Requires <code>--trace-level decisions</code>. <code>--summarize-trace</code> bool false Print trace summary after simulation. Requires <code>--trace-level decisions</code>. <p>See Cluster Architecture: Counterfactual Regret.</p>"},{"location":"design/configuration/#fitness-evaluation","title":"Fitness Evaluation","text":"Flag Type Default Description <code>--fitness-weights</code> string \"\" Fitness function weights. Format: <code>metric:weight,metric:weight,...</code> <p>When configured, BLIS computes a single fitness score from aggregated metrics. Latency metrics are normalized via <code>1/(1 + value/1000)</code> where <code>value</code> is in ticks (microseconds) and 1000 = 1ms reference (lower is better); throughput metrics via <code>value/(value + reference)</code> where <code>referenceRPS = 100.0</code> and <code>referenceTPS = 10000.0</code> (higher is better). Useful for automated policy comparison across multiple simulation runs.</p>"},{"location":"design/configuration/#defaultsyaml","title":"defaults.yaml","text":"<p>The <code>defaults.yaml</code> file serves as a model registry and workload preset store:</p> <pre><code>version: \"1.0\"\n\nmodels:\n  - id: \"meta-llama/llama-3.1-8b-instruct\"\n    GPU: \"H100\"\n    tensor_parallelism: 2\n    vllm_version: \"0.6.1\"\n    alpha_coeffs: [1601.35, 3.51, 1805.54]\n    beta_coeffs: [6910.42, 17.67, 2.84]\n    total_kv_blocks: 132139\n\ndefaults:\n  \"meta-llama/llama-3.1-8b-instruct\":\n    GPU: \"H100\"\n    tensor_parallelism: 2\n    vllm_version: \"0.6.1\"\n\nworkloads:\n  chatbot:\n    prompt_tokens: 512\n    output_tokens: 512\n    # ... distribution parameters\n</code></pre>"},{"location":"design/configuration/#resolution-process","title":"Resolution Process","text":"<p>When BLIS starts:</p> <ol> <li>If <code>--roofline</code> is set:</li> <li>Auto-resolve model config: check <code>model_configs/</code> for existing <code>config.json</code>, fetch from HuggingFace on miss (set <code>HF_TOKEN</code> for gated models)</li> <li>Auto-resolve hardware config from bundled <code>hardware_config.json</code></li> <li>Load alpha coefficients and <code>total_kv_blocks</code> from <code>defaults.yaml</code> (beta coefficients are replaced by roofline computation)</li> <li><code>--model-config-folder</code> and <code>--hardware-config</code> override auto-resolution when explicitly set</li> <li>If <code>--alpha-coeffs</code> and <code>--beta-coeffs</code> are both all-zero and no roofline config is provided:</li> <li>Look up the model in <code>defaults.yaml</code> using <code>--model</code>, <code>--hardware</code>, <code>--tp</code>, <code>--vllm-version</code></li> <li>Load alpha/beta coefficients and <code>total_kv_blocks</code> from the matching entry</li> <li>Override <code>--total-kv-blocks</code> only if the user did not explicitly set it</li> <li>If coefficients are still all-zero but <code>--model-config-folder</code> and <code>--hardware-config</code> are provided:</li> <li>Enable roofline mode (implicit activation)</li> <li>If coefficients were explicitly provided via CLI:</li> <li>Use them directly, no <code>defaults.yaml</code> lookup</li> </ol>"},{"location":"design/configuration/#coefficient-calibration","title":"Coefficient Calibration","text":"<p>BLIS uses a data-driven calibration strategy to ensure simulation accuracy. This process runs once per environment configuration (model, GPU, TP degree, vLLM version):</p> <ol> <li>Initialization: Define baseline estimates for alpha and beta coefficients as starting points for optimization</li> <li>Profiling: Execute training workloads on a live vLLM instance to collect ground-truth mean and P90 metrics for TTFT, ITL, and E2E</li> <li>Optimization: Run BLIS iteratively using Blackbox Bayesian Optimization to minimize the multi-objective loss:</li> </ol> <p>$$\\text{Loss} = \\sum_{m \\in {\\text{TTFT, ITL, E2E}}} \\left( |GT_{\\text{mean},m} - Sim_{\\text{mean},m}| + |GT_{\\text{p90},m} - Sim_{\\text{p90},m}| \\right)$$</p> <ol> <li>Artifact generation: Optimal alpha/beta coefficients are stored in <code>defaults.yaml</code> for production use</li> </ol> <p>For environments where live profiling is not feasible, the Roofline model provides analytical step time estimation without any training data.</p>"},{"location":"design/configuration/#cli-flag-summary-by-sub-config","title":"CLI Flag Summary by Sub-Config","text":"Sub-Config Flags KVCacheConfig <code>--total-kv-blocks</code>, <code>--block-size-in-tokens</code>, <code>--kv-cpu-blocks</code>, <code>--kv-offload-threshold</code>, <code>--kv-transfer-bandwidth</code>, <code>--kv-transfer-base-latency</code> BatchConfig <code>--max-num-running-reqs</code>, <code>--max-num-scheduled-tokens</code>, <code>--long-prefill-token-threshold</code> LatencyCoeffs <code>--alpha-coeffs</code>, <code>--beta-coeffs</code> ModelHardwareConfig <code>--model</code>, <code>--hardware</code>, <code>--tp</code>, <code>--vllm-version</code>, <code>--roofline</code>, <code>--model-config-folder</code>, <code>--hardware-config</code> PolicyConfig <code>--scheduler</code>, <code>--priority-policy</code> WorkloadConfig <code>--workload</code>, <code>--workload-spec</code>, <code>--workload-traces-filepath</code>, <code>--defaults-filepath</code>, <code>--rate</code>, <code>--num-requests</code>, <code>--prompt-tokens*</code>, <code>--output-tokens*</code>, <code>--prefix-tokens</code> DeploymentConfig <code>--num-instances</code>, <code>--admission-policy</code>, <code>--admission-latency</code>, <code>--token-bucket-capacity</code>, <code>--token-bucket-refill-rate</code>, <code>--routing-policy</code>, <code>--routing-latency</code>, <code>--routing-scorers</code>, <code>--snapshot-refresh-interval</code>, <code>--trace-level</code>, <code>--counterfactual-k</code> Top-level <code>--seed</code>, <code>--horizon</code>, <code>--log</code>, <code>--results-path</code>, <code>--policy-config</code>, <code>--fitness-weights</code>, <code>--summarize-trace</code>"},{"location":"design/core-engine/","title":"Core Engine","text":"<p>This page describes BLIS's single-instance discrete event simulation engine. For multi-instance cluster orchestration, see Cluster Architecture.</p> <p>Canonical sources: System invariants (INV-1 through INV-8) are defined in <code>docs/standards/invariants.md</code>. If invariant descriptions here diverge, <code>invariants.md</code> is authoritative.</p>"},{"location":"design/core-engine/#overview","title":"Overview","text":"<p>Each BLIS instance is a self-contained discrete event simulator. The engine maintains an event queue (min-heap), processes events in timestamp order, and advances a simulation clock. The core loop is:</p> <pre><code>while events remain and clock &lt; horizon:\n    event = pop earliest event from queue\n    advance clock to event.timestamp\n    execute event (may produce new events)\n</code></pre> <p>The engine models the full vLLM inference pipeline: request arrival, queueing, batch formation, step execution, KV cache management, and latency estimation \u2014 all without real GPU hardware.</p>"},{"location":"design/core-engine/#event-queue","title":"Event Queue","text":"<p>The event queue is a min-heap ordered by event timestamp. Events represent state transitions in the simulation:</p> Event Type Trigger Effect <code>ArrivalEvent</code> Request enters system Computes queueing delay (alpha overhead), schedules <code>QueuedEvent</code> <code>QueuedEvent</code> Request enters wait queue Adds request to wait queue; if no <code>StepEvent</code> exists, schedules one (work-conserving) <code>StepEvent</code> Batch ready for execution Runs the 4-phase Step() cycle (see below) <code>ScheduledEvent</code> Request moves to running batch Timeline marker for tracing (scheduling delay recorded in <code>scheduleBatch</code>) <code>PreemptionEvent</code> KV cache eviction Timeline marker for tracing (preemption count recorded in <code>scheduleBatch</code>) <code>RequestLeftEvent</code> Request completes Timeline marker for tracing (E2E metrics recorded in <code>processCompletions</code>) <p>Clock monotonicity (INV-3): The simulation clock never decreases. Events are processed in strictly non-decreasing timestamp order.</p> <p>Work-conserving (INV-8): After every step completion, if the wait queue is non-empty, a <code>StepEvent</code> must exist in the event queue. The simulator never idles while work is waiting.</p> <p></p>"},{"location":"design/core-engine/#step-phases","title":"Step Phases","text":"<p>The <code>Step()</code> function is a 4-line orchestrator that delegates to four phases:</p>"},{"location":"design/core-engine/#phase-1-schedule-batch-schedulebatch","title":"Phase 1: Schedule Batch (<code>scheduleBatch</code>)","text":"<ol> <li>Assign priority scores to all queued requests via the priority policy</li> <li>Reorder the wait queue via the scheduling policy (e.g., FCFS, SJF, priority-based)</li> <li>Invoke batch formation to select which requests enter the running batch</li> <li>Schedule <code>PreemptionEvent</code> for any evicted requests</li> <li>Schedule <code>ScheduledEvent</code> for any newly admitted requests</li> </ol>"},{"location":"design/core-engine/#phase-2-execute-batch-step-executebatchstep","title":"Phase 2: Execute Batch Step (<code>executeBatchStep</code>)","text":"<ol> <li>Compute step time via the latency model based on batch composition</li> <li>For each request in the batch:</li> <li>If in prefill phase: advance the progress index through input tokens (respecting chunked prefill limits)</li> <li>If in decode phase: advance the progress index by one output token</li> <li>Record TTFT at the prefill-to-decode boundary</li> <li>Return the computed step time to Phase 4 for scheduling the next <code>StepEvent</code> (the clock itself advances only at the event-loop level when the next event is popped)</li> </ol>"},{"location":"design/core-engine/#phase-3-process-completions-processcompletions","title":"Phase 3: Process Completions (<code>processCompletions</code>)","text":"<p>Identify completed requests (all output tokens generated), release their KV blocks, record E2E metrics, and schedule <code>RequestLeftEvent</code>. Note: TTFT is already recorded in Phase 2 at the prefill-to-decode boundary, so Phase 3 only handles E2E and completion bookkeeping. This separation (TTFT in Phase 2, E2E in Phase 3) is the \"two-pass\" design that ensures TTFT is recorded before E2E.</p>"},{"location":"design/core-engine/#phase-4-schedule-next-step-schedulenextstep","title":"Phase 4: Schedule Next Step (<code>scheduleNextStep</code>)","text":"<ol> <li>If the running batch still has requests, schedule the next <code>StepEvent</code> at <code>now + stepTime</code></li> <li>If the running batch is empty but the wait queue has requests, schedule the next <code>StepEvent</code> at <code>now + stepTime</code> (work-conserving)</li> <li>If both are empty, do nothing (next event will be a future arrival)</li> </ol>"},{"location":"design/core-engine/#request-lifecycle","title":"Request Lifecycle","text":"<p>Requests follow a linear state machine with one exception (preemption):</p> <pre><code>                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                      \u2502         Preemption               \u2502\n                      \u2502    (re-enqueue at front)         \u2502\n                      \u25bc                                  \u2502\n  Arrival \u2500\u2500\u25b6 Queued \u2500\u2500\u25b6 Running \u2500\u2500\u25b6 Completed\n               \u2502                        \u2502\n               \u2502                        \u25bc\n               \u2502                   RequestLeft\n               \u2502\n  Arrival \u2500\u2500\u25b6 DroppedUnservable\n             (input too large for KV cache)\n</code></pre> <p></p>"},{"location":"design/core-engine/#states","title":"States","text":"State Description Queued In wait queue, not yet in running batch. Ordered by scheduling policy. Running In running batch, actively being processed. Progressing through prefill or decode. Completed All output tokens generated. KV blocks released. Metrics recorded."},{"location":"design/core-engine/#key-timestamps","title":"Key Timestamps","text":"<p>These are the conceptual timestamps in a request's lifecycle. Some are stored as struct fields (e.g., <code>ArrivalTime</code>, <code>FirstTokenTime</code>), while others are computed at metric recording time.</p> Timestamp When Recorded Used For Arrival time Request creation E2E and scheduling delay baseline Enqueue time After alpha queueing delay Conceptual start of wait queue residence Schedule time Batch formation selects request Scheduling delay = time in wait queue First token time End of prefill phase TTFT = FirstTokenTime (stored on Request) Completion time All tokens generated E2E = FirstTokenTime + sum(ITLs) <p>Causality invariant (INV-5): <code>arrival_time &lt;= enqueue_time &lt;= schedule_time &lt;= completion_time</code></p>"},{"location":"design/core-engine/#preemption","title":"Preemption","text":"<p>When KV cache pressure forces eviction, running requests are preempted: 1. Request is removed from the running batch (tail-first eviction) 2. Its KV blocks are freed 3. It is re-enqueued at the front of the wait queue (not the back) 4. A <code>PreemptionEvent</code> is recorded for tracing</p> <p>Preempted requests reset to the beginning of prefill (ProgressIndex = 0) and their KV blocks are freed. However, the freed blocks' prefix hashes are preserved in the KV cache's free list \u2014 when the request is re-scheduled, prefix caching may find these blocks (if not yet evicted by LRU), reducing recomputation. This matches vLLM's \"recompute\" preemption mode.</p>"},{"location":"design/core-engine/#dropped-requests","title":"Dropped Requests","text":"<p>Requests whose input tokens require more KV blocks than the total cache capacity are dropped at enqueue time with a <code>DroppedUnservable</code> counter increment. This prevents livelock where the simulator would endlessly preempt and re-enqueue a request that can never fit.</p>"},{"location":"design/core-engine/#batch-formation","title":"Batch Formation","text":"<p>BLIS models vLLM's continuous batching algorithm through the <code>VLLMBatchFormation</code> implementation.</p>"},{"location":"design/core-engine/#two-phase-algorithm","title":"Two-Phase Algorithm","text":"<p>Phase 1: Continuing Requests Process requests already in the running batch: - Apply chunked prefill limits (<code>--long-prefill-token-threshold</code>) - Allocate token budget for decode tokens - If KV allocation fails for a continuing request, preempt requests from the batch tail to free blocks</p> <p>Phase 2: New Requests Dequeue requests from the wait queue: - Compute cached prefix blocks (prefix caching reduces allocation needs) - Allocate KV blocks for uncached prefix tokens being processed this step (bounded by chunked prefill threshold and remaining token budget) - Stop dequeuing when: max batch size reached (<code>--max-num-running-reqs</code>), allocation fails (cache full), token budget exhausted, or a preemption occurred during Phase 1</p>"},{"location":"design/core-engine/#constraints","title":"Constraints","text":"Constraint Flag Effect Max batch size <code>--max-num-running-reqs</code> Limits number of concurrent requests in the running batch Token budget <code>--max-num-scheduled-tokens</code> Limits total new tokens across all running requests per step Chunked prefill <code>--long-prefill-token-threshold</code> Splits long prefills across multiple steps"},{"location":"design/core-engine/#preemption-strategy","title":"Preemption Strategy","text":"<p>When KV allocation fails for a continuing request: 1. Evict requests from the batch tail (reverse order of admission) 2. Free their KV blocks 3. Re-enqueue evicted requests at the front of the wait queue 4. Retry allocation for the original request 5. Circuit breaker: Stop if allocation still fails after exhausting the batch, or if the request itself is unservable</p>"},{"location":"design/core-engine/#kv-cache-management","title":"KV Cache Management","text":"<p>The KV cache simulates GPU memory organized as fixed-size blocks. Each block holds <code>--block-size-in-tokens</code> tokens (default: 16).</p>"},{"location":"design/core-engine/#single-tier-cache","title":"Single-Tier Cache","text":"<p>The default KV cache operates entirely in GPU memory:</p> <ul> <li>Block allocation: Requests are allocated blocks proportional to their token count</li> <li>Prefix caching: Hierarchical block hashing enables prefix sharing across requests. Each block's hash chains with the prior block's hash, creating semantic prefix signatures. When a new request shares a prefix with an existing request, the cached blocks are reused rather than reallocated.</li> <li>LRU eviction: Free blocks are managed via a doubly-linked list with LRU ordering</li> <li>Reference counting: Shared blocks (prefix caching) are reference-counted and exempt from eviction while any request references them</li> <li>Transactional allocation: Multi-block allocations are rolled back on failure (no partial allocation)</li> </ul> <p>Conservation invariant (INV-4): <code>allocated_blocks + free_blocks = total_blocks</code> at all times.</p>"},{"location":"design/core-engine/#tiered-cache-gpu-cpu","title":"Tiered Cache (GPU + CPU)","text":"<p>When <code>--kv-cpu-blocks</code> is set to a positive value, BLIS enables a two-tier cache:</p> <ul> <li>GPU tier: Full KV cache with prefix caching and LRU eviction</li> <li>CPU tier: Simple capacity store for offloaded blocks</li> <li>Offload trigger: When GPU utilization exceeds <code>--kv-offload-threshold</code> (default: 0.9), blocks are offloaded to CPU</li> <li>Reload: On GPU allocation failure, blocks are reloaded from CPU with a transfer latency penalty</li> <li>Transfer latency: Per reloaded block: <code>base_latency + ceil(block_size_tokens / bandwidth)</code>. Accumulated across all reloaded blocks. Non-blocking (added to step time).</li> <li>Thrashing detection: Blocks offloaded and reloaded within 1000 ticks (1ms) increment a thrashing counter</li> </ul>"},{"location":"design/core-engine/#latency-models","title":"Latency Models","text":"<p>BLIS predicts GPU step time through one of two latency model backends. The choice is made automatically based on available configuration.</p>"},{"location":"design/core-engine/#blackbox-model-default","title":"Blackbox Model (Default)","text":"<p>Uses trained regression coefficients to predict step time:</p> <pre><code>StepTime    = beta0 + beta1 * cache_miss_tokens + beta2 * decode_tokens\nQueueingTime = alpha0 + alpha1 * input_length\nOutputTokenProcessingTime = alpha2\n</code></pre> <ul> <li>Beta coefficients model GPU execution time as a linear function of batch composition</li> <li>Alpha coefficients model non-GPU overhead (tokenization, API serialization, output processing)</li> <li>Coefficients are trained offline via Bayesian optimization against real vLLM measurements</li> <li>Pre-trained coefficients for common model/GPU combinations are shipped in <code>defaults.yaml</code></li> </ul> <p>See Configuration Reference: Coefficient Calibration for the training process.</p>"},{"location":"design/core-engine/#roofline-model-analytical","title":"Roofline Model (Analytical)","text":"<p>Uses analytical FLOPs/bandwidth estimation when no trained coefficients are available:</p> <pre><code>Phase Time = max(total_FLOPs / peak_compute, total_bytes / peak_bandwidth)\nStep Time  = Prefill Phase Time + Decode Phase Time\n</code></pre> <ul> <li>Requires HuggingFace <code>config.json</code> (model architecture: layers, heads, hidden dim)</li> <li>Requires <code>hardware_config.json</code> (GPU specs: peak TFLOPS, peak bandwidth, MFU)</li> <li>Accounts for Tensor Parallelism, All-Reduce latency, and per-layer overheads</li> <li>No training data needed \u2014 works for any supported model immediately</li> </ul> <p>See Roofline Estimation for implementation details.</p>"},{"location":"design/core-engine/#alpha-overhead","title":"Alpha Overhead","text":"<p>Alpha overhead models non-GPU processing time: - Queueing time (<code>alpha0 + alpha1 * input_length</code>): Delays request enqueue but does not block the server. The simulation clock is not advanced by this overhead. - Output token processing time (<code>alpha2</code>): Added to per-request ITL/TTFT metrics but does not block the next step.</p> <p>This is architecturally correct for vLLM, where CPU post-processing (tokenization, output serialization) runs concurrently with GPU execution.</p>"},{"location":"design/core-engine/#scheduling-policies","title":"Scheduling Policies","text":"<p>Scheduling policies control the order in which queued requests are selected for batch formation. They operate per-instance. To add a new scheduling policy, see Extension Recipes.</p> Policy Ordering Rule Use Case <code>fcfs</code> No reordering (arrival order) Default, fair <code>priority-fcfs</code> Priority descending, then arrival ascending SLO-aware scheduling <code>sjf</code> Input token count ascending, then arrival ascending Shortest-job-first for TTFT optimization <code>reverse-priority</code> Priority ascending (starves high-priority) Pathological testing only"},{"location":"design/core-engine/#priority-policies","title":"Priority Policies","text":"<p>Priority policies assign a numeric score to each request before scheduling:</p> Policy Score Computation Behavior <code>constant</code> Fixed score for all requests No differentiation (default) <code>slo-based</code> <code>base + age_weight * (clock - arrival)</code> Favors older requests, prevents starvation. Note: despite the name, does not currently use per-request SLO metadata. <code>inverted-slo</code> <code>base - age_weight * age</code> Starves older requests (pathological)"},{"location":"design/core-engine/#metrics","title":"Metrics","text":"<p>BLIS records per-request and aggregate metrics throughout the simulation.</p>"},{"location":"design/core-engine/#per-request-metrics","title":"Per-Request Metrics","text":"Metric Definition TTFT Time from arrival to first token: includes queueing delay, prefill step times, and output processing overhead (alpha2) E2E <code>FirstTokenTime + sum(ITLs)</code>, where each ITL includes step time + alpha2 ITL Observed time between consecutive decode steps (includes alpha2 per token) Scheduling Delay Time from request arrival to entering the running batch (includes alpha queueing overhead + wait queue residence)"},{"location":"design/core-engine/#aggregate-metrics","title":"Aggregate Metrics","text":"Metric Aggregation TTFT, E2E, ITL distributions Mean, p90, p95, p99 in JSON output. Cluster-internal <code>Distribution</code> type also computes p50, min, max for fitness evaluation. Throughput Output tokens per second, requests per second Preemption count Total KV cache evictions KV allocation failures Failed block allocations Dropped unservable Requests too large for cache"},{"location":"design/core-engine/#conservation-invariant-inv-1","title":"Conservation Invariant (INV-1)","text":"<p>At simulation end: <code>injected_requests == completed_requests + still_queued + still_running + dropped_unservable</code></p> <p>This is the fundamental accounting invariant that ensures no requests are silently lost.</p>"},{"location":"design/core-engine/#determinism","title":"Determinism","text":"<p>BLIS guarantees deterministic output: the same seed produces byte-identical stdout across runs (INV-6).</p> <p>Key mechanisms: - Partitioned RNG: Each subsystem (workload generation, scheduling, etc.) uses an independent random stream derived from the seed, so adding randomness to one subsystem doesn't perturb others - Sorted map iteration: Where map iteration order affects output, keys are sorted first - Stdout/stderr separation: Deterministic results go to stdout; wall-clock timing and diagnostics go to stderr via logrus</p>"},{"location":"design/roofline/","title":"Roofline Step Time Estimation Logic","text":"<p>This document describes the analytical approach used to estimate the GPU latency for a single inference step using a roofline model. This requires no training, and works off-the-shelf for any Huggingface LLM whose <code>config.json</code> is saved under <code>model_configs/</code>, except Mixture-of-Expert (MoE) models currently.</p>"},{"location":"design/roofline/#1-why-roofline","title":"1. Why Roofline?","text":"<p>The Blackbox optimization approach outlined in Configuration: Coefficient Calibration requires training over ground-truth workload metrics obtained by running live vLLM instances over GPUs. In practice, collecting this ground-truth data and training the GPU latency model for each LLM can take several hours. For a faster approximation of GPU latencies without actually ever running vLLM, we recommend the roofline approach. This technique only requires the Huggingface LLM config (<code>config.json</code>) and hardware specifications (e.g. GPU Peak TFLOPS/Peak BW from NVIDIA datasheets) in <code>hardware_config.json</code>. It allows BLIS to generalize across diverse LLMs, TP values and workloads, while preserving TTFT/ITL/E2E accuracy.</p>"},{"location":"design/roofline/#2-core-methodology-the-roofline-model","title":"2. Core Methodology: The Roofline Model","text":"<p>The simulator predicts execution time by identifying whether a phase (prefill/decode) is Compute-Bound or Memory-Bound. </p> <p>For each phase:</p> <p>$$\\text{Phase Time} = \\max\\left( \\frac{\\text{Total FLOPS}}{\\text{Peak Performance}}, \\frac{\\text{Total Bytes Transferred}}{\\text{Memory Bandwidth}} \\right)$$</p> <p>As a result, overall step time is given by:</p> <p>$$\\text{Step Time} = \\text{Prefill Phase Time} + \\text{Decode Phase Time}$$</p>"},{"location":"design/roofline/#3-calculation-phases","title":"3. Calculation Phases","text":""},{"location":"design/roofline/#a-flops-calculation-calculatetransformerflops","title":"A. FLOPs Calculation (<code>calculateTransformerFlops</code>)","text":"<p>We track two types of operations: * GEMM Ops: Matrix multiplications for QKV projections, Attention Output, and MLP (SwiGLU) layers. Includes $QK^T$ and $AV$ operations. * Vector Ops: Non-tensor core operations like Softmax, RoPE (rotary embeddings), and normalization.</p>"},{"location":"design/roofline/#b-memory-access-calculatememoryaccessbytes","title":"B. Memory Access (<code>calculateMemoryAccessBytes</code>)","text":"<p>We calculate the data movement between HBM (High Bandwidth Memory) and the processor: * Weights: Static model parameters loaded once per layer. * KV Cache Growth: Writing new Key/Value tokens to memory. * KV Cache Access: Reading the history (past tokens) for attention.</p>"},{"location":"design/roofline/#3-execution-pipeline","title":"3. Execution Pipeline","text":"<p>The final step time is the sum of independent phases and overheads:</p> <ol> <li>Prefill Phase: Calculated for the initial prompt processing chunk.</li> <li>Decode Phase: Calculated for generating new tokens (usually memory-bound).</li> <li>Communication Overhead: If using Tensor Parallelism ($TP &gt; 1$), adds All-Reduce latency per layer.</li> <li>Hardware Overheads: Static kernel launch times and layer-by-layer overhead constants.</li> </ol>"},{"location":"design/roofline/#4-key-performance-variables","title":"4. Key Performance Variables","text":"<ul> <li>MFU (Model Flops Utilization): Scaled TFLOPs efficiency factors for Prefill vs. Decode.</li> <li>TP Factor: Divides compute and memory load across multiple GPUs.</li> <li>Bandwidth Efficiency: Real-world effective bandwidth versus theoretical peak bandwidth.</li> </ul>"},{"location":"design/roofline/#5-onboarding-a-new-llmgpu","title":"5. Onboarding a new LLM/GPU","text":""},{"location":"design/roofline/#automatic-recommended-roofline-flag","title":"Automatic (recommended): <code>--roofline</code> flag","text":"<p>The simplest way to run roofline mode is with the <code>--roofline</code> flag, which auto-resolves the model config:</p> <pre><code>./simulation_worker run --model meta-llama/llama-3.1-8b-instruct --roofline --hardware H100 --tp 1\n</code></pre> <p>The flag automatically: 1. Checks <code>model_configs/</code> for an existing <code>config.json</code> (previously fetched) 2. Fetches from HuggingFace on miss and writes into <code>model_configs/</code> (supports <code>HF_TOKEN</code> for gated models)</p> <p>For models not in <code>defaults.yaml</code>, add an <code>hf_repo</code> entry mapping the BLIS model name to the case-sensitive HuggingFace repo path.</p>"},{"location":"design/roofline/#manual-explicit-config-paths","title":"Manual: explicit config paths","text":"<p>Alternatively, download the <code>config.json</code> manually:</p> <ul> <li>Download the <code>config.json</code> for the LLM of your choice into <code>model_configs/</code>. This is an example config.json for <code>meta-llama/Llama-3.3-70B-Instruct</code>. The recommended file structure is <code>model_configs/llama-3.1-8b-instruct/config.json</code>.</li> </ul>"},{"location":"design/roofline/#adding-a-new-gpu","title":"Adding a new GPU","text":"<ul> <li>Refer to NVIDIA datasheets for GPU specs (for example, datasheet for H100) and add an entry to <code>hardware_config.json</code> as follows:</li> </ul> <pre><code>&lt;GPU_name&gt;: {\n        \"TFlopsEff\":        &lt;Peak TFLOPS from datasheet&gt;,      \n        \"BwEffTBs\":         &lt;Peak BW from datasheet&gt;,\n        \"BwEffConstant\":    0.72,\n        \"TOverheadMicros\":  500.0,\n        \"perLayerOverhead\": 20.0,\n        \"mfuPrefill\":       0.65,\n        \"mfuDecode\":        0.12,\n        \"allReduceLatency\": 20.0\n    }\n</code></pre> <p>Note: The Peak TFLOPS and BW for a given GPU family might vary by GPU connectivity (e.g. SXM vs PCIe). We recommend a separate entry for each GPU connectivity type - e.g. A100-SXM, A100-PCIe etc in <code>hardware_config.json</code>. </p>"},{"location":"process/convergence/","title":"Universal Convergence Protocol","text":"<p>Status: Active (v1.0 \u2014 extracted 2026-02-26 from hypothesis.md)</p> <p>This document defines the convergence protocol used by all review gates across BLIS workflows: - PR workflow (docs/process/pr-workflow.md): Plan Review (10 perspectives), Code Review (10 perspectives) - Hypothesis workflow (docs/process/hypothesis.md): Design Review (5 perspectives), Code Review (5 perspectives), FINDINGS Review (10 perspectives) - Design process (docs/process/design.md): Design Doc Review (8 perspectives) - Macro-plan process (docs/process/macro-plan.md): Macro Plan Review (8 perspectives)</p> <p>Executable implementation: The <code>convergence-review</code> skill automates this protocol \u2014 dispatching perspectives, tallying findings independently, and enforcing the re-run gate. Invoke with <code>/convergence-review &lt;gate-type&gt; [artifact-path] [--model opus|sonnet|haiku]</code> (default: <code>haiku</code>).</p>"},{"location":"process/convergence/#the-protocol","title":"The Protocol","text":"<ol> <li>Run all N perspectives in parallel (one round)</li> <li>Collect all findings; each classified as CRITICAL / IMPORTANT / SUGGESTION</li> <li>If zero CRITICAL and zero IMPORTANT across all N reviewers: Converged \u2014 proceed to next step</li> <li>If any CRITICAL or IMPORTANT from any reviewer: Fix all issues, return to step 1 (re-run entire round)</li> <li>Repeat until convergence</li> </ol>"},{"location":"process/convergence/#rules","title":"Rules","text":"<ul> <li>Max 10 rounds per gate. Each gate has its own independent round counter. If a gate fails to converge within 10 rounds, suspend the work: document the remaining issues as future work.</li> <li>No minimum round count. Convergence in Round 1 is valid if no reviewer flags any CRITICAL or IMPORTANT item.</li> <li>Hard gate \u2014 NO EXCEPTIONS. You MUST re-run after fixes. You may NOT skip the re-run, propose alternative steps, or rationalize that fixes were \"trivial enough.\" The re-run is the only evidence of convergence. This is non-negotiable.</li> <li>SUGGESTION-level items (documentation nits, cosmetic fixes, off-by-one line citations) do not block convergence.</li> </ul>"},{"location":"process/convergence/#severity-levels","title":"Severity Levels","text":"<p>Each reviewer must classify every finding:</p> <ul> <li>CRITICAL: Must fix before proceeding. Examples: missing control experiment (RCV-4), status classification contradicted by data, silent data loss in analyzer, cross-document contradiction.</li> <li>IMPORTANT: Should fix before proceeding. The key test: would proceeding with this unfixed item mislead a reader or produce incorrect conclusions? Examples: sub-threshold effect size in one seed, stale text contradicting current results, undocumented confound.</li> <li>SUGGESTION: Does not affect correctness or reader understanding. Examples: off-by-one line citation (\u00b12 lines), cosmetic terminology, style consistency.</li> </ul> <p>When in doubt between IMPORTANT and SUGGESTION: If fixing the item would change any conclusion, metric, or user guidance, it is IMPORTANT. If it would only improve readability without changing any conclusion, it is SUGGESTION. If multiple reviewers classify the same item at different severities, the highest severity applies.</p>"},{"location":"process/convergence/#agent-failure-handling","title":"Agent Failure Handling","text":"<ul> <li>Timeout: 5 minutes per reviewer agent. If an agent exceeds this, check its output file and restart if stalled.</li> <li>Failure: If a reviewer agent fails or hangs, fall back to performing that review directly (read the artifact yourself with that reviewer's checklist). Do not skip a reviewer perspective.</li> <li>External contributors: Submit your artifacts via PR. Maintainers will run the review protocol on your behalf.</li> </ul>"},{"location":"process/convergence/#expected-convergence-rates","title":"Expected Convergence Rates","text":"<p>Gates with more perspectives (FINDINGS Review: 10) will naturally converge more slowly than gates with fewer (Design Review: 5). This is correct behavior \u2014 more eyes = higher quality bar. Typical expectations: - Hypothesis Design Review (5 perspectives): 1-2 rounds (empirical, from PR #310-#433) - Hypothesis Code Review (5 perspectives): 1-3 rounds (empirical) - Design Doc Review (8 perspectives): 1-2 rounds (estimated \u2014 no empirical data yet) - Macro Plan Review (8 perspectives): 1-2 rounds (estimated) - PR Plan/Code Review (10 perspectives): 1-3 rounds (empirical, from PR #381-#433) - FINDINGS Review (10 perspectives): 1-5 rounds (empirical)</p>"},{"location":"process/convergence/#references","title":"References","text":"<ul> <li>PR workflow: docs/process/pr-workflow.md</li> <li>Hypothesis workflow: docs/process/hypothesis.md</li> <li>Design process: docs/process/design.md</li> <li>Macro-plan process: docs/process/macro-plan.md</li> </ul>"},{"location":"process/design/","title":"Design Process","text":"<p>Status: Active (v1.0 \u2014 updated 2026-02-26)</p> <p>This document describes the process for writing a BLIS design document. For the design document template itself, see docs/templates/design-guidelines.md.</p>"},{"location":"process/design/#when-a-design-doc-is-needed","title":"When a Design Doc is Needed","text":"<ul> <li>New subsystem modules (new interface + integration)</li> <li>Backend swaps (alternative implementations requiring interface extraction)</li> <li>Architecture changes affecting module boundaries</li> </ul> <p>Not needed for: Bug fixes, new policy templates behind existing interfaces, documentation changes.</p>"},{"location":"process/design/#steps","title":"Steps","text":"<ol> <li>Identify the extension type \u2014 policy template, subsystem module, backend swap, or tier composition (see design guidelines Section 5)</li> <li>Choose the design doc species \u2014 decision record, specification, problem analysis, or system overview (Section 3.2)</li> <li>Complete the DES checklist (Section 2.6) \u2014 model scoping, event design, state/statistics, V&amp;V, randomness</li> <li>Write the design doc per the template's required sections (Section 3.3): motivation, scope, modeling decisions, invariants, decisions with trade-offs, extension points, validation strategy</li> <li>Apply the staleness test (Section 3.1) \u2014 would this content mislead if the implementation changes?</li> <li>Convergence review \u2014 <code>/convergence-review design &lt;design-doc-path&gt;</code> dispatches 8 parallel perspectives and enforces convergence (see docs/process/convergence.md). Manual alternative: review against each perspective checklist below.</li> <li>Human review \u2014 approve before macro/micro planning begins</li> </ol>"},{"location":"process/design/#design-review-perspectives-8","title":"Design Review Perspectives (8)","text":"<p>For each perspective, check every item. Classify findings as CRITICAL / IMPORTANT / SUGGESTION per the convergence protocol. Section references below refer to design-guidelines.md unless otherwise noted.</p> <p>Perspective 1 \u2014 Motivation &amp; Scoping: - Are the analysis questions clear and specific? - Is the modeling decisions table complete (modeled / simplified / omitted)? - Does every \"simplified\" entry state what real-system behavior is lost? - Has each component been evaluated against the six model scoping criteria (design-guidelines Section 2.1)?</p> <p>Perspective 2 \u2014 DES Foundations: - Is the DES design review checklist (Section 2.6) completed with all 10 questions answered? - Are new events classified as exogenous or endogenous? - Do new events specify priority constants for tie-breaking? - Is state/statistics separation maintained (Section 2.3)? - Are new randomness sources declared with PartitionedRNG subsystem names?</p> <p>Perspective 3 \u2014 Module Contract Completeness: - Does every new or modified module have all 6 contract aspects (observes, controls, owns, invariants, events, extension friction)? - Are invariants named (INV-N) and cross-referenced with existing invariants? - Is the extension friction count specified and within reference targets (Section 4.5)?</p> <p>Perspective 4 \u2014 Extension Framework Fit: - Is the extension type correctly identified (policy template / subsystem module / backend swap / tier composition)? - Is the correct recipe from Section 5 followed? - Is the no-op default specified (existing behavior unchanged when extension not configured)? - Is parallel development path described?</p> <p>Perspective 5 \u2014 Prohibited Content: - Any Go struct definitions with field lists? (Prohibited \u2014 Section 3.4) - Any method implementations? (Prohibited) - Any file paths with line numbers? (Prohibited) - Any interface signatures in Go syntax for pre-freeze interfaces? (Prohibited)</p> <p>Perspective 6 \u2014 Trade-off Quality: - Does every non-obvious decision have alternatives listed with rationale? - For each decision: what breaks if it's wrong? - Is there a Decision Status column (Proposed / Implemented / Superseded)?</p> <p>Perspective 7 \u2014 Validation Strategy: - How will correctness be verified? (Which invariants?) - How will fidelity be validated? (Against what real-system data?) - Are both verification and validation addressed (not just one)?</p> <p>Perspective 8 \u2014 Staleness Resistance: - Apply the staleness test (Section 3.1) to every section - Would any content mislead if the implementation changes during micro-planning? - Is content described behaviorally (what crosses a boundary and why) rather than structurally (how the boundary is implemented)?</p>"},{"location":"process/design/#quality-gates","title":"Quality Gates","text":"<ul> <li> Extension type identified and correct recipe followed</li> <li> DES checklist from Section 2.6 completed</li> <li> No prohibited content (Section 3.4): no Go structs, no method implementations, no file:line references</li> <li> Every non-obvious decision has alternatives listed with rationale</li> <li> Validation strategy specified (which invariants? against what real-system data?)</li> </ul>"},{"location":"process/design/#prerequisites","title":"Prerequisites","text":"Skill Purpose Manual Alternative <code>convergence-review</code> Dispatch parallel review perspectives (Step 6) Review against each perspective checklist above"},{"location":"process/design/#references","title":"References","text":"<ul> <li>Template: docs/templates/design-guidelines.md</li> <li>Convergence protocol: docs/process/convergence.md</li> <li>Standards: docs/standards/rules.md, docs/standards/invariants.md</li> </ul>"},{"location":"process/hypothesis/","title":"Hypothesis Experiment Process","text":"<p>Status: Active (v2.0 \u2014 updated 2026-02-23)</p> <p>This document describes the end-to-end process for running a hypothesis-driven experiment in BLIS. For experiment standards (rigor, classification, analysis), see docs/standards/experiments.md. For the FINDINGS.md template, see docs/templates/hypothesis.md. For experiment status and coverage gaps, see hypotheses/README.md.</p>"},{"location":"process/hypothesis/#prerequisites","title":"Prerequisites","text":"<p>This workflow uses the following Claude Code skills. Each has a manual alternative for contributors without AI tools.</p> Skill Purpose Used In Manual Alternative <code>superpowers:using-git-worktrees</code> Create isolated workspace Step 0 <code>git worktree add .worktrees/h-&lt;name&gt; -b h-&lt;name&gt;</code> <code>convergence-review</code> Dispatch parallel perspectives and enforce convergence Steps 2, 5, 8 Sequential manual checklists (one per perspective) using the checklist sections below <code>commit-commands:commit-push-pr</code> Commit, push, create PR Step 10 Standard git commands (<code>git add</code>, <code>git commit</code>, <code>git push</code>, <code>gh pr create</code>) <p>For external contributors without AI review infrastructure: Submit your experiment artifacts via PR. Maintainers will run the review protocols on your behalf. You can also conduct reviews manually by having people review with the perspective checklists documented in each gate.</p>"},{"location":"process/hypothesis/#overview","title":"Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 0: Create Worktree          \u2502 (Isolated workspace)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 1: Select &amp; Classify        \u2502 (Family, VV&amp;UQ, type)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 2: Design + Design Review   \u2502 (5 perspectives \u2192 convergence)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 3: Human Approval           \u2502 (Approve design before implementation)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 4: Implement                \u2502 (run.sh + analyze.py using harness)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 5: Code Review              \u2502 (5 perspectives \u2192 convergence)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 6: Run Experiments          \u2502 (Execute across seeds)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 7: Analyze &amp; Document       \u2502 (FINDINGS.md)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 8: FINDINGS Review          \u2502 (10 perspectives \u2192 convergence)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2191 (iterate rounds until convergence)\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 9: Self-Audit               \u2502 (6 dimensions, no agent)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 10: Verify + Commit + PR    \u2502 (verification gate, commit-push-pr)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key insights: 1. Three review gates at different lifecycle stages \u2014 each catches issues the others cannot 2. Human gate before implementation (Step 3) \u2014 prevents wasted experiment runs 3. Universal convergence protocol \u2014 same rules for all three gates 4. Self-audit \u2014 catches substance issues that pattern-matching agents miss</p>"},{"location":"process/hypothesis/#quick-reference","title":"Quick Reference","text":"Step Action 0. Worktree <code>/superpowers:using-git-worktrees h-&lt;name&gt;</code> 1. Classify Choose family, VV&amp;UQ category, type from experiments.md 2. Design ED-1\u2013ED-6 compliance, then 5-perspective Design Review 3. Human gate Present design for approval \u2014 pause until approved 4. Implement <code>run.sh</code> + <code>analyze.py</code> using <code>hypotheses/lib/</code> harness 5. Code Review 5-perspective code review \u2192 convergence 6. Run Execute <code>./run.sh</code> across required seeds 7. Document Write FINDINGS.md using template 8. FINDINGS Review 10-perspective review \u2192 convergence (iterate rounds) 9. Self-audit 6 dimensions of deliberate critical thinking 10. Commit + PR Verification gate (if code fixes) + <code>/commit-commands:commit-push-pr</code>"},{"location":"process/hypothesis/#step-by-step-process","title":"Step-by-Step Process","text":""},{"location":"process/hypothesis/#step-0-create-isolated-worktree","title":"Step 0: Create Isolated Worktree","text":"<p>Context: Main repo (inference-sim)</p> <p>Create an isolated workspace BEFORE any work begins.</p> <pre><code>/superpowers:using-git-worktrees h-&lt;hypothesis-name&gt;\n</code></pre> <p>This creates <code>.worktrees/h-&lt;name&gt;/</code> with a new branch. All subsequent steps happen in the worktree.</p> <p>Manual alternative: <code>git worktree add .worktrees/h-&lt;name&gt; -b h-&lt;name&gt; &amp;&amp; cd .worktrees/h-&lt;name&gt;</code></p>"},{"location":"process/hypothesis/#step-1-select-and-classify-hypothesis","title":"Step 1: Select and Classify Hypothesis","text":"<p>Context: Worktree</p> <ol> <li>Select hypothesis \u2014 from <code>docs/plans/research.md</code>, coverage gaps in hypotheses/README.md, or a new observation</li> <li>Classify:</li> <li>(a) Which family? (See experiments.md for the 6 families and sentence patterns)</li> <li>(b) Verification, Validation, or UQ? (Determines evidence requirements)</li> <li>(c) Deterministic or statistical? If statistical, which subtype (dominance, monotonicity, equivalence, Pareto)?</li> </ol> <p>The family determines design rules; the VV&amp;UQ category determines evidence requirements.</p> <p>Tip: Pose the hypothesis WITHOUT reading the code first. Code-grounded hypotheses test implementation, not behavior. See Generating Hypotheses below.</p>"},{"location":"process/hypothesis/#step-2-design-experiment-design-review","title":"Step 2: Design Experiment + Design Review","text":"<p>Context: Worktree</p> <p>Design the experiment following ED-1 through ED-6 (see experiments.md): - ED-1: Controlled comparison (vary exactly one dimension) - ED-2: Rate awareness (run at target rate AND where effect should vanish) - ED-3: Precondition verification (in script, not just prose) - ED-4: Workload seed independence - ED-5: Reproducibility (everything from <code>run.sh</code> alone) - ED-6: Config diff against referenced experiments</p> <p>Then run the 5-perspective Design Review using the universal convergence protocol.</p> <p>Primary mechanism (Claude Code): <pre><code>/convergence-review h-design\n</code></pre></p> <p>Manual alternative: Launch all 5 perspectives as parallel Task agents (or review sequentially with each checklist below), then apply the convergence protocol.</p>"},{"location":"process/hypothesis/#design-review-perspectives","title":"Design Review Perspectives","text":"<p>Perspective 1 \u2014 Hypothesis Quality: - Is the hypothesis behavioral, testable, and diagnostic? - Does it follow the family-specific sentence pattern from experiments.md? - Is the diagnostic clause present (\"If this fails, it would indicate...\")? - Is it correctly classified (family, VV&amp;UQ category, type)?</p> <p>Perspective 2 \u2014 Experiment Design Rigor (ED-1\u2013ED-6): - Is exactly one dimension varied? (ED-1) - Is there a rate where the effect should vanish, to confirm mechanism dependence? (ED-2) - Are preconditions verified in the script? (ED-3) - Is seed handling correct? (ED-4) - Is the experiment reproducible from <code>run.sh</code> alone? (ED-5) - If reusing calibration from a prior experiment, is the config diff documented? (ED-6)</p> <p>Perspective 3 \u2014 Parameter Calibration: - Are parameters computed analytically from known coefficients (alpha/beta), not guessed? - Are capacity estimates matched to the actual workload mode (CLI defaults vs workload-spec YAML)? - Is the operating point correct for the intended effect? (e.g., near saturation for queueing effects, sub-saturation for baseline)</p> <p>Perspective 4 \u2014 Control Completeness: - Does every proposed mechanism have a planned control experiment? (RCV-4) - Does each control isolate exactly one variable? - Is the baseline configuration clearly defined?</p> <p>Perspective 5 \u2014 DES and Domain Fit: - Will the experiment create the conditions needed for the hypothesis to be testable? - Are there DES-specific subtleties (event ordering, clock granularity, alpha overhead) that could confound results? - Is the experiment duration sufficient? Is the warmup period adequate?</p> <p>Cross-gate regression: If a later gate (Code Review, FINDINGS Review) discovers a design-level flaw (e.g., confounding variable, wrong operating point), the workflow loops back to Step 2 for re-design, re-convergence, and re-approval.</p>"},{"location":"process/hypothesis/#step-3-human-approval-of-experiment-design","title":"Step 3: Human Approval of Experiment Design","text":"<p>Context: Worktree (after Design Review convergence)</p> <p>Present the experiment design for human approval. The human reviews: - Hypothesis classification (family, VV&amp;UQ, type) - Experiment design (ED-1\u2013ED-6 compliance) - Parameter choices (computed from coefficients, not guessed) - Planned controls (one per proposed mechanism) - Expected outcomes and diagnostic implications</p> <p>This is a hard gate. Do not proceed to implementation until the human approves. Do not say \"I'll proceed unless you stop me.\" The pause is the point.</p> <p>In parallel mode: Each hypothesis gets its own independent human approval. The team lead presents each design; the human approves each independently.</p>"},{"location":"process/hypothesis/#step-4-implement-experiment-code","title":"Step 4: Implement Experiment Code","text":"<p>Context: Worktree (after human approval)</p> <p>Create <code>hypotheses/&lt;name&gt;/run.sh</code> and <code>hypotheses/&lt;name&gt;/analyze.py</code>.</p> <p>Harness requirements (mandatory): - <code>run.sh</code> MUST source <code>hypotheses/lib/harness.sh</code> and use <code>blis_run</code> for every simulation call - Every <code>blis_run</code> call MUST have an appropriate timeout tier (<code>TIMEOUT_QUICK</code>/<code>TIMEOUT_STANDARD</code>/<code>TIMEOUT_EXTENDED</code>) - If using <code>--total-kv-blocks</code>, call <code>preflight_kv_check</code> with max expected input tokens - <code>analyze.py</code> MUST import <code>analyze_helpers</code> and use <code>parse_blis_output</code> (handles timeouts gracefully)</p> <p>Reference comment: If reusing calibration from a prior experiment, include <code># Reference: hypotheses/&lt;name&gt;/run.sh</code> with the file path.</p>"},{"location":"process/hypothesis/#step-5-code-review-5-perspectives","title":"Step 5: Code Review (5 perspectives)","text":"<p>Context: Worktree (after implementation, BEFORE running experiments)</p> <p>Every <code>run.sh</code> and <code>analyze.py</code> must be code-reviewed BEFORE running experiments. This is non-negotiable. Three of four major bugs in PR #310 would have been caught by code review before a single experiment ran.</p> <p>Cross-gate regression: If this gate discovers a design-level flaw (e.g., confounding variable, wrong operating point), loop back to Step 2 for re-design, re-convergence, and re-approval. The experiment-wide limit of 2 cross-gate regressions applies (see Step 8 for the circuit breaker).</p> <p>Run the 5-perspective Code Review using the universal convergence protocol.</p> <p>Primary mechanism (Claude Code): <pre><code>/convergence-review h-code hypotheses/&lt;name&gt;/\n</code></pre></p> <p>Manual alternative: Launch all 5 perspectives as parallel Task agents (or review sequentially with each checklist below), then apply the convergence protocol.</p>"},{"location":"process/hypothesis/#code-review-perspectives","title":"Code Review Perspectives","text":"<p>Perspective 1 \u2014 Parser\u2013Output Format Agreement: - For every regex or field extraction in <code>analyze.py</code>, verify the pattern matches the actual output format in the simulator code - <code>cmd/root.go</code> \u2014 what text does the CLI print? (e.g., <code>\"Preemption Rate: %.4f\"</code>) - <code>sim/metrics_utils.go</code> \u2014 what JSON fields exist? - Match every regex in <code>analyze.py</code> against the format string in the producer code - Silent defaults: Verify that when a regex matches nothing, <code>analyze.py</code> emits a warning to stderr rather than silently defaulting to 0</p> <p>Perspective 2 \u2014 CLI Flag Correctness: - For every flag in <code>run.sh</code>, verify the flag name and value match <code>cmd/root.go</code> defaults and help text - Check for typos that strict parsing would reject - Cross-reference every CLI flag against <code>cmd/root.go</code> flag definitions</p> <p>Perspective 3 \u2014 YAML Field Validation: - Verify workload YAML field names against <code>sim/workload/spec.go</code> struct tags - <code>KnownFields(true)</code> will reject typos at runtime, but catching them at review saves a failed experiment run</p> <p>Perspective 4 \u2014 Config Diff (ED-6): - If the experiment reuses calibration from a prior experiment, diff every CLI flag and YAML field between the two experiments - Explicitly list differences - Verify the <code># Reference:</code> comment in <code>run.sh</code> points to the correct file</p> <p>Perspective 5 \u2014 Seed and Determinism: - Verify <code>--seed</code> is passed correctly and workload YAML <code>seed:</code> field doesn't conflict - Verify seeds vary across runs as intended (ED-4) - Check that <code>run.sh</code> builds the binary and is fully self-contained (ED-5)</p> <p>Evidence: what code review catches</p> Bug Round discovered Would code review have caught it? YAML <code>input_dist</code> vs <code>input_distribution</code> (H5) Round 1 run failure Yes \u2014 cross-ref against <code>spec.go</code> struct tags Analyzer regex <code>Preemptions?: (\\d+)</code> vs actual <code>Preemption Rate: 0.1750</code> (H10) Round 4 Yes \u2014 cross-ref against <code>cmd/root.go</code> format string H10 routing policy mismatch with H8 Round 2 Yes \u2014 ED-6 config diff H5 bucket cap=500 &lt; mean_input=512 Round 2 Possibly \u2014 first-principles check on parameters"},{"location":"process/hypothesis/#step-6-run-experiments","title":"Step 6: Run Experiments","text":"<p>Context: Worktree (after Code Review convergence)</p> <p>Execute experiments across required seeds: - Deterministic experiments: Single seed sufficient (determinism is the point) - Statistical experiments: Minimum 3 seeds (42, 123, 456) for each configuration - Verify reproducibility: running <code>./run.sh</code> twice produces identical output (ED-5)</p>"},{"location":"process/hypothesis/#step-7-analyze-and-document-findingsmd","title":"Step 7: Analyze and Document FINDINGS.md","text":"<p>Context: Worktree (after experiments complete)</p> <ol> <li>Analyze \u2014 produce comparison tables, compute effect sizes</li> <li>Verify root cause \u2014 trace every causal claim through code (RCV-1, RCV-2, RCV-3)</li> <li>Document FINDINGS.md \u2014 use the template. All sections must be present and non-empty.</li> <li>Update <code>hypotheses/README.md</code> \u2014 add a row to the \"Validated Hypotheses\" table and update \"Coverage by Family\" if needed</li> </ol>"},{"location":"process/hypothesis/#step-8-findings-review-10-perspectives","title":"Step 8: FINDINGS Review (10 perspectives)","text":"<p>Context: Worktree (after FINDINGS.md documented)</p> <p>Run the 10-perspective FINDINGS Review using the universal convergence protocol.</p> <p>Primary mechanism (Claude Code): <pre><code>/convergence-review h-findings hypotheses/&lt;name&gt;/FINDINGS.md\n</code></pre></p> <p>Manual alternative: Launch all 10 as parallel Task agents (each receives the FINDINGS.md path and its specific focus area), then apply the convergence protocol.</p> <p>Cross-gate regression: If this gate discovers a design-level flaw (e.g., confounding variable not identified in design), loop back to Step 2 for re-design, re-convergence, and re-approval. Maximum 2 cross-gate regressions per experiment (across all gates combined) \u2014 if the design still has fundamental issues after 2 regressions, suspend the experiment and escalate for a re-scoping decision.</p>"},{"location":"process/hypothesis/#findings-review-perspectives","title":"FINDINGS Review Perspectives","text":"<p>Reviewer 1 \u2014 Code Verifier: - READ the actual source files cited in the FINDINGS.md. Verify every <code>file:line</code> citation against current code. - Does the code at the cited location actually produce the claimed behavior? - Are there off-by-one errors in line citations? (Acceptable: \u00b12 lines. Flag: &gt;2 lines off.) - Does the mechanism explanation match what the code does, not just what it's named?</p> <p>Reviewer 2 \u2014 Experiment Designer: - Are there confounding variables? Is exactly one dimension varied? (ED-1) - Was the experiment run at a rate where the effect should vanish, to confirm mechanism dependence? (ED-2) - Are experiment preconditions verified in the script (e.g., queue depth &gt; batch size for SJF tests)? (ED-3) - Is workload seed handling correct? Does <code>--seed</code> on the CLI properly vary across runs? (ED-4) - Is the experiment reproducible from <code>run.sh</code> alone \u2014 binary built, seeds documented, no manual steps? (ED-5) - Is the config diff against referenced experiments documented? (ED-6) - Are there missing control experiments or confound matrix cells? - Are parameters properly calibrated? (e.g., bucket cap vs mean input) - Cross-reference every CLI flag in <code>run.sh</code> against <code>cmd/root.go</code> flag definitions. - Cross-reference every YAML field name against <code>sim/workload/spec.go</code> struct tags.</p> <p>Reviewer 3 \u2014 Statistical Rigor: - Are \"surprises\" computed from first principles? (RCV-2) - Is the sample size adequate (seeds, operating points)? - Are claims properly scoped (not over-generalized from narrow evidence)? - Is the evidence quality table complete and honest? - Do per-seed effect sizes meet the legacy thresholds (&gt;20% for dominance, &lt;5% for equivalence)? - Is the status classification consistent with the data? (e.g., \"Confirmed\" requires &gt;20% in ALL seeds.)</p> <p>Reviewer 4 \u2014 Control Experiment Auditor: - Does every proposed mechanism (RCV-3) have a corresponding control experiment (RCV-4)? - Were control experiments actually EXECUTED, not just proposed? Look for conditional language (\"one would test\", \"could be confirmed by\") vs past tense with data (\"the control showed 0.0% difference\"). Verify that control results appear in the Results section with actual numbers, not just in Root Cause Analysis as narrative. - Does each control isolate exactly one variable? Diff the CLI flags between treatment and control runs in <code>run.sh</code> \u2014 the control should differ by exactly one flag or parameter. - Do the control results confirm or refute the proposed mechanism? - Do the control experiment results in the Evidence Quality table accurately reflect the current round (not stale text from a prior round)? - Does the mechanism explain the direction using experimental evidence, not just code-reading claims?</p> <p>Reviewer 5 \u2014 Standards Compliance: - Are ALL FINDINGS.md sections present and non-empty? (per <code>docs/templates/hypothesis.md</code>) - Is the hypothesis correctly classified (family, VV&amp;UQ category, type)? - Does the Devil's Advocate section (RCV-5) argue both directions convincingly? - Are scope and limitations (RCV-6) complete \u2014 operating point, dependencies, what was NOT tested, generalizability, UQ? - Does the standards audit correctly check findings against <code>docs/standards/rules.md</code> and <code>docs/standards/invariants.md</code>? - Are any new rules or invariants warranted by the findings?</p> <p>Reviewer 6 \u2014 Substance and Logic: - Are there logical errors in the conclusions? - Are there mathematical mistakes in effect size calculations or statistical claims? - Does the evidence actually support the claims? (Not just \"the numbers are close enough\") - Are alternative explanations adequately considered?</p> <p>Reviewer 7 \u2014 DES Mechanism Expert: - Are there event-ordering subtleties that could explain the results differently? - Are assumptions about DES timing correct (alpha overhead, step quantization, clock granularity)? - Could the result be an artifact of the simulation architecture rather than the modeled system behavior?</p> <p>Reviewer 8 \u2014 Reproducibility and Robustness: - Can <code>run.sh</code> reproduce the results from scratch on a clean checkout? - Are results fragile to small parameter variations? (Would \u00b110% on key parameters change the conclusion?) - Are all intermediate files generated by the script, not checked in as stale artifacts?</p> <p>Reviewer 9 \u2014 Cross-Experiment Consistency: - Do the findings contradict any prior experiment results? If so, is the contradiction acknowledged and explained? - Are references to prior experiments accurate? (Check specific claims against referenced FINDINGS.md files) - Are there stale references to prior rounds that should have been updated?</p> <p>Reviewer 10 \u2014 User Guidance and Actionability: - Are the \"Implications for Users\" practical and specific enough to act on? - Are proposed issues (bugs, enhancements, follow-up hypotheses) well-scoped? - Would a BLIS user reading this FINDINGS.md understand what to do differently?</p> <p>The overlap between reviewers is intentional \u2014 different perspectives checking the same FINDINGS.md catch different issues. Evidence from PR #385: Reviewer 1 (code) and Reviewer 3 (rigor) both caught H19's stale evidence quality row; Reviewer 2 (design) caught H16's sub-threshold seed; Reviewer 4 (control) caught H21's unexecuted control experiments.</p>"},{"location":"process/hypothesis/#step-9-self-audit-6-dimensions","title":"Step 9: Self-Audit (6 dimensions)","text":"<p>Context: Worktree (after FINDINGS Review convergence)</p> <p>For Claude: This is NOT an agent pass. Stop, think critically, and answer each question from your own reasoning. Do NOT dispatch agents.</p> <p>Why this step exists: In PR9, 3 real bugs were found by critical thinking after 4 automated passes found 0 issues. Automated review perspectives check structure; this step checks substance.</p> <p>Self-audit dimensions \u2014 think through each one:</p> <ol> <li>Logic bugs in analyzer code: Trace through <code>analyze.py</code> mentally. Are there edge cases where regex parsing silently defaults to 0? Could integer vs float conversion produce wrong results?</li> <li>Results determinism/reproducibility: Would running <code>./run.sh</code> again produce identical output? Are there any non-deterministic dependencies (timestamps, system load, file ordering)?</li> <li>FINDINGS.md internal consistency: Does the Status match the data in the Results section? Does the Devil's Advocate section actually argue against the conclusion? Are all per-seed values consistent with the summary?</li> <li>Cross-experiment contradictions: Do these findings contradict any known BLIS behavior documented in prior experiments or MEMORY.md? If so, is the contradiction explained?</li> <li>User guidance practicality: Would a BLIS user know what to do with these findings? Are the implications actionable?</li> <li>Issue filing completeness: For each actionable finding, is there a clear issue to file? Are there findings that need issues but don't have them planned?</li> </ol> <p>Fix all issues found. Then proceed to Step 10.</p>"},{"location":"process/hypothesis/#step-10-verification-gate-commit-pr","title":"Step 10: Verification Gate + Commit + PR","text":"<p>Context: Worktree (after self-audit)</p> <p>If the experiment discovered code fixes (e.g., #386 KV livelock, #387 conservation test update), run the verification gate before committing:</p> <pre><code>go build ./...          # Build passes\ngo test ./... -count=1  # All tests pass\ngolangci-lint run ./... # Zero lint issues\n</code></pre> <p>Commit and PR:</p> <pre><code>/commit-commands:commit-push-pr\n</code></pre> <p>The PR description should include: - Hypothesis sentence and status - Key findings (1-3 bullet points) - <code>Fixes #NNN</code> for any issues this experiment addresses</p> <p>Manual alternative: Standard git commands (<code>git add</code>, <code>git commit</code>, <code>git push</code>, <code>gh pr create</code>).</p> <p>Post-PR issue filing: See Two-Track Issue Filing below.</p>"},{"location":"process/hypothesis/#universal-convergence-protocol","title":"Universal Convergence Protocol","text":"<p>Canonical source: <code>docs/process/convergence.md</code>. If this section diverges, convergence.md is authoritative.</p> <p>All three review gates (Design Review, Code Review, FINDINGS Review) use the same convergence protocol: run all N perspectives in parallel, fix any CRITICAL/IMPORTANT findings, re-run until zero CRITICAL and zero IMPORTANT in a round. Max 10 rounds per gate. See <code>docs/process/convergence.md</code> for the full protocol, severity definitions, agent failure handling, and expected convergence rates.</p> <p>Executable implementation: The <code>convergence-review</code> skill automates this protocol. Invoke with <code>/convergence-review &lt;gate-type&gt; [artifact-path] [--model opus|sonnet|haiku]</code>.</p>"},{"location":"process/hypothesis/#parallel-execution-mode","title":"Parallel Execution Mode","text":"<p>The #385/#390 pattern: run N hypothesis experiments simultaneously with a team lead coordinating.</p>"},{"location":"process/hypothesis/#setup","title":"Setup","text":"<ol> <li>Team lead creates worktree: <code>.worktrees/h-&lt;batch-name&gt;/</code></li> <li>Team lead builds binary once: <code>go build -o simulation_worker main.go</code> (agents reference this shared binary)</li> <li>Team lead creates team with N hypothesis agents, each assigned to <code>hypotheses/&lt;name&gt;/</code></li> <li>Each agent runs the full pipeline independently (Steps 1-9)</li> </ol>"},{"location":"process/hypothesis/#coordination-rules","title":"Coordination Rules","text":"<ul> <li>Each agent creates files ONLY in its own <code>hypotheses/&lt;name&gt;/</code> directory \u2014 no file conflicts</li> <li>README.md updates are deferred to the team lead's consolidation step (not done by individual agents)</li> <li>Team lead MUST independently run convergence review for each experiment. Do NOT delegate convergence assessment to the same agent that ran the experiment. Evidence from #390: agents self-reported \"Round 1 convergence\" but actual independent review found 3 CRITICAL + 18 IMPORTANT issues.</li> <li>Step 3 is a synchronization point. All agents pause at Step 3 until the human has reviewed and approved each design independently. The team lead should batch-present all designs for human review to minimize idle time.</li> <li>Solo mode is the degenerate case (team size = 1)</li> </ul>"},{"location":"process/hypothesis/#consolidation","title":"Consolidation","text":"<p>After all agents complete: 1. Team lead reviews all proposed issues for deduplication 2. Team lead updates <code>hypotheses/README.md</code> with all new experiments 3. Cross-experiment consistency check across all N experiments 4. Single PR via <code>commit-push-pr</code> skill</p>"},{"location":"process/hypothesis/#two-track-issue-filing","title":"Two-Track Issue Filing","text":""},{"location":"process/hypothesis/#immediate-track-file-as-soon-as-discovered","title":"Immediate Track (file as soon as discovered)","text":"<p>Only for bugs that affect experiment validity: - Simulator panics or crashes during experiment - Conservation violations (INV-1) discovered during analysis - Livelock or infinite loops (R19) preventing experiment completion - Data loss or silent failures affecting measured metrics</p> <p>Example: #386 (KV livelock) was filed immediately because it caused infinite preempt-requeue cycles.</p>"},{"location":"process/hypothesis/#post-convergence-track-file-after-convergence-pr-creation","title":"Post-Convergence Track (file AFTER convergence + PR creation)","text":"<p>For all findings-derived issues: - Bugs found but not affecting current experiment validity - Enhancements, design limitations, new hypotheses - Standards updates (new rules, invariants) - Promotion to Go test suite</p> <p>Why issues come last: Findings can change across rounds (H10 went from \"untested\" to \"confirmed\" between Rounds 3-4). Filing issues before convergence risks creating wrong issues. File once, file right.</p> <p>See Issue Taxonomy for the complete filing guide.</p>"},{"location":"process/hypothesis/#quality-gates","title":"Quality Gates","text":""},{"location":"process/hypothesis/#pre-execution-gates-check-before-running-experiments-step-5","title":"Pre-Execution Gates (check BEFORE running experiments \u2014 Step 5)","text":"<ul> <li> <code>run.sh</code> sources <code>hypotheses/lib/harness.sh</code> and uses <code>blis_run</code> for every simulation call</li> <li> Every <code>blis_run</code> call has an appropriate timeout tier (<code>TIMEOUT_QUICK</code>/<code>TIMEOUT_STANDARD</code>/<code>TIMEOUT_EXTENDED</code>)</li> <li> KV safety pre-flight: if experiment uses <code>--total-kv-blocks</code>, call <code>preflight_kv_check</code> with max expected input tokens</li> <li> <code>analyze.py</code> imports <code>analyze_helpers</code> and uses <code>parse_blis_output</code> (handles timeouts gracefully)</li> <li> <code>run.sh</code> flags verified against <code>cmd/root.go</code> help text</li> <li> <code>analyze.py</code> regexes verified against actual output format strings in <code>cmd/root.go</code> and <code>sim/metrics_utils.go</code></li> <li> Workload YAML field names verified against <code>sim/workload/spec.go</code> struct tags</li> <li> Config diff against referenced experiments documented (ED-6)</li> <li> Code Review (5 perspectives) converged</li> </ul>"},{"location":"process/hypothesis/#per-round-gates-check-after-each-findings-review-round-step-8","title":"Per-Round Gates (check after each FINDINGS Review round \u2014 Step 8)","text":"<ul> <li> Every causal claim cites <code>file:line</code> (RCV-1)</li> <li> Every \"surprise\" has a first-principles calculation (RCV-2)</li> <li> Root cause explains mechanism AND direction (RCV-3)</li> <li> All reviewer assessments completed and all CRITICAL/IMPORTANT items addressed</li> </ul>"},{"location":"process/hypothesis/#final-gates-check-before-pr-step-10","title":"Final Gates (check before PR \u2014 Step 10)","text":"<ul> <li> Hypothesis classified (deterministic or statistical + subtype)</li> <li> Experiment design follows ED-1 through ED-6</li> <li> If reusing prior calibration data, config diff documented (ED-6)</li> <li> Results reproducible via <code>./run.sh</code></li> <li> FINDINGS Review converged: zero CRITICAL and zero IMPORTANT items across all 10 reviewers in the current round</li> <li> Self-audit completed (6 dimensions)</li> <li> All review feedback addressed or explicitly acknowledged as open</li> <li> Findings classified per the findings table (including resolution type)</li> <li> Standards audit completed</li> <li> Promotion assessment completed (see Promotion of Confirmed Hypotheses)</li> <li> <code>hypotheses/README.md</code> updated with new experiment row(s) and coverage changes</li> <li> If code fixes involved: <code>go build</code>, <code>go test</code>, <code>golangci-lint</code> all pass</li> </ul>"},{"location":"process/hypothesis/#post-pr-gates-check-after-pr-creation-step-10","title":"Post-PR Gates (check after PR creation \u2014 Step 10)","text":"<ul> <li> Issues filed per Issue Taxonomy \u2014 one per actionable finding</li> <li> Each issue has correct label (<code>bug</code>, <code>enhancement</code>, <code>hypothesis</code>, <code>design</code>, or <code>standards</code>)</li> <li> Each issue references the PR number</li> <li> No issues filed for \"documented here\" findings with no action needed</li> </ul>"},{"location":"process/hypothesis/#when-to-run-experiments","title":"When to Run Experiments","text":"<ul> <li>Validating that a new feature works as designed (post-PR confirmation)</li> <li>Testing intuitive claims about system behavior (from <code>docs/plans/research.md</code>)</li> <li>Investigating unexpected behavior observed during development</li> <li>Exploring design tradeoffs between configurations</li> <li>Filling coverage gaps identified in the family coverage table</li> </ul>"},{"location":"process/hypothesis/#generating-hypotheses","title":"Generating Hypotheses","text":"<p>Hypotheses can come from internal sources (your own experiments and development) or external sources (user questions, literature, analytical models). This section provides structured guidance for generating good hypotheses. See also experiments.md for family-specific sentence patterns.</p>"},{"location":"process/hypothesis/#sources-of-hypotheses","title":"Sources of hypotheses","text":"Source How it works Example User intuition \"I think X should be better than Y because of Z\" \"SJF should reduce TTFT for mixed workloads because short jobs finish first\" Coverage gaps Check the family coverage table for untested families Workload/arrival family has 0 experiments \u2192 \"Gamma sampler should match theoretical CV\" Experiment findings Surprises and open questions from completed experiments spawn follow-up hypotheses H10's maybeOffload finding \u2192 \"test at GPU=1500 for preemption-path offload\" Bug reports \"This behavior seems wrong\" \u2192 formalize as a testable claim H12: preemption panic \u2192 \"conservation should hold even under preemption pressure\" Analytical models Divergence between theory and simulation \u2192 \"does the DES match M/M/k under matching assumptions?\" \"Under Poisson arrivals, queue length should match M/M/k within 5%\" Literature / external Published results about inference serving systems \"Prefix caching should reduce TTFT proportional to prefix length (as in vLLM literature)\" Design docs Claims made in design documents that have never been validated \"The composable scorer framework should produce Pareto-optimal configurations\""},{"location":"process/hypothesis/#what-makes-a-good-hypothesis","title":"What makes a good hypothesis","text":"<p>A good hypothesis is behavioral (about observable system behavior), testable (with a clear experiment), and diagnostic (failure points to something worth investigating).</p> Criterion Good Bad Behavioral \"Burst smoothing should reduce tail latency\" \"The token bucket decrements currentTokens correctly\" Testable \"TTFT should decrease monotonically as prefix_length increases\" \"The system should be fast\" Diagnostic \"If this fails, it indicates the cache eviction path has a bug\" \"If this fails, something is wrong\" Conceptual \"Tiered storage should reduce preemptions\" \"tiered.go:224 should delete the hash\" Intuitive \"More instances should roughly halve latency under saturation\" \"The event queue should process 2x events\""},{"location":"process/hypothesis/#anti-patterns-in-hypothesis-generation","title":"Anti-patterns in hypothesis generation","text":"Anti-pattern Problem Fix Code-grounded hypothesis Tests implementation, not behavior. Prevents discovery of design gaps. Pose the hypothesis WITHOUT reading the code first. Unfalsifiable hypothesis \"The system should work correctly\" \u2014 no way to fail Specify a concrete metric and direction: \"TTFT P99 should be lower for A than B\" Hypothesis that tests the obvious \"More resources should improve performance\" \u2014 trivially true Add a diagnostic clause: \"...and the improvement should be proportional to the resource increase (not sub-linear due to contention)\" Hypothesis with no failure action Confirmation and refutation both lead to \"ok, noted\" Every hypothesis should specify: \"If this fails, investigate X\" Over-scoped hypothesis \"The entire system should be correct under all configurations\" Decompose by family: scheduler invariant + structural model + robustness are separate experiments"},{"location":"process/hypothesis/#how-to-propose-a-new-hypothesis","title":"How to propose a new hypothesis","text":"<ol> <li>Check coverage: Read the family coverage table. Prioritize families with low coverage.</li> <li>Choose a family: Which domain does your claim target? (See experiments.md for the 6 families.)</li> <li>Write the sentence: Use the family-specific pattern from experiments.md.</li> <li>Add the diagnostic clause: \"If this fails, it would indicate...\"</li> <li>Check for redundancy: Search existing hypotheses in <code>docs/plans/research.md</code> and on GitHub: issues labeled <code>hypothesis</code>.</li> <li>File as a GitHub issue: Use the Hypothesis Proposal issue template on GitHub (click \"New Issue\" \u2192 \"Hypothesis Proposal\"). This template has fields for family, VV&amp;UQ category, diagnostic value, and experiment design.</li> </ol> <p>External contributors should file a GitHub issue using the Hypothesis Proposal template. Maintainers will triage, prioritize, and run the review protocol.</p>"},{"location":"process/hypothesis/#issue-taxonomy-after-convergence","title":"Issue Taxonomy (after convergence)","text":"<p>After convergence and PR creation, walk the findings classification table in FINDINGS.md and file one GitHub issue per actionable finding. Not every hypothesis produces issues \u2014 a clean confirmation (like H13) may produce none.</p> <p>Issue types and labels:</p> Issue Type Label When to file Title format Example Bug <code>--label bug</code> Code defect discovered during experiment <code>bug: &lt;component&gt; \u2014 &lt;defect&gt;</code> <code>bug: sim/simulator.go \u2014 preempt() panics on empty RunningBatch</code> (H12) Enhancement <code>--label enhancement</code> New feature, rule, or documentation improvement needed <code>enhancement: &lt;area&gt; \u2014 &lt;improvement&gt;</code> <code>enhancement: CLI \u2014 document token-bucket per-input-token cost model</code> (H5) New hypothesis <code>--label hypothesis</code> Follow-up experiment spawned by current findings <code>hypothesis: &lt;claim to test&gt;</code> <code>hypothesis: test tiered KV at GPU=1500 blocks to trigger preemption-path offload</code> (H10) Design limitation <code>--label design</code> System works as coded but has undocumented behavioral limitation <code>design: &lt;limitation&gt;</code> <code>design: no burst-smoothing sweet spot under Gamma CV&gt;3</code> (H5) Standards update <code>--label standards</code> New rule or invariant discovered that should be added <code>standards: &lt;rule/invariant&gt;</code> <code>standards: R17 signal freshness \u2014 routing signals have tiered staleness</code> (H3) Promotion <code>--label promotion</code> Confirmed hypothesis finding promoted to Go test suite <code>enhancement: promote &lt;hypothesis&gt; &lt;finding&gt; to Go test suite</code> <code>enhancement: promote H-Overload conservation under 10x to Go test suite</code> (#337) <p>Mapping from resolution type to expected issues:</p> Resolution Expected issues Clean confirmation Usually none. Optionally: promotion to Go test suite, standards update confirming existing rules. Confirmation with wrong mechanism Enhancement: update documentation with correct mechanism. Confirmation with bug discovery Bug: one per code defect. Enhancement: if detector/tooling needs improvement. Partial confirmation with surprise New hypothesis: follow-up experiments to investigate surprise. Refuted \u2014 system design flaw Design: architectural limitation. Enhancement: proposed fix. Refuted \u2014 mechanism not plausible Design: document the limitation. Enhancement: update CLI help or user docs if misleading. Refuted \u2014 wrong mental model Usually none. Optionally: enhancement if CLI help text is misleading. Inconclusive \u2014 parameter-dependent New hypothesis: test at different parameters. Converged to open question New hypothesis: specific experiment or tooling to resolve. <p>Issue body template:</p> <pre><code>## Context\nDiscovered in hypothesis experiment &lt;name&gt; (PR #NNN).\n\n## Finding\n&lt;One-paragraph description from FINDINGS.md&gt;\n\n## Evidence\n&lt;Key data point or code reference&gt;\n\n## Proposed action\n&lt;What should be done \u2014 fix, new experiment, documentation update&gt;\n</code></pre> <p>What NOT to file: - Issues for findings that are \"documented here\" with no action needed - Duplicate issues for findings already covered by existing open issues - Issues for scope limitations that are acknowledged in FINDINGS.md (these are future work, not bugs)</p>"},{"location":"process/hypothesis/#promotion-of-confirmed-hypotheses","title":"Promotion of Confirmed Hypotheses","text":"<p>After convergence, assess whether any confirmed findings should be promoted from bash-script experiments to the Go test suite and/or formal invariants. Hypothesis experiments run as bash scripts are NOT in CI \u2014 a regression would not be caught by <code>go test ./...</code>.</p>"},{"location":"process/hypothesis/#when-to-promote","title":"When to promote","text":"Condition Promote to Why Confirmed deterministic hypothesis Go test (regression protection in CI) Deterministic properties are exact \u2014 they can be encoded as pass/fail tests. Deterministic invariant aspect of a statistical hypothesis Go test for the invariant aspect Statistical hypotheses often contain deterministic sub-claims (e.g., conservation holds across all configs). New invariant discovered <code>docs/standards/invariants.md</code> entry Codify as a formal system property with verification strategy. New rule discovered <code>docs/standards/rules.md</code> entry Codify as an antipattern check for PR reviews."},{"location":"process/hypothesis/#what-a-promoted-test-looks-like","title":"What a promoted test looks like","text":"<pre><code>// TestClusterConservation_AcrossPolicyCombinations tests INV-1 at cluster level.\n// Promoted from hypothesis H12 (hypotheses/h12-conservation/).\nfunc TestClusterConservation_AcrossPolicyCombinations(t *testing.T) {\n    configs := []struct{ routing, scheduler, admission string }{\n        {\"round-robin\", \"fcfs\", \"always-admit\"},\n        {\"least-loaded\", \"fcfs\", \"always-admit\"},\n        {\"weighted\", \"priority-fcfs\", \"token-bucket\"},\n        // ... all 10 H12 configurations\n    }\n    for _, cfg := range configs {\n        t.Run(cfg.routing+\"/\"+cfg.scheduler+\"/\"+cfg.admission, func(t *testing.T) {\n            // Run cluster simulation\n            // Assert: injected == completed + still_queued + still_running\n        })\n    }\n}\n</code></pre> <p>The bash experiment remains as the full reproducible artifact with analysis. The Go test is the CI-integrated regression guard.</p>"},{"location":"process/hypothesis/#why-iterate-until-convergence-not-fixed-rounds","title":"Why Iterate Until Convergence (Not Fixed Rounds)?","text":"<p>Evidence from PR #310 (H5, H10, H13):</p> Round What happened What was caught 1 Initial experiments Wrong root causes for H5 and H10 2 Code + external review Corrected math (H5), identified mechanism (H10), designed confound matrix 3 Confound matrix + calibrated bucket H5 burst-smoothing mechanism refuted, H10 analyzer bug masked preemptions 4 Corrected analyzer H10 confirmed \u2014 preemptions DO occur, cache hits INCREASE <p>H13 converged in Round 1 (deterministic = pass/fail). H5 converged in Round 3. H10 required Round 4 due to an analyzer bug. Fixed round counts would have either stopped too early (missing the H10 bug) or forced unnecessary work (H13 didn't need Round 2).</p>"},{"location":"process/hypothesis/#why-internal-agents-beat-external-llms","title":"Why internal agents beat external LLMs","text":"Capability External (<code>/review-plan</code>) Internal (Task agent) Read source files No Yes \u2014 verifies every citation Cross-ref regexes against format strings No Yes \u2014 catches analyzer bugs Check YAML fields against struct tags No Yes \u2014 catches typos Run <code>grep</code> to verify claims No Yes \u2014 can search for executed vs proposed controls API reliability Fragile (auth, timeouts, rate limits) Reliable (same process)"},{"location":"process/hypothesis/#references","title":"References","text":"<ul> <li>Standards: docs/standards/experiments.md</li> <li>Template: docs/templates/hypothesis.md</li> <li>Hypothesis catalog: docs/plans/research.md</li> <li>Validated experiments: hypotheses/README.md</li> <li>PR workflow (structural inspiration): docs/process/pr-workflow.md</li> </ul>"},{"location":"process/hypothesis/#appendix-workflow-evolution","title":"Appendix: Workflow Evolution","text":"<p>v1.0 (PR #310): Three external LLM reviews per round, no design gate, no code review gate, ad-hoc git commands. v2.0 (2026-02-23, #392): Three review gates (Design 5, Code 5, FINDINGS 10) with universal convergence protocol, human approval gate, self-audit, verification gate, parallel execution, two-track issue filing, explicit worktree/commit skill integration. Structural alignment with PR workflow v3.0.</p>"},{"location":"process/macro-plan/","title":"Macro Plan Process","text":"<p>Status: Active (v1.0 \u2014 updated 2026-02-26)</p> <p>This document describes the process for creating a macro-level implementation plan (multi-PR feature). For the macro plan template, see docs/templates/macro-plan.md.</p>"},{"location":"process/macro-plan/#when-a-macro-plan-is-needed","title":"When a Macro Plan is Needed","text":"<ul> <li>Features spanning 2+ PRs</li> <li>Work requiring a dependency DAG between PRs</li> <li>Features touching multiple module boundaries</li> </ul> <p>Not needed for: Single-PR features, bug fixes, documentation changes.</p>"},{"location":"process/macro-plan/#steps","title":"Steps","text":"<ol> <li>Design doc(s) as input \u2014 read the relevant design doc(s) and/or GitHub issues</li> <li>Decompose into PRs \u2014 each PR should be independently mergeable and testable</li> <li>Define the dependency DAG \u2014 which PRs can be parallelized? Which must be sequential?</li> <li>Define module contracts per PR boundary \u2014 what does each PR guarantee to the next?</li> <li>Identify frozen interfaces \u2014 which interfaces are stable (can be developed against in parallel)?</li> <li>Identify flexible internals \u2014 which implementation details may change during micro-planning?</li> <li>Convergence review \u2014 <code>/convergence-review macro-plan &lt;plan-path&gt;</code> dispatches 8 parallel perspectives and enforces convergence (see docs/process/convergence.md). Manual alternative: review against each perspective checklist below.</li> <li>Human review \u2014 approve before micro-planning begins for any PR in the plan</li> </ol>"},{"location":"process/macro-plan/#macro-plan-review-perspectives-8","title":"Macro Plan Review Perspectives (8)","text":"<p>For each perspective, check every item. Classify findings as CRITICAL / IMPORTANT / SUGGESTION per the convergence protocol. Section references below refer to design-guidelines.md and macro-plan template unless otherwise noted.</p> <p>Perspective 1 \u2014 Objective Clarity: - Are 3-7 crisp objectives defined? - Are non-goals explicitly listed? - Is the model scoping table present (modeled / simplified / omitted / justification)? - Are analysis questions specific enough to drive component selection?</p> <p>Perspective 2 \u2014 Concept Model Quality: - Is the concept model under 80 lines? - Does every building block have all 6 module contract aspects (observes, controls, owns, invariants, events, extension friction)? - Is real-system correspondence documented (llm-d / vLLM / SGLang mapping table)? - Is the state ownership map complete (exactly one owner per mutable state)?</p> <p>Perspective 3 \u2014 PR Decomposition: - Is every PR independently mergeable and testable? - Does the dependency DAG have no cycles? - Can module contracts be tested with mocks (parallel development enabled)? - Does each PR identify its extension type (policy template / subsystem module / backend swap / tier composition)?</p> <p>Perspective 4 \u2014 Abstraction Level: - Zero Go code in Sections A-F and H-K (only Section G may have frozen interface signatures)? - Are all pre-freeze interfaces described behaviorally, not as Go code? - Is every code snippet a FACT about merged code, not an ASPIRATION about unwritten code? - Are module contracts using the template from Phase 2, not Go structs?</p> <p>Perspective 5 \u2014 Risk Register: - Does every non-obvious architectural decision have a risk entry? - For decisions with cost-of-being-wrong &gt;= 3 PRs, is validation MANDATORY with a specific gate? - Does each validation gate have exact success criteria (not \"looks good\")? - Are abort plans specified (what changes if validation fails)?</p> <p>Perspective 6 \u2014 Cross-Cutting Infrastructure: - Are test infrastructure, documentation, and CI changes each assigned to a specific PR? - Is the interface freeze schedule documented (which PR freezes which interface)? - Is CLAUDE.md update ownership clear (the PR that causes the change updates it)? - Are no items left as \"address when needed\"?</p> <p>Perspective 7 \u2014 Extension Friction: - For each new module boundary, is the touch-point count for adding one more variant specified? - Are touch-point counts within reference targets from design guidelines Section 4.5? - If friction exceeds targets, is this acknowledged and justified?</p> <p>Perspective 8 \u2014 Design Bug Prevention: - Is scaffolding creep prevented (every struct/method/flag exercised by end of introducing PR)? - Is documentation drift prevented (CLAUDE.md updated in the same PR that causes the change)? - Is test infrastructure duplication prevented (shared packages created early)? - Is golden dataset staleness prevented (regeneration steps included)? - Are DES-specific anti-patterns addressed (type catalog, fidelity for its own sake, golden without invariant)?</p>"},{"location":"process/macro-plan/#quality-gates","title":"Quality Gates","text":"<ul> <li> Every PR in the plan is independently mergeable (no PR requires another PR's uncommitted code)</li> <li> Dependency DAG has no cycles</li> <li> Module contracts are testable with mocks (parallel development enabled)</li> <li> No Go struct definitions or method implementations (those belong in micro plans)</li> <li> Extension friction assessed for each new module boundary</li> </ul>"},{"location":"process/macro-plan/#prerequisites","title":"Prerequisites","text":"Skill Purpose Manual Alternative <code>convergence-review</code> Dispatch parallel review perspectives (Step 7) Review against each perspective checklist above"},{"location":"process/macro-plan/#references","title":"References","text":"<ul> <li>Template: docs/templates/macro-plan.md</li> <li>Design guidelines: docs/templates/design-guidelines.md</li> <li>Convergence protocol: docs/process/convergence.md</li> <li>Standards: docs/standards/rules.md</li> </ul>"},{"location":"process/pr-workflow/","title":"PR Development Workflow","text":"<p>Status: Active (v3.0 - updated 2026-02-23)</p>"},{"location":"process/pr-workflow/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Current Template Versions</li> <li>Prerequisites</li> <li>Overview</li> <li>Quick Reference: Simplified Invocations</li> <li>Step-by-Step Process</li> <li>Step 1: Create Isolated Worktree</li> <li>Step 2: Create Implementation Plan</li> <li>Step 2.5: Multi-Perspective Plan Review</li> <li>Step 3: Human Review of Plan</li> <li>Step 4: Execute Plan</li> <li>Step 4.5: Multi-Perspective Code Review</li> <li>Step 4.75: Pre-Commit Self-Audit</li> <li>Step 5: Commit, Push, and Create PR</li> <li>Workflow Variants</li> <li>PR Size Tiers</li> <li>Skill Reference Quick Guide</li> <li>Example A: Macro Plan PR</li> <li>Example B: Issue/Design-Doc PR</li> <li>Tips for Success</li> <li>Common Issues and Solutions</li> <li>Appendix: Workflow Evolution</li> </ul> <p>This document describes the complete workflow for implementing a PR from any source: a macro plan section, GitHub issues, a design document, or a feature request.</p>"},{"location":"process/pr-workflow/#current-template-versions","title":"Current Template Versions","text":"<p>Update this section when templates change. All examples below reference these versions.</p> <ul> <li>Design guidelines: <code>docs/templates/design-guidelines.md</code> \u2014 DES foundations, module architecture, extension framework. Read before writing any design doc or macro plan.</li> <li>Macro-planning template: <code>docs/templates/macro-plan.md</code> (updated 2026-02-18 \u2014 aligned with design guidelines)</li> <li>Micro-planning template: <code>docs/templates/micro-plan.md</code> (updated 2026-02-18)</li> <li>Active macro plan: <code>docs/plans/2026-02-19-weighted-scoring-macro-plan.md</code> (scorer framework)</li> <li>Archived design docs: <code>docs/plans/archive/</code> (completed design docs for reference)</li> </ul>"},{"location":"process/pr-workflow/#prerequisites","title":"Prerequisites","text":"<p>This workflow requires the following Claude Code skills to be available:</p> Skill Purpose Used In <code>commit-commands:clean_gone</code> Clean up stale branches before worktree creation Step 1 (pre-cleanup) <code>superpowers:using-git-worktrees</code> Create isolated workspace for PR work Step 1 <code>superpowers:writing-plans</code> Generate implementation plan from templates Step 2 <code>pr-review-toolkit:review-pr</code> Holistic cross-cutting review (pre-pass) Step 2.5, Step 4.5 <code>convergence-review</code> Dispatch parallel perspectives and enforce convergence Step 2.5, Step 4.5 <code>superpowers:executing-plans</code> Execute plan tasks continuously Step 4 <code>superpowers:systematic-debugging</code> Structured root-cause analysis on failure Step 4 (on failure) <code>superpowers:verification-before-completion</code> Enforced verification gate before commit Step 4.5 (after passes) <code>commit-commands:commit-push-pr</code> Commit, push, and create PR Step 5 <p>Verification: <pre><code># List all available skills\n/agents\n</code></pre></p> <p>If a required skill is unavailable: - Check Claude Code version (skills may be added in newer versions) - Check skill installation in <code>~/.claude/plugins/</code> or <code>~/.claude/skills/</code> - For official superpowers skills, ensure plugins are up to date</p> <p>Alternative workflows: If skills are unavailable, you can implement each step manually: - Step 1: Use <code>git worktree add ../repo-prN -b prN-name</code> directly - Step 2: Follow <code>docs/templates/micro-plan.md</code> template manually - Step 2.5/4.5: Manual code review or skip automated review - Step 4: Implement tasks manually following plan; on failure, debug manually - Step 4.75: Self-audit is always available (no skill required \u2014 just critical thinking) - Step 5: Use standard git commands (<code>git add</code>, <code>git commit</code>, <code>git push</code>, <code>gh pr create</code>)</p>"},{"location":"process/pr-workflow/#overview","title":"Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 1: using-worktrees \u2502 (Create isolated workspace for PR)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 2: writing-plans   \u2502 (Create behavioral contracts + executable tasks)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 2.5: plan review   \u2502 (review-pr pre-pass \u2192 convergence-review)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 3: Human Review    \u2502 (Approve plan)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 4: executing-plans \u2502 (Implement tasks continuously)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 4.5: code review   \u2502 (review-pr pre-pass \u2192 convergence-review)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 4.75: self-audit   \u2502 (Deliberate critical thinking \u2014 no agent)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 5: commit-push-pr  \u2502 (Commit, push, create PR - all in one)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key insights: 1. Worktree isolation from start (Step 1) - Create worktree BEFORE any work    - Entire PR lifecycle (planning + implementation) in isolated workspace    - Main worktree never touched    - Enables parallel work on multiple PRs</p> <ol> <li>Three-stage quality assurance:</li> <li>Plan Review (Step 2.5) - two-stage: holistic <code>review-pr</code> pre-pass, then <code>convergence-review</code> with 10 targeted perspectives<ul> <li>Catches design issues before implementation</li> </ul> </li> <li>Code Review (Step 4.5) - two-stage: holistic <code>review-pr</code> pre-pass, then <code>convergence-review</code> with 10 targeted perspectives<ul> <li>Catches implementation issues before PR creation</li> </ul> </li> <li>Self-Audit (Step 4.75) - Deliberate critical thinking across 10 dimensions<ul> <li>Catches substance bugs that pattern-matching agents miss</li> </ul> </li> </ol>"},{"location":"process/pr-workflow/#quick-reference-simplified-invocations","title":"Quick Reference: Simplified Invocations","text":"<p>No copy-pasting required! Use @ file references and simple commands:</p> Step Command 1. Create worktree <code>/superpowers:using-git-worktrees pr&lt;N&gt;-&lt;name&gt;</code> 2. Create plan <code>/superpowers:writing-plans for &lt;work-item&gt; in @docs/plans/&lt;name&gt;-plan.md using @docs/templates/micro-plan.md and @&lt;source-document&gt;</code> 2.5. Review plan <code>/pr-review-toolkit:review-pr</code> then <code>/convergence-review pr-plan docs/plans/pr&lt;N&gt;-&lt;name&gt;-plan.md</code> 3. Human review plan Review contracts, tasks, appendix, then approve to proceed 4. Execute plan <code>/superpowers:executing-plans @docs/plans/pr&lt;N&gt;-&lt;name&gt;-plan.md</code> 4.5. Review code <code>/pr-review-toolkit:review-pr</code> then <code>/convergence-review pr-code</code> 4.75. Self-audit Deliberate critical thinking: logic, design, determinism, consistency, docs, edge cases, test epistemology, construction sites, error paths, docs DRY 5. Commit, push, PR <code>/commit-commands:commit-push-pr</code> <p>Example for PR 8 (same-session workflow with <code>.worktrees/</code>):</p> <p>Note: Examples below use file paths from completed PRs. Referenced plan files may have been archived or removed. Adapt the pattern using current plans from <code>docs/plans/</code>. <pre><code># Step 1: Create worktree (stays in same session)\n/superpowers:using-git-worktrees pr8-routing-state-and-policy-bundle\n\n# Output: Worktree ready at .worktrees/pr8-routing-state-and-policy-bundle/\n# (shell cwd already switched \u2014 continue directly)\n\n# Step 2: Create plan\n/superpowers:writing-plans for PR8 in @docs/plans/pr8-routing-state-and-policy-bundle-plan.md using @docs/templates/micro-plan.md and @docs/plans/2026-02-11-macro-implementation-plan-v2.md\n\n# Step 2.5: Review plan (two-stage)\n/pr-review-toolkit:review-pr\n/convergence-review pr-plan docs/plans/pr8-routing-state-and-policy-bundle-plan.md  # [--model opus|sonnet|haiku]\n\n# Step 3: Human review\n# [Read plan, verify contracts and tasks, approve to proceed]\n\n# Step 4: Execute implementation\n/superpowers:executing-plans @docs/plans/pr8-routing-state-and-policy-bundle-plan.md\n\n# Step 4.5: Review code (two-stage)\n/pr-review-toolkit:review-pr\n/convergence-review pr-code  # [--model opus|sonnet|haiku]\n\n# Step 5: Commit, push, and create PR\n/commit-commands:commit-push-pr\n</code></pre></p> <p>Example (separate-session workflow with sibling directory): <pre><code># Step 1: Create worktree\n/superpowers:using-git-worktrees pr6-routing-policy\n# Output: Worktree created at ../inference-sim-pr6/\n\n# Open NEW Claude Code session in worktree:\n# Terminal: cd ../inference-sim-pr6/ &amp;&amp; claude\n\n# Steps 2-5: Same as above, in the new session\n</code></pre></p>"},{"location":"process/pr-workflow/#step-by-step-process","title":"Step-by-Step Process","text":""},{"location":"process/pr-workflow/#step-1-create-isolated-worktree-using-using-git-worktrees-skill","title":"Step 1: Create Isolated Worktree Using <code>using-git-worktrees</code> Skill","text":"<p>Context: Main repo (inference-sim)</p> <p>Pre-cleanup (optional): Run <code>commit-commands:clean_gone</code> to remove stale local branches whose remote tracking branches have been deleted. This prevents <code>.worktrees/</code> from accumulating cruft from previous PRs.</p> <pre><code>/commit-commands:clean_gone\n</code></pre> <p>Skill: <code>superpowers:using-git-worktrees</code></p> <p>Why first? Create isolated workspace BEFORE any work begins. This ensures: - Main worktree stays clean (no uncommitted plans or code) - Plan document committed on feature branch (not main) - Complete isolation for entire PR lifecycle (planning + implementation) - Ability to work on multiple PRs in parallel</p> <p>Invocation (simplified): <pre><code>/superpowers:using-git-worktrees pr&lt;N&gt;-&lt;feature-name&gt;\n</code></pre></p> <p>Example: <pre><code>/superpowers:using-git-worktrees pr6-routing-policy\n</code></pre></p> <p>What Happens: - Creates a new git worktree (project-local in <code>.worktrees/</code> or as a sibling directory) - Creates and checks out a new branch (<code>pr6-routing-policy</code>) - Shell working directory switches to the worktree - Isolates work from main development</p> <p>Output: Path to worktree directory (e.g., <code>.worktrees/pr6-routing-policy/</code>)</p> <p>Next \u2014 choose one:</p> <ul> <li> <p>Continue in same session (recommended for <code>.worktrees/</code>): The skill already switched your working directory into the worktree. You can proceed directly to Step 2 in the same Claude Code session.</p> </li> <li> <p>Open a new session (required for sibling directories): If the worktree is outside the project (e.g., <code>../inference-sim-pr6/</code>), open a new Claude Code session there:   <pre><code>cd ../inference-sim-pr6/\nclaude\n</code></pre></p> </li> </ul> <p>All remaining steps happen in the worktree (same session or new session).</p>"},{"location":"process/pr-workflow/#step-2-create-implementation-plan-using-writing-plans-skill","title":"Step 2: Create Implementation Plan Using <code>writing-plans</code> Skill","text":"<p>Context: Worktree (same or new session)</p> <p>Skill: <code>superpowers:writing-plans</code></p> <p>Invocation (simplified): <pre><code>/superpowers:writing-plans for &lt;work-item&gt; in @docs/plans/&lt;name&gt;-plan.md using @docs/templates/micro-plan.md and @&lt;source-document&gt;\n</code></pre></p> <p>The <code>&lt;source-document&gt;</code> can be any of: - A macro plan: <code>@docs/plans/&lt;active-macro-plan&gt;.md</code> (e.g., <code>@docs/plans/2026-02-19-weighted-scoring-macro-plan.md</code>) - A design document: <code>@docs/plans/&lt;design-doc&gt;.md</code> or <code>@docs/plans/archive/&lt;design-doc&gt;.md</code> - GitHub issues: reference by number in the prompt text (e.g., \"for issues #183, #189, #195\")</p> <p>Examples: <pre><code># From macro plan section:\n/superpowers:writing-plans for PR6 in @docs/plans/pr6-routing-policy-plan.md using @docs/templates/micro-plan.md and @docs/plans/2026-02-11-macro-implementation-plan-v2.md\n\n# From design document:\n/superpowers:writing-plans for hardening PR in @docs/plans/hardening-plan.md using @docs/templates/micro-plan.md and @docs/plans/2026-02-18-hardening-antipattern-refactoring-design.md\n\n# From GitHub issues:\n/superpowers:writing-plans for issues #183 #189 #195 in @docs/plans/kv-bugfix-plan.md using @docs/templates/micro-plan.md\n</code></pre></p> <p>What Happens: - Claude reads the source document (macro plan section, design doc, or issue descriptions) - Claude reads <code>docs/templates/micro-plan.md</code> as the template - Claude inspects the codebase (Phase 0: Component Context) - Claude creates behavioral contracts (Phase 1) - Claude breaks implementation into 6-12 TDD tasks (Phase 4) - Claude saves plan to the specified output file in the worktree</p> <p>Output: - Plan file at <code>docs/plans/&lt;name&gt;-plan.md</code> (in worktree, on feature branch) - Contains: behavioral contracts, executable tasks, test strategy, appendix</p> <p>Tips: - Use @ file references instead of copy-pasting - Claude automatically extracts the relevant context from the source document - Template structure is preserved automatically - For issue-based work, Claude reads issue details via <code>gh issue view</code></p>"},{"location":"process/pr-workflow/#step-25-multi-perspective-plan-review-rounds","title":"Step 2.5: Multi-Perspective Plan Review (Rounds)","text":"<p>Context: Worktree (same or new session)</p> <p>Two-stage review (Claude Code):</p> <p>Stage 1 \u2014 Holistic pre-pass: Run a single deep review to catch cross-cutting issues before the formal convergence protocol. <pre><code>/pr-review-toolkit:review-pr\n</code></pre> Fix any issues found, then proceed to Stage 2.</p> <p>Stage 2 \u2014 Formal convergence: Dispatch 10 targeted perspectives in parallel with convergence enforcement. <pre><code>/convergence-review pr-plan docs/plans/pr&lt;N&gt;-&lt;name&gt;-plan.md\n</code></pre></p> <p>The <code>convergence-review</code> skill dispatches all 10 perspectives in parallel, tallies findings independently, and enforces the re-run gate. See docs/process/convergence.md for the protocol rules.</p> <p>Why two stages? <code>review-pr</code> does a holistic sweep that catches emergent cross-cutting issues (the kind a human reviewer would spot). Fixing those first means the convergence review starts from a cleaner baseline \u2014 fewer rounds needed because obvious issues are already addressed.</p> <p>For Claude (manual alternative): When running without the skill, run all 10 perspectives below in parallel as a single round. Use <code>/pr-review-toolkit:review-pr</code> with the exact prompt text shown for Perspectives 1-4 and 6-10. Perform Perspective 5 directly (no agent). Collect all findings, then report the full round results to the user.</p> <p>If 0 CRITICAL and 0 IMPORTANT findings: The round converged. Proceed to Step 3.</p> <p>If any CRITICAL or IMPORTANT findings: Fix all issues, then re-run the entire round from scratch. Repeat until a round produces 0 CRITICAL and 0 IMPORTANT findings.</p> <p>Why rounds with multiple perspectives? Generic \"review everything\" misses issues that targeted perspectives catch. Different lenses find different bugs: cross-doc consistency catches stale references, architecture catches boundary violations, substance catches design bugs. Running them in parallel maximizes coverage per round. The hypothesis process proved this model: 3 parallel reviewers with different foci caught issues that sequential single-reviewer rounds missed.</p> <p>For convergence rules (max rounds, re-run requirements, severity definitions), see Universal Convergence Protocol.</p>"},{"location":"process/pr-workflow/#perspective-1-substance-design-review","title":"Perspective 1: Substance &amp; Design Review","text":"<p>Catch design bugs, mathematical errors, and logical flaws that structural checks miss.</p> <p>Prompt: <pre><code>/pr-review-toolkit:review-pr Review this plan for substance: Are the behavioral contracts logically sound? Are there mathematical errors, scale mismatches, or unit confusions? Could the design actually achieve what the contracts promise? Check formulas, thresholds, and edge cases from first principles \u2014 not just structural completeness. @docs/plans/&lt;name&gt;-plan.md\n</code></pre></p> <p>Catches: Design bugs, mathematical errors, logical inconsistencies, scale mismatches, missing edge cases.</p> <p>Why this perspective exists: In PR9, the fitness normalization formula (<code>1/(1+value)</code>) passed all structural checks but was a design bug (500,000x scale imbalance between throughput and latency). A substance-focused review caught what structure-focused reviews missed.</p>"},{"location":"process/pr-workflow/#perspective-2-cross-document-consistency","title":"Perspective 2: Cross-Document Consistency","text":"<p>Verify the micro plan matches the source document and both match the codebase.</p> <p>Prompt: <pre><code>/pr-review-toolkit:review-pr Does the micro plan's scope match the source document? Are file paths consistent? Does the deviation log account for all differences between what the source says and what the micro plan does? Check for stale references. @docs/plans/&lt;name&gt;-plan.md and @&lt;source-document&gt;\n</code></pre></p> <p>For issue-based work, replace <code>@&lt;source-document&gt;</code> with the issue numbers in the prompt text.</p> <p>Catches: Stale references, scope mismatch, missing deviations, wrong file paths.</p>"},{"location":"process/pr-workflow/#perspective-3-architecture-boundary-verification","title":"Perspective 3: Architecture Boundary Verification","text":"<p>Verify the plan respects architectural boundaries and separation of concerns.</p> <p>Prompt: <pre><code>/pr-review-toolkit:review-pr Does this plan maintain architectural boundaries? Are we ensuring individual instances don't have access to cluster-level state? Are types in the right packages? Check the plan against the actual code for boundary violations. Also check: (1) does the plan introduce multiple construction sites for the same type? (2) does adding one field to a new type require &gt;3 files? (3) does library code (sim/) call logrus.Fatalf anywhere in the new code? @docs/plans/pr&lt;N&gt;-&lt;name&gt;-plan.md\n</code></pre></p> <p>Catches: Import cycle risks, boundary violations, missing bridge types, wrong abstraction level, construction site proliferation, high touch-point multipliers, error handling boundary violations.</p>"},{"location":"process/pr-workflow/#perspective-4-codebase-readiness","title":"Perspective 4: Codebase Readiness","text":"<p>Verify the files to be modified are clean and ready for the planned changes.</p> <p>Prompt: <pre><code>/pr-review-toolkit:review-pr We're about to implement this PR. Review the codebase for readiness. Check each file the plan will modify for stale comments, existing bugs, or issues that would complicate implementation. @docs/plans/pr&lt;N&gt;-&lt;name&gt;-plan.md\n</code></pre></p> <p>Catches: Stale comments (\"planned for PR N\"), pre-existing bugs, missing dependencies, unclear insertion points.</p>"},{"location":"process/pr-workflow/#perspective-5-plan-structural-validation","title":"Perspective 5: Plan Structural Validation","text":"<p>Verify the plan is complete, internally consistent, and implementation-ready.</p> <p>For Claude: Perform these 4 checks directly (no agent needed). Report all issues found.</p> <p>Check 1: Task Dependencies - For each task, verify it can actually start given what comes before it. - Trace the dependency chain: what files does each task create/modify? Does any task require a file or type that hasn't been created yet? - Flag tasks that modify the same file and could conflict.</p> <p>Check 2: Template Completeness - Verify all sections from <code>docs/templates/micro-plan.md</code> are present and non-empty:   - Header (Goal, Architecture, Source Reference)   - Part 1: A) Executive Summary, B) Behavioral Contracts, C) Component Interaction, D) Deviation Log, E) Review Guide   - Part 2: F) Implementation Overview, G) Task Breakdown, H) Test Strategy, I) Risk Analysis   - Part 3: J) Sanity Checklist   - Appendix: File-Level Details</p> <p>Check 3: Executive Summary Clarity - Read the executive summary as if you're a new team member with no context. - Is it clear what the PR does and why? - Can you understand the scope without reading the rest of the plan?</p> <p>Check 4: Under-specified Tasks - For each task, verify it has complete code in every step (no \"add validation\" without showing exact code). - Verify exact test commands with expected output. - Verify exact commit commands. - Flag any step that an executing agent would need to figure out on its own.</p> <p>Catches: Broken task ordering, missing template sections, unclear summaries, vague implementation steps that will cause agent confusion.</p>"},{"location":"process/pr-workflow/#perspective-6-des-expert-review","title":"Perspective 6: DES Expert Review","text":"<p>Catch event-driven simulation bugs that domain-agnostic reviewers miss.</p> <p>Prompt: <pre><code>/pr-review-toolkit:review-pr Review this plan as a discrete-event simulation expert. Check for: event ordering bugs, clock monotonicity violations, stale signal propagation between event types, heap priority errors, event-driven race conditions, and incorrect assumptions about DES event processing semantics. Verify that any new events respect the (timestamp, priority, seqID) ordering contract.\n</code></pre></p> <p>Catches: Event ordering violations, clock regression, stale-signal bugs, priority inversion in event queues.</p>"},{"location":"process/pr-workflow/#perspective-7-vllmsglang-expert-review","title":"Perspective 7: vLLM/SGLang Expert Review","text":"<p>Catch mismatches between BLIS's model and real inference server behavior.</p> <p>Prompt: <pre><code>/pr-review-toolkit:review-pr Review this plan as a vLLM/SGLang inference serving expert. Check for: batching semantics that don't match real continuous-batching servers, KV cache eviction policies that differ from vLLM's implementation, chunked prefill behavior mismatches, preemption policy differences, and missing scheduling features that real servers have. Flag any assumption about LLM serving that this plan gets wrong.\n</code></pre></p> <p>Catches: Batching model inaccuracies, KV cache behavior mismatches, prefill/decode pipeline errors, scheduling assumption violations.</p>"},{"location":"process/pr-workflow/#perspective-8-distributed-inference-platform-expert-review","title":"Perspective 8: Distributed Inference Platform Expert Review","text":"<p>Catch multi-instance coordination and routing issues.</p> <p>Prompt: <pre><code>/pr-review-toolkit:review-pr Review this plan as a distributed inference platform expert (llm-d, KServe, vLLM multi-node). Check for: multi-instance coordination bugs, routing load imbalance under high request rates, stale snapshot propagation between instances, admission control edge cases at scale, horizontal scaling assumption violations, and prefix-affinity routing correctness across instances.\n</code></pre></p> <p>Catches: Load imbalance, stale routing state, admission control failures, scaling assumption violations, cross-instance coordination bugs.</p>"},{"location":"process/pr-workflow/#perspective-9-performance-scalability-analyst","title":"Perspective 9: Performance &amp; Scalability Analyst","text":"<p>Catch algorithmic and memory efficiency issues.</p> <p>Prompt: <pre><code>/pr-review-toolkit:review-pr Review this plan as a performance and scalability analyst. Check for: algorithmic complexity issues (O(n\u00b2) where O(n) suffices), unnecessary allocations in hot paths, map iteration in O(n) loops that could grow, benchmark-sensitive changes, memory growth patterns, and any changes that would degrade performance at 1000+ requests or 10+ instances.\n</code></pre></p> <p>Catches: Algorithmic complexity regressions, hot-path allocations, memory growth, scalability bottlenecks.</p>"},{"location":"process/pr-workflow/#perspective-10-security-robustness-reviewer","title":"Perspective 10: Security &amp; Robustness Reviewer","text":"<p>Catch input validation gaps and failure mode issues.</p> <p>Prompt: <pre><code>/pr-review-toolkit:review-pr Review this plan as a security and robustness reviewer. Check for: input validation completeness (all CLI flags, YAML fields, config values), panic paths reachable from user input, resource exhaustion vectors (unbounded loops, unlimited memory growth), degenerate input handling (empty, zero, negative, NaN, Inf), and configuration injection risks.\n</code></pre></p> <p>Catches: Input validation gaps, user-reachable panics, resource exhaustion, degenerate input failures, injection risks.</p>"},{"location":"process/pr-workflow/#step-3-human-review-of-plan","title":"Step 3: Human Review of Plan","text":"<p>Context: Worktree (same or new session)</p> <p>Action: Final human review of the plan (after automated review)</p> <p>Focus Areas: 1. Part 1 (Design Validation) - Review behavioral contracts, component interaction, risks 2. Part 2 (Executable Tasks) - Verify task breakdown makes sense, no dead code 3. Deviation Log - Check if deviations from source document are justified 4. Appendix - Spot-check file-level details for accuracy</p> <p>Common Issues to Catch: - Behavioral contracts too vague or missing edge cases - Tasks not properly ordered (dependencies) - Missing test coverage for contracts - Deviations from source document not justified - Dead code or scaffolding</p> <p>Outcome: - \u2705 Approve plan \u2192 proceed to Step 4 (implementation) - \u274c Need revisions \u2192 iterate with Claude, re-review (Step 2.5), then approve</p> <p>Note: The plan will be committed together with the implementation in Step 5 (single commit for entire PR).</p>"},{"location":"process/pr-workflow/#step-4-execute-plan-using-executing-plans-skill","title":"Step 4: Execute Plan Using <code>executing-plans</code> Skill","text":"<p>Context: Worktree (same or new session)</p> <p>Skill: <code>superpowers:executing-plans</code></p> <p>Invocation (simplified): <pre><code>/superpowers:executing-plans @docs/plans/pr&lt;N&gt;-&lt;feature-name&gt;-plan.md\n</code></pre></p> <p>Example: <pre><code>/superpowers:executing-plans @docs/plans/pr6-routing-policy-plan.md\n</code></pre></p> <p>That's it! The skill automatically: - Reads the plan - Executes all tasks continuously (no pausing for human input) - Stops only on test failure, lint failure, or build error</p> <p>What Happens: - Claude reads the plan file - Claude executes all tasks sequentially without pausing   - Each task: write test \u2192 verify fail \u2192 implement \u2192 verify pass \u2192 lint \u2192 commit - If a failure occurs, Claude stops and reports the issue - On success, all tasks complete and Claude reports a summary</p> <p>On Failure: If a task fails (test failure, build error, lint error), invoke structured debugging instead of ad-hoc poking:</p> <pre><code>/superpowers:systematic-debugging\n</code></pre> <p>This skill provides structured root-cause analysis: reproduce \u2192 isolate \u2192 hypothesize \u2192 verify \u2192 fix. Prevents the common pattern of making random changes hoping the test passes. After the fix, resume execution from the failing task.</p> <p>Output: - Implemented code in working directory - All tests passing (<code>go test ./...</code>) - Lint clean (<code>golangci-lint run ./...</code>) - Commits for each task (referencing contracts)</p>"},{"location":"process/pr-workflow/#step-45-multi-perspective-code-review-rounds","title":"Step 4.5: Multi-Perspective Code Review (Rounds)","text":"<p>Context: Worktree (after implementation complete)</p> <p>Two-stage review (Claude Code):</p> <p>Stage 1 \u2014 Holistic pre-pass: Run a single deep review to catch cross-cutting issues before the formal convergence protocol. <pre><code>/pr-review-toolkit:review-pr\n</code></pre> Fix any issues found, then proceed to Stage 2.</p> <p>Stage 2 \u2014 Formal convergence: Dispatch 10 targeted perspectives in parallel with convergence enforcement. <pre><code>/convergence-review pr-code\n</code></pre></p> <p>The <code>convergence-review</code> skill dispatches all 10 perspectives in parallel, tallies findings independently, and enforces the re-run gate. See docs/process/convergence.md for the protocol rules.</p> <p>Why two stages? <code>review-pr</code> does a holistic sweep that catches emergent cross-cutting issues. In past PRs, this pre-pass found issues (runtime-breaking regressions, stale panic message prefixes) that individual targeted perspectives missed because they were each focused on their narrow lens. Fixing those first reduces convergence rounds.</p> <p>For Claude (manual alternative): When running without the skill, run all 10 perspectives below in parallel as a single round. Use <code>/pr-review-toolkit:review-pr</code> with the exact prompt text shown for each perspective. Collect all findings, then report the full round.</p> <p>If 0 CRITICAL and 0 IMPORTANT findings: The round converged. Run verification gate.</p> <p>If any CRITICAL or IMPORTANT findings: Fix all issues, then re-run the entire round from scratch. Repeat until convergence (see Universal Convergence Protocol).</p> <p>Why 10 perspectives in parallel? Each catches issues the others miss. In the standards-audit-hardening PR, Perspective 1 (substance) found a runtime-breaking regression, Perspective 3 (tests) found weakened coverage, Perspective 7 (vLLM expert) confirmed CLI validation matches real server semantics, and Perspective 10 (security) found pre-existing factory validation gaps. Domain-specific perspectives (DES, vLLM, distributed platform) catch issues that generic code-quality reviewers miss.</p>"},{"location":"process/pr-workflow/#perspective-1-substance-design","title":"Perspective 1: Substance &amp; Design","text":"<p>Catch logic bugs, design mismatches, and mathematical errors in the implementation.</p> <p>Prompt: <pre><code>/pr-review-toolkit:review-pr Review this diff for substance: Are there logic bugs, design mismatches between contracts and implementation, mathematical errors, or silent regressions? Check from first principles \u2014 not just structural patterns. Does the implementation actually achieve what the behavioral contracts promise?\n</code></pre></p> <p>Catches: Design bugs, formula errors, silent regressions, semantic mismatches between intent and implementation.</p>"},{"location":"process/pr-workflow/#perspective-2-code-quality-error-handling","title":"Perspective 2: Code Quality + Error Handling","text":"<p>Find bugs, logic errors, silent failures, and convention violations.</p> <p>Prompt: <pre><code>/pr-review-toolkit:review-pr Also check: (1) any new error paths that use `continue` or early `return` \u2014 do they clean up partial state? (2) any map iteration that accumulates floats \u2014 are keys sorted? (3) any struct field added \u2014 are all construction sites updated? (4) does library code (sim/) call logrus.Fatalf anywhere in new code? (5) any exported mutable maps \u2014 should they be unexported with IsValid*() accessors? (6) any YAML config fields using float64 instead of *float64 where zero is valid? (7) any division where the denominator derives from runtime state without a zero guard? (8) any new interface with methods only meaningful for one implementation? (9) any method &gt;50 lines spanning multiple concerns (scheduling + latency + metrics)? (10) any changes to docs/standards/ files \u2014 are CLAUDE.md working copies updated to match?\n</code></pre></p> <p>Catches: Logic errors, nil pointer risks, silent failures (discarded return values), panic paths reachable from user input, CLAUDE.md convention violations, dead code, silent <code>continue</code> data loss, non-deterministic map iteration, construction site drift, library code calling os.Exit, exported mutable maps, YAML zero-value ambiguity, division by zero in runtime computation, leaky interfaces, monolith methods, documentation drift.</p>"},{"location":"process/pr-workflow/#perspective-3-test-behavioral-quality","title":"Perspective 3: Test Behavioral Quality","text":"<p>Verify tests are truly behavioral (testing WHAT) not structural (testing HOW).</p> <p>Prompt: <pre><code>/pr-review-toolkit:review-pr Are all the tests well written and truly behavioral? Do they test observable behavior (GIVEN/WHEN/THEN) or just assert internal structure? Would they survive a refactor? Rate each test as Behavioral, Mixed, or Structural. Also check: are there any golden dataset tests (comparing against captured output values) that lack a corresponding invariant test? Golden tests encode current behavior as \"correct\" \u2014 if the code had a bug when the golden values were captured, the test perpetuates the bug. Flag any golden test whose expected values are not independently validated by an invariant test (e.g., request conservation, KV block conservation, causality).\n</code></pre></p> <p>Catches: Structural tests (Go struct assignment, trivial getters), type assertions in factory tests, exact-formula assertions instead of behavioral invariants, tests that pass even if the feature is broken, golden-only tests that would perpetuate pre-existing bugs (issue #183: codellama golden dataset encoded a silently-dropped request as the expected value since its initial commit).</p>"},{"location":"process/pr-workflow/#perspective-4-getting-started-experience","title":"Perspective 4: Getting-Started Experience","text":"<p>Simulate the journey of a new user and a new contributor.</p> <p>Prompt: <pre><code>/pr-review-toolkit:review-pr Is it easy for a user and contributor to get started? Simulate both journeys: (1) a user doing capacity planning with the CLI, and (2) a contributor adding a new algorithm. Where would they get stuck? What's missing?\n</code></pre></p> <p>Catches: Missing example files, undocumented output metrics, incomplete contributor guide, unclear extension points, README not updated for new features.</p>"},{"location":"process/pr-workflow/#perspective-5-automated-reviewer-simulation","title":"Perspective 5: Automated Reviewer Simulation","text":"<p>Catch what GitHub Copilot, Claude, and Codex would flag.</p> <p>Prompt: <pre><code>/pr-review-toolkit:review-pr The upstream community uses github copilot, claude, codex apps to perform a review of this PR. Please do a rigorous check (and fix any issues) so that this will pass the review.\n</code></pre></p> <p>Catches: Exported mutable globals, user-controlled panic paths, YAML typo acceptance, NaN/Inf validation gaps, redundant code, style nits.</p>"},{"location":"process/pr-workflow/#perspective-6-des-expert-review_1","title":"Perspective 6: DES Expert Review","text":"<p>Prompt: <pre><code>/pr-review-toolkit:review-pr Review this diff as a discrete-event simulation expert. Check for: event ordering bugs, clock monotonicity violations, stale signal propagation between event types, heap priority errors, event-driven race conditions, work-conserving property violations, and incorrect assumptions about DES event processing semantics.\n</code></pre></p> <p>Catches: Event ordering violations, clock regression, stale-signal bugs, work-conserving property gaps.</p>"},{"location":"process/pr-workflow/#perspective-7-vllmsglang-expert-review_1","title":"Perspective 7: vLLM/SGLang Expert Review","text":"<p>Prompt: <pre><code>/pr-review-toolkit:review-pr Review this diff as a vLLM/SGLang inference serving expert. Check for: batching semantics that don't match real continuous-batching servers, KV cache eviction mismatches, chunked prefill behavior errors, preemption policy differences, and missing scheduling features. Flag any assumption about LLM serving that this code gets wrong.\n</code></pre></p> <p>Catches: Batching model inaccuracies, KV cache behavior mismatches, scheduling assumption violations.</p>"},{"location":"process/pr-workflow/#perspective-8-distributed-inference-platform-expert-review_1","title":"Perspective 8: Distributed Inference Platform Expert Review","text":"<p>Prompt: <pre><code>/pr-review-toolkit:review-pr Review this diff as a distributed inference platform expert (llm-d, KServe, vLLM multi-node). Check for: multi-instance coordination bugs, routing load imbalance, stale snapshot propagation, admission control edge cases, horizontal scaling assumptions, and prefix-affinity routing correctness.\n</code></pre></p> <p>Catches: Load imbalance, stale routing state, scaling assumption violations, cross-instance bugs.</p>"},{"location":"process/pr-workflow/#perspective-9-performance-scalability-analyst_1","title":"Perspective 9: Performance &amp; Scalability Analyst","text":"<p>Prompt: <pre><code>/pr-review-toolkit:review-pr Review this diff as a performance and scalability analyst. Check for: algorithmic complexity issues, unnecessary allocations in hot paths, map iteration in O(n) loops, benchmark-sensitive changes, memory growth patterns, and changes that would degrade performance at 1000+ requests or 10+ instances.\n</code></pre></p> <p>Catches: Complexity regressions, hot-path allocations, memory growth, scalability bottlenecks.</p>"},{"location":"process/pr-workflow/#perspective-10-security-robustness-reviewer_1","title":"Perspective 10: Security &amp; Robustness Reviewer","text":"<p>Prompt: <pre><code>/pr-review-toolkit:review-pr Review this diff as a security and robustness reviewer. Check for: input validation completeness, panic paths reachable from user input, resource exhaustion vectors, degenerate input handling (empty, zero, NaN, Inf), and configuration injection risks.\n</code></pre></p> <p>Catches: Validation gaps, user-reachable panics, resource exhaustion, degenerate input failures.</p>"},{"location":"process/pr-workflow/#during-any-perspective-filing-pre-existing-issues","title":"During Any Perspective: Filing Pre-Existing Issues","text":"<p>Review passes naturally surface pre-existing bugs in surrounding code. These are valuable discoveries but outside the current PR's scope.</p> <p>Rule: File a GitHub issue immediately. Do not fix in the current PR.</p> <pre><code>gh issue create --title \"Bug: &lt;concise description&gt;\" --body \"&lt;location, impact, discovery context&gt;\" --label bug\n</code></pre> <p>Label guide: Use <code>bug</code> for code defects, <code>design</code> for design limitations, <code>enhancement</code> for feature gaps, <code>hardening</code> for correctness/invariant issues. Every issue must have at least one label \u2014 unlabeled issues are invisible in filtered views.</p> <p>Why not fix in-PR? - Scope creep \u2014 muddies the diff, makes review harder, risks introducing regressions in unrelated code - Attribution \u2014 the fix deserves its own tests and its own commit history - Tracking \u2014 issues that aren't filed are issues that are lost</p> <p>After filing: Reference the issue number in the PR description under a \"Discovered Issues\" section so reviewers know it was found and tracked.</p> <p>Example (from #38 log level recalibration): Code review found that <code>simulator.go:593</code> silently drops a request on KV allocation failure \u2014 a pre-existing bug predating the PR. Filed as #183 rather than fixing in-scope.</p>"},{"location":"process/pr-workflow/#convergence-protocol","title":"Convergence Protocol","text":"<p>Canonical source: docs/process/convergence.md. The same protocol applies to all review gates (PR plan, PR code, hypothesis design/code/FINDINGS).</p> <p>In summary: run all perspectives as a parallel round. If zero CRITICAL and zero IMPORTANT across all reviewers, the round converged. If any CRITICAL or IMPORTANT from any reviewer, fix all issues and re-run the entire round. Max 10 rounds per gate. Hard gate \u2014 no exceptions.</p> <p>Executable implementation: <code>/convergence-review pr-code</code> (or <code>pr-plan</code>) automates dispatch, tallying, and re-run enforcement.</p>"},{"location":"process/pr-workflow/#after-convergence-enforced-verification-gate","title":"After Convergence: Enforced Verification Gate","text":"<p>For Claude: After fixing issues from all passes, invoke the verification skill to ensure all claims are backed by evidence. This is non-optional \u2014 do NOT skip to Step 5.</p> <p>Skill: <code>superpowers:verification-before-completion</code></p> <pre><code>/superpowers:verification-before-completion\n</code></pre> <p>This skill enforces running verification commands and confirming output before making any success claims. It requires: - <code>go build ./...</code> \u2014 build passes - <code>go test ./... -count=1</code> \u2014 all tests pass (with counts) - <code>golangci-lint run ./...</code> \u2014 zero lint issues - <code>git status</code> \u2014 working tree status reported</p> <p>Why a skill instead of prose? In PR9, the manual \"run these commands\" instruction was easy to skip or half-execute. The skill makes verification non-optional and evidence-based.</p> <p>Report: build exit code, test pass/fail counts, lint issue count, working tree status. Wait for user approval before proceeding to Step 4.75.</p>"},{"location":"process/pr-workflow/#step-475-pre-commit-self-audit-no-agent-deliberate-thinking","title":"Step 4.75: Pre-Commit Self-Audit (No Agent \u2014 Deliberate Thinking)","text":"<p>Context: Worktree (after verification gate passes)</p> <p>For Claude: This is NOT an agent pass. Stop, think critically, and answer each question below from your own reasoning. Do NOT dispatch agents or read files \u2014 you already have the full context. Report all issues found. If you find zero issues, explain why you're confident for each dimension.</p> <p>Why this step exists: In PR9, the 4-perspective automated code review (Step 4.5) found 0 new issues in the final perspective. Then the user asked \"are you confident?\" and Claude found 3 real bugs by thinking critically: a wrong reference scale for token throughput normalization, non-deterministic map iteration in output, and inconsistent comment patterns. Automated review perspectives check structure; this step checks substance.</p> <p>Self-audit dimensions \u2014 think through each one:</p> <ol> <li>Logic bugs: Trace through the core algorithm mentally. Are there edge cases where the math breaks? Division by zero? Off-by-one? Wrong comparisons?</li> <li>Design bugs: Does the design actually achieve what the contracts promise? Would a user get the expected behavior? Are there scale mismatches, unit confusions, or semantic errors?</li> <li>Determinism (R2, INV-6): Is all output deterministic? Any map iteration used for ordered output? Any floating-point accumulation order dependencies?</li> <li>Consistency: Are naming patterns consistent across all changed files? Do comments match code? Do doc strings match implementations? Are there stale references?</li> <li>Documentation: Would a new user find everything they need? Would a contributor know how to extend this? Are CLI flags documented everywhere (CLAUDE.md, README, <code>--help</code>)?</li> <li>Defensive edge cases: What happens with zero input? Empty collections? Maximum values? What if the user passes unusual but valid flag combinations?</li> <li>Test epistemology (R7, R12): For every test that compares against a golden value, ask: \"How do I know this expected value is correct?\" If the answer is \"because the code produced it,\" that test catches regressions but not pre-existing bugs. Verify a corresponding invariant test validates the result from first principles. (See issue #183: a golden test perpetuated a silently-dropped request for months.)</li> <li>Construction site uniqueness (R4): Does this PR add fields to existing structs? If so, are ALL construction sites updated? Grep for <code>StructName{</code> across the codebase. Are there canonical constructors, or are structs built inline in multiple places?</li> <li>Error path completeness (R1, R5): For every error/failure path in new code, what happens to partially-mutated state? Does every <code>continue</code> or early <code>return</code> clean up what was started? Is there a counter or log so the failure is observable?</li> <li>Documentation DRY (source-of-truth map): Does this PR modify content that exists as a working copy elsewhere? Check the source-of-truth map in <code>docs/standards/principles.md</code>. If a canonical source was updated (rules.md, invariants.md, principles.md, extension-recipes.md), verify all working copies listed in the map are also updated. If a new file or section was added, verify it appears in the File Organization tree. If a hypothesis experiment was completed, verify <code>hypotheses/README.md</code> is updated.</li> </ol> <p>Fix all issues found. Then wait for user approval before Step 5.</p> <p>Why no agent? Agents are good at pattern-matching (finding style violations, checking structure). They're bad at stepping back and asking \"does this actually make sense?\" That requires the kind of critical thinking that only happens when you deliberately pause and reflect.</p>"},{"location":"process/pr-workflow/#step-5-commit-push-and-create-pr-using-commit-commandscommit-push-pr","title":"Step 5: Commit, Push, and Create PR Using <code>commit-commands:commit-push-pr</code>","text":"<p>Context: Worktree (after code review passed and all issues fixed)</p> <p>Skill: <code>commit-commands:commit-push-pr</code></p> <p>Invocation (simplified): <pre><code>/commit-commands:commit-push-pr\n</code></pre></p> <p>What Happens: - Reviews git status and staged/unstaged changes - Creates a commit with appropriate message (or amends if per-task commits exist) - Pushes branch to origin - Creates GitHub PR automatically - All in one command!</p> <p>The skill automatically: 1. Analyzes current git state (per-task commits from Step 4, or uncommitted changes) 2. Creates/amends commit with appropriate message (references behavioral contracts) 3. Pushes branch to origin with <code>-u</code> flag 4. Creates PR using <code>gh pr create</code> with title and description</p> <p>Commit message includes: - PR title from source document or work item - Multi-line description of changes - List of implemented behavioral contracts (BC-1, BC-2, etc.) - Co-authored-by line</p> <p>PR description includes: - Summary from source document - GitHub closing keywords from the plan's <code>Closes:</code> field (e.g., <code>Fixes #183, fixes #189</code>) \u2014 these auto-close issues on merge - Behavioral contracts (GIVEN/WHEN/THEN) - Testing verification - Checklist of completed items</p> <p>Output: - Commit(s) pushed to GitHub (per-task commits from Step 4 + plan file) - PR URL (e.g., <code>https://github.com/user/repo/pull/123</code>)</p> <p>Note: If you prefer a single squashed commit, manually squash before Step 5: <pre><code>git reset --soft HEAD~N  # N = number of task commits\ngit commit -m \"PR&lt;N&gt;: &lt;title&gt;\"\n/commit-commands:commit-push-pr\n</code></pre></p>"},{"location":"process/pr-workflow/#workflow-variants","title":"Workflow Variants","text":""},{"location":"process/pr-workflow/#option-a-subagent-driven-development-in-session","title":"Option A: Subagent-Driven Development (In-Session)","text":"<p>Alternative to Step 4 - Use for simpler PRs where you want tighter iteration:</p> <p>Skill: <code>superpowers:subagent-driven-development</code></p> <p>Invocation: <pre><code>Use the subagent-driven-development skill to implement docs/plans/pr&lt;N&gt;-&lt;feature-name&gt;-plan.md.\n</code></pre></p> <p>Differences: - Executes in current session (no separate session needed) - Fresh subagent per task (better context isolation) - Immediate code review after each task - Faster iteration for small changes</p> <p>Trade-offs: - \u2705 Faster for simple PRs (no session switching) - \u2705 Better for iterative refinement - \u26a0\ufe0f Uses current session's context (can grow large) - \u26a0\ufe0f Review after every task (vs continuous execution in executing-plans)</p>"},{"location":"process/pr-workflow/#pr-size-tiers","title":"PR Size Tiers","text":"<p>Not all PRs need the same level of review. Use these objective criteria to select the appropriate tier:</p> Tier Criteria Plan Review (Step 2.5) Code Review (Step 4.5) Self-Audit (Step 4.75) Small Docs-only with no process/workflow semantic changes (typo fixes, formatting, comment updates, link fixes), OR \u22643 files changed AND only mechanical changes (renames, formatting) AND no behavioral logic changes AND no new interfaces/types AND no new CLI flags Skip convergence review; single <code>review-pr</code> pre-pass sufficient Skip convergence review; single <code>review-pr</code> pre-pass sufficient Full (all 10 dimensions) Medium 4-10 files changed, OR new policy template behind existing interface Full two-stage (pre-pass + convergence) Full two-stage (pre-pass + convergence) Full (all 10 dimensions) Large &gt;10 files, OR new interfaces/modules, OR architecture changes Full two-stage (pre-pass + convergence) Full two-stage (pre-pass + convergence) Full (all 10 dimensions) <p>Rules: - Steps 1, 2, 3, 4, 5 are always required \u2014 worktree, plan, human review, execution, and commit apply to all tiers. - Self-audit is always full \u2014 the 10-dimension critical thinking check catches substance bugs that no automated review can. It costs 5 minutes and has caught 3+ real bugs in every PR where it was applied. - When in doubt, tier up \u2014 if you're unsure whether a change is Small or Medium, use Medium. The cost of an extra convergence round is 10-15 minutes; the cost of a missed design bug is hours of rework. - Human reviewer can override \u2014 if the human reviewer at Step 3 believes the tier is wrong, they can request a different tier.</p> <p>Examples: - Fix a typo in README.md \u2192 Small (1 file, docs-only) - Add R18-R20 to micro-plan checklist \u2192 Small (1 file, docs-only, no behavior change) - Add a new routing policy \u2192 Medium (new policy template, ~3 files) - Extract KV cache to sub-package \u2192 Large (&gt;10 files, architecture change)</p>"},{"location":"process/pr-workflow/#skill-reference-quick-guide","title":"Skill Reference Quick Guide","text":"Skill When to Use Input Output <code>commit-commands:clean_gone</code> Step 1 - Pre-cleanup of stale branches None Removed stale branches <code>using-git-worktrees</code> Step 1 - Create isolated workspace FIRST Branch name Worktree directory path <code>writing-plans</code> Step 2 - Create implementation plan from source document Source document (macro plan/design doc/issues) + <code>docs/templates/micro-plan.md</code> Plan file with contracts + tasks <code>pr-review-toolkit:review-pr</code> Step 2.5/4.5 - Holistic cross-cutting pre-pass Plan file or current diff Issues list with severity <code>convergence-review</code> Step 2.5 - Dispatch 10 parallel perspectives + enforce convergence Gate type + plan file path Converged/not-converged with findings <code>executing-plans</code> Step 4 - Execute plan tasks continuously Plan file path Implemented code + commits <code>systematic-debugging</code> Step 4 (on failure) - Structured root-cause analysis Failing test/error context Root cause + fix <code>subagent-driven-development</code> Step 4 (alt) - Execute plan in-session Plan file path Implemented code + commits <code>convergence-review</code> Step 4.5 - Dispatch 10 parallel perspectives + enforce convergence Gate type Converged/not-converged with findings <code>verification-before-completion</code> Step 4.5 (gate) - Enforced build/test/lint verification None Evidence-based pass/fail <code>commit-commands:commit-push-pr</code> Step 5 - Commit, push, create PR (all in one) Current branch state Commit + push + PR URL"},{"location":"process/pr-workflow/#example-a-macro-plan-pr-workflow-same-session-with-worktrees","title":"Example A: Macro Plan PR Workflow (Same-Session with <code>.worktrees/</code>)","text":"<p>Note: Examples below use file paths from completed PRs (PR6, PR8, hardening). Referenced plan files have been archived or removed. Adapt the pattern using current plans from <code>docs/plans/</code>.</p> <pre><code># Step 1: Clean up stale branches, then create worktree\n/commit-commands:clean_gone\n/superpowers:using-git-worktrees pr8-routing-state-and-policy-bundle\n\n# Output: Worktree ready at .worktrees/pr8-routing-state-and-policy-bundle/\n# (continue directly \u2014 no new session needed)\n\n# Step 2: Create plan (source = macro plan section)\n/superpowers:writing-plans for PR8 in @docs/plans/pr8-routing-state-and-policy-bundle-plan.md using @docs/templates/micro-plan.md and @docs/plans/2026-02-11-macro-implementation-plan-v2.md\n\n# Output: Plan created at docs/plans/pr8-routing-state-and-policy-bundle-plan.md\n\n# Step 2.5: Plan review (two-stage)\n# Stage 1: Holistic pre-pass\n/pr-review-toolkit:review-pr\n# Fix any issues found, then:\n# Stage 2: Formal convergence\n/convergence-review pr-plan docs/plans/pr8-routing-state-and-policy-bundle-plan.md\n\n# Step 3: Human review plan\n# [Read plan, verify contracts and tasks, approve to proceed]\n\n# Step 4: Execute implementation\n/superpowers:executing-plans @docs/plans/pr8-routing-state-and-policy-bundle-plan.md\n\n# Output: Tasks execute continuously \u2192 done (stops on failure)\n\n# Step 4.5: Code review (two-stage)\n# Stage 1: Holistic pre-pass\n/pr-review-toolkit:review-pr\n# Fix any issues found, then:\n# Stage 2: Formal convergence\n/convergence-review pr-code\n# Enforced verification gate\n/superpowers:verification-before-completion\n\n# Step 4.75: Self-audit (no agent \u2014 deliberate critical thinking)\n# Think through: logic bugs, design bugs, determinism, consistency, docs, edge cases\n# [Fix any issues found, re-verify]\n\n# Step 5: Commit plan + implementation, push, and create PR (all in one!)\n/commit-commands:commit-push-pr\n\n# Output:\n# - Single commit created (plan + implementation)\n# - Branch pushed to origin\n# - PR created on GitHub\n# - PR URL returned\n</code></pre> <p>Key benefit: No copy-pasting! Just use @ file references and let Claude extract the context. No session switching needed with project-local <code>.worktrees/</code>.</p>"},{"location":"process/pr-workflow/#example-b-issuedesign-doc-pr-workflow","title":"Example B: Issue/Design-Doc PR Workflow","text":"<pre><code># Step 1: Create worktree for hardening work (source = design doc + issues)\n/commit-commands:clean_gone\n/superpowers:using-git-worktrees hardening-antipatterns\n\n# Step 2: Create plan (source = design document, not macro plan)\n/superpowers:writing-plans for hardening PR in @docs/plans/hardening-plan.md using @docs/templates/micro-plan.md and @docs/plans/2026-02-18-hardening-antipattern-refactoring-design.md\n\n# Step 2.5: Plan review (two-stage, same as Example A)\n/pr-review-toolkit:review-pr\n/convergence-review pr-plan docs/plans/hardening-plan.md\n\n# Steps 3-5: Identical to Example A\n</code></pre> <p>The workflow is the same regardless of source. The only difference is what you pass as <code>@&lt;source-document&gt;</code> in Step 2. The template, review passes, execution, and quality gates are identical.</p>"},{"location":"process/pr-workflow/#tips-for-success","title":"Tips for Success","text":"<ol> <li>Use automated reviews proactively - Run <code>review-pr</code> after plan creation and after implementation (don't wait for human review to catch issues)</li> <li>Fix critical issues immediately - Don't proceed with known critical issues (they compound)</li> <li>Re-run targeted reviews after fixes - Verify fixes worked: <code>/pr-review-toolkit:review-pr code tests</code></li> <li>Use worktrees for complex PRs - Avoid disrupting main workspace</li> <li>Review after execution - Use automated code review (Step 4.5) after all tasks complete</li> <li>Reference contracts in commits - Makes review easier and more traceable</li> <li>Update CLAUDE.md immediately - Don't defer documentation</li> <li>Keep source documents updated - Mark PRs as completed in macro plan; close resolved issues</li> <li>Don't trust automated passes alone - The self-audit (Step 4.75) catches substance bugs that pattern-matching agents miss. In PR9, 3 real bugs were found by critical thinking after 4 automated passes found 0 issues.</li> <li>Checkpoint long sessions - For PRs with 8+ tasks or multi-round reviews, write a checkpoint summary to <code>.claude/checkpoint.md</code> after each major phase (planning, implementation, review). If you hit context limits or need to continue in a new session, read the checkpoint first. This prevents losing progress and avoids re-reading the entire conversation history.</li> </ol>"},{"location":"process/pr-workflow/#headless-mode-for-reviews-context-overflow-workaround","title":"Headless Mode for Reviews (Context Overflow Workaround)","text":"<p>If multi-agent review passes hit \"Prompt is too long\" errors during consolidation (a recurring friction point), switch to headless mode: run each review agent as an isolated invocation that writes findings to a file, then consolidate in a lightweight final pass.</p> <pre><code>#!/bin/bash\n# headless-review.sh \u2014 Run review agents with full context each\nBRANCH=$(git branch --show-current)\nPLAN=\"docs/plans/pr&lt;N&gt;-&lt;name&gt;-plan.md\"\nmkdir -p .review\n\n# Run each pass in its own context (no overflow)\nclaude -p \"Pass 1: Code quality review of branch $BRANCH. Read all changed Go files. Write findings to .review/01-code-quality.md\" \\\n  --allowedTools \"Read,Grep,Glob,Bash\" &amp;\nclaude -p \"Pass 2: Test behavioral quality review. Rate each new test. Write findings to .review/02-test-quality.md\" \\\n  --allowedTools \"Read,Grep,Glob,Bash\" &amp;\nclaude -p \"Pass 3: Getting-started review. Simulate user + contributor journeys. Write findings to .review/03-getting-started.md\" \\\n  --allowedTools \"Read,Grep,Glob,Bash\" &amp;\nwait\n\n# Lightweight consolidation (reads only the small finding files)\nclaude -p \"Read .review/*.md files. Produce a consolidated summary sorted by severity.\" \\\n  --allowedTools \"Read,Glob\"\n</code></pre> <p>When to use: When Step 2.5 or Step 4.5 hits context limits. Not needed for most PRs \u2014 only when the conversation history is already long.</p>"},{"location":"process/pr-workflow/#review-strategy-tips","title":"Review Strategy Tips","text":"<p>Always use focused prompts, not generic invocations. Each perspective catches different issues:</p> Perspective What It Catches That Others Miss Substance &amp; design Design bugs, mathematical errors, logical flaws (substance, not structure) Cross-doc consistency Stale source document references, scope mismatch, wrong file paths Architecture boundary Import cycles, boundary violations, wrong abstraction level Codebase readiness Stale comments, pre-existing bugs, missing dependencies Structural validation Broken task dependencies, missing sections, vague steps, unclear summaries Code quality Logic errors, silent failures, convention violations Test behavioral quality Structural tests, type assertions, formula-coupled assertions Getting-started experience Missing examples, undocumented output, contributor friction Automated reviewer sim Mutable globals, user-controlled panics, YAML typo acceptance <p>Run all perspectives in parallel as a round. Collect all findings, then fix. Do NOT fix between individual perspectives \u2014 that breaks parallelism and makes it impossible to tell whether the round converged.</p> <p>Convergence = clean round. A round converges when ALL perspectives report 0 CRITICAL and 0 IMPORTANT findings. If any perspective found issues, the round did not converge \u2014 fix all issues and re-run the entire round. The re-run determines convergence, not the initial round with fixes applied.</p>"},{"location":"process/pr-workflow/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"process/pr-workflow/#issue-plan-too-generic-agents-ask-clarifying-questions","title":"Issue: Plan too generic, agents ask clarifying questions","text":"<p>Solution: The simplified invocation with @ references handles this automatically: <pre><code>/superpowers:writing-plans for PR6 in @docs/plans/pr6-plan.md using @docs/templates/micro-plan.md and @docs/plans/2026-02-11-macro-implementation-plan-v2.md\n</code></pre></p> <p>Claude reads the full macro plan and extracts PR6 context (architecture, dependencies, etc.) automatically.</p> <p>If still too generic: Add specific guidance in the invocation: <pre><code>/superpowers:writing-plans for PR6 in @docs/plans/pr6-plan.md using @docs/templates/micro-plan.md and @docs/plans/2026-02-11-macro-implementation-plan-v2.md\n\nPay special attention to:\n- Integration with existing SnapshotProvider (see sim/cluster/snapshot.go)\n- Round-robin as default routing policy\n</code></pre></p>"},{"location":"process/pr-workflow/#issue-tasks-miss-behavioral-contracts-during-execution","title":"Issue: Tasks miss behavioral contracts during execution","text":"<p>Solution: After execution completes, verify all contracts are tested: <pre><code>\"Confirm all contracts are tested:\n- BC-1: Show test results\n- BC-2: Show test results\"\n</code></pre></p>"},{"location":"process/pr-workflow/#issue-lint-fails-at-the-end-with-many-issues","title":"Issue: Lint fails at the end with many issues","text":"<p>Solution: Ensure task Step 5 (lint check) runs in each task: <pre><code>Each task Step 5 must run:\ngolangci-lint run ./path/to/modified/package/...\n</code></pre></p>"},{"location":"process/pr-workflow/#issue-dead-code-introduced-unused-functions-fields","title":"Issue: Dead code introduced (unused functions, fields)","text":"<p>Solution: In Step 3 plan review, check: - Every struct field used by end of task or later task? - Every method called by tests or production code? - Every parameter actually needed?</p> <p>Better Solution: Run <code>review-pr</code> at Step 2.5 to catch dead code in plan: <pre><code>/pr-review-toolkit:review-pr code\n# code-reviewer agent catches unused abstractions in task code examples\n</code></pre></p>"},{"location":"process/pr-workflow/#issue-review-finds-many-critical-issues-overwhelming-to-fix","title":"Issue: Review finds many critical issues, overwhelming to fix","text":"<p>Solution: Fix issues in priority order: 1. First pass: Fix all critical issues, re-run review 2. Second pass: Fix important issues, re-run review 3. Third pass: Consider suggestions 4. Use targeted review: After fixes, only re-run affected aspects    <pre><code># Example: After fixing error handling\n/pr-review-toolkit:review-pr errors\n</code></pre></p>"},{"location":"process/pr-workflow/#issue-uncertain-if-review-findings-are-valid","title":"Issue: Uncertain if review findings are valid","text":"<p>Solution: Review agents provide file:line references: 1. Check the specific code location mentioned 2. Understand the context (sometimes agents miss context) 3. If uncertain, ask Claude to explain the finding 4. If agent is wrong, document why and proceed 5. Consider adding a comment in code explaining why the pattern is intentional</p>"},{"location":"process/pr-workflow/#appendix-workflow-evolution","title":"Appendix: Workflow Evolution","text":"<p>v1.0 (pre-2026-02-14): Manual agent team prompts, separate design/execution plans v2.0 (2026-02-14): Unified planning with <code>writing-plans</code> skill, batch execution with <code>executing-plans</code> skill, automated two-stage review with <code>pr-review-toolkit:review-pr</code>, simplified invocations with @ file references v2.1 (2026-02-16): Same-session worktree workflow (project-local <code>.worktrees/</code> no longer requires new session); continuous execution replaces batch checkpoints (tasks run without pausing, stop only on failure) v2.2 (2026-02-16): Focused review passes replace generic review-pr invocations. Step 2.5 expanded to 3 passes (cross-doc consistency, architecture boundary, codebase readiness). Step 4.5 expanded to 4 passes (code quality, test behavioral quality, getting-started experience, automated reviewer simulation). Based on PR8 experience where each focused pass caught issues the others missed. v2.3 (2026-02-16): Step 2.5 expanded to 4 passes \u2014 added Pass 4 (structural validation: task dependencies, template completeness, executive summary clarity, under-specified task detection). Based on PR9 experience where deferred items fell through cracks in the macro plan, and an under-specified documentation task would have confused the executing agent. v2.4 (2026-02-16): Four targeted skill integrations addressing real failure modes: (1) <code>review-plan</code> as Pass 0 in Step 2.5 \u2014 external LLM review catches design bugs that self-review misses (PR9: fitness normalization bug passed 3 focused passes). (2) <code>superpowers:systematic-debugging</code> as on-failure handler in Step 4 \u2014 structured root-cause analysis instead of ad-hoc debugging. (3) <code>superpowers:verification-before-completion</code> replaces manual verification prose after Step 4.5 \u2014 makes build/test/lint gate non-skippable. (4) <code>commit-commands:clean_gone</code> as pre-cleanup in Step 1 \u2014 prevents stale branch accumulation. v2.5 (2026-02-16): Three additions from <code>/insights</code> analysis of 212 sessions: (1) Step 4.75 (pre-commit self-audit) \u2014 deliberate critical thinking step with no agent, checking logic/design/determinism/consistency/docs/edge-cases. In PR9, this step found 3 real bugs (wrong reference scale, non-deterministic output, inconsistent comments) that 4 automated passes missed. (2) Headless mode documentation for review passes \u2014 workaround for context overflow during multi-agent consolidation, the #1 recurring friction point across 212 sessions. (3) Checkpointing tip for long sessions \u2014 prevents progress loss when hitting context limits mid-PR. v2.6 (2026-02-18): Two additions: (1) \"Filing Pre-Existing Issues\" subsection to Step 4.5 \u2014 file a GitHub issue immediately for pre-existing bugs found during review, do not fix in-PR. Based on #38 experience where #183 was discovered. (2) Antipattern prevention from hardening audit of 20+ issues \u2014 Step 4.75 expanded to 9 self-audit dimensions (added test epistemology, construction site uniqueness, error path completeness); Step 4.5 Pass 1 prompt expanded with 4 antipattern checks; Step 2.5 Pass 2 prompt expanded with 3 modularity checks. Companion change: <code>prmicroplanprompt-v2.md</code> updated with construction site audit (Phase 0), extension friction assessment (Phase 2), invariant test requirement (Phase 6), and 6 new sanity checklist items (Phase 8). v2.7 (2026-02-18): Generalized workflow from \"macro plan only\" to any source document (macro plan sections, design docs, GitHub issues, feature requests). Updated template references, review prompts, examples, and invocation patterns. Added Example B showing issue/design-doc workflow alongside macro plan workflow. The same rigor (behavioral contracts, TDD tasks, 5+4 review passes, self-audit) applies regardless of work source. v2.8 (2026-02-18): Auto-close issues on PR merge. Added <code>**Closes:**</code> field to micro plan header template (<code>prmicroplanprompt-v2.md</code>) that captures GitHub closing keywords (e.g., <code>Fixes #183, fixes #189</code>). Updated Step 5 PR description spec to propagate closing keywords into the PR body. GitHub auto-closes referenced issues when the PR merges \u2014 no manual cleanup needed. v2.9 (2026-02-20): Convergence re-run protocol for both Step 2.5 and Step 4.5. After all review passes complete with fixes, re-run all passes from scratch to verify fixes didn't introduce cross-pass issues. Repeat until convergence (0 CRITICAL, 0 IMPORTANT). Evidence: Wave 1 parallel PR session \u2014 Track B's full re-run validated that 3 fixes (including 2 CRITICAL: overwrite-existing-test-file and incomplete zero-value coverage) introduced no regressions across all 5 passes.</p> <p>v3.0 (2026-02-23): Three structural changes aligned with the hypothesis process model: (1) Multi-perspective rounds replace sequential passes. Step 2.5 and Step 4.5 now run all perspectives in parallel as a single \"round\" instead of sequentially with fixes between passes. This matches the hypothesis process's 3-parallel-reviewer model. (2) External LLM review (<code>review-plan</code>) removed. Replaced by an internal \"Substance &amp; Design\" perspective that checks for mathematical errors, scale mismatches, and logical flaws \u2014 the same coverage, without the external API dependency. (3) Convergence redefined. A round converges when ALL perspectives report 0 CRITICAL and 0 IMPORTANT findings on first pass. If issues are found, fix and re-run the entire round. Convergence is a property of a clean round, not of iterative fixes within a round.</p> <p>Key improvements in v2.0: - Simplified invocations: No copy-pasting! Use @ file references (e.g., <code>@docs/plans/&lt;macro-plan&gt;.md</code>) - Single planning stage: Produces both design contracts and executable tasks - Automated plan review: Catches design issues before implementation (Step 2.5) - Automated code review: Catches implementation issues before PR creation (Step 4.5) - Built-in checkpoint reviews: During execution (Step 4) \u2014 replaced by continuous execution in v2.1 - Reduced manual overhead: Skills handle context extraction automatically</p> <p>Example workflow brevity: - v1.0: ~200 words of manual prompts per PR - v2.0: 5 simple commands with @ references</p> <p>For questions or workflow improvements, discuss with Claude using this document as context.</p>"},{"location":"standards/experiments/","title":"BLIS Experiment Standards","text":"<p>Hypothesis-driven experimentation is a first-class activity in BLIS \u2014 equal in rigor to implementation and design. The experiment framework is grounded in Verification, Validation, and Uncertainty Quantification (VV&amp;UQ) from simulation science.</p>"},{"location":"standards/experiments/#vvuq-framing","title":"VV&amp;UQ Framing","text":"<p>Every hypothesis falls into one of three VV&amp;UQ categories. This determines what kind of evidence is needed.</p> Category Question Evidence required Examples Verification Does the code implement the intended math/logic? Exact invariant checks. Failure = bug. H12 (conservation), H13 (determinism), H22 (input validation) Validation Does the model match expected system behavior? Statistical comparison against analytical baselines or real data within a pre-specified accuracy interval. Cross-validation against M/M/k; H19 (roofline vs blackbox) Uncertainty Quantification How confident are we in the region where a finding holds? Confidence intervals on thresholds; probability statements on properties. H8 (preemption cliff at 2100\u00b1? blocks); H10 (28% improvement at this operating point \u2014 what about others?) <p>Most current experiments are Verification (invariant checking) or informal Validation (metric comparison). Future experiments should increasingly incorporate UQ \u2014 every threshold finding should include a confidence interval, every \"confirmed\" result should quantify the probability of holding under parameter variation.</p>"},{"location":"standards/experiments/#purposes","title":"Purposes","text":"<p>The VV&amp;UQ categories above classify what kind of evidence an experiment needs. But experiments also serve broader purposes beyond their category:</p> <ul> <li>Verification and Validation \u2014 the primary purposes, aligned with the VV&amp;UQ categories above</li> <li>Discovery \u2014 a secondary outcome of any experiment. Bugs, design gaps, and undocumented limitations often surface during verification or validation experiments. Discovery is not a separate VV&amp;UQ category \u2014 it's a valuable byproduct. Example: H5 (a Validation experiment) discovered that the per-input-token cost model makes burst smoothing structurally impossible \u2014 a design limitation finding that was not the hypothesis being tested.</li> </ul>"},{"location":"standards/experiments/#how-to-choose-your-vvuq-category","title":"How to choose your VV&amp;UQ category","text":"<pre><code>Is your hypothesis about whether the CODE is correct?\n  \u2192 Yes: Verification (e.g., \"conservation holds,\" \"deterministic output\")\n  \u2192 No: \u2193\nIs your hypothesis comparing the MODEL's output to expected behavior?\n  \u2192 Yes: Validation (e.g., \"policy A beats B,\" \"TTFT \u221d input tokens\")\n  \u2192 No: \u2193\nIs your hypothesis about the BOUNDARIES or CONFIDENCE of a finding?\n  \u2192 Yes: UQ (e.g., \"preemption cliff at 2100\u00b1100 blocks,\" \"P(stable) &gt; 0.95\")\n</code></pre> <p>The VV&amp;UQ category determines what counts as evidence: - Verification: Exact invariant checks. One failure = bug. Single seed sufficient. - Validation: Statistical comparison within pre-specified accuracy interval. 3+ seeds. Formal tests (KS, Mann-Whitney U). - UQ: Confidence intervals on thresholds. Parameter sweeps. Sensitivity analysis.</p>"},{"location":"standards/experiments/#formal-statistical-rigor","title":"Formal statistical rigor","text":"<p>Experiments involving statistical claims must use proper hypothesis tests, not ad-hoc thresholds:</p> <ul> <li>Distribution validation (workload/arrival family): Kolmogorov-Smirnov test \u2014 compares a sample against a theoretical CDF. Reject if p &lt; 0.05. In Python: <code>from scipy.stats import kstest; stat, p = kstest(samples, 'expon', args=(0, 1/rate))</code>.</li> <li>Metric comparison (cross-policy family): Mann-Whitney U test \u2014 non-parametric comparison of two independent samples. Report effect size AND confidence interval, not just \"X% better.\" In Python: <code>from scipy.stats import mannwhitneyu; stat, p = mannwhitneyu(a_values, b_values)</code>.</li> <li>Threshold estimation (performance-regime family): Report thresholds with confidence intervals. \"Preemption cliff at 2100 blocks\" \u2192 \"Preemption cliff at 2100 \u00b1 100 blocks (95% CI across seeds 42, 123, 456).\"</li> <li>Invariant probability (scheduler invariants family): For stochastic invariants, estimate P(invariant holds) with a confidence interval, not just \"holds for 3 seeds.\"</li> </ul> <p>Note on scipy: The tests above use <code>scipy.stats</code>. Install with <code>pip install scipy</code> if needed. For experiments that only use standard-library Python, the legacy thresholds (below) remain acceptable.</p> <p>Legacy thresholds (still valid for experiments without scipy): - &gt;20% improvement consistent across all seeds = significant - &lt;10% in any seed = inconclusive - Within 5% across all seeds = equivalent (for equivalence tests)</p> <p>These thresholds were chosen pragmatically \u2014 20% ensures the effect is visible above seed-to-seed variance in typical BLIS experiments; 5% accounts for floating-point and timing noise. They are not derived from formal power analysis. New experiments should prefer formal tests where scipy is available.</p>"},{"location":"standards/experiments/#cross-validation-against-analytical-models","title":"Cross-validation against analytical models","text":"<p>Where applicable, validate DES outputs against analytically-tractable models under matching assumptions. This grounds the simulator in theory.</p> <ul> <li>M/M/k baseline: M/M/k is the standard queueing model with Markovian (Poisson) arrivals, Markovian (exponential) service times, and k servers. Under matching assumptions, compare DES queue length distribution against the M/M/k analytical solution. Caveat: BLIS uses batching and deterministic service times (alpha/beta coefficients), so exact M/M/k matching is not possible. The comparison requires configuring BLIS with <code>--max-batch-size 1</code> and interpreting the service time distribution as approximately exponential. Divergence may indicate modeling errors OR fundamental architectural differences from M/M/k assumptions.</li> <li>Little's Law: For any stable configuration, verify L = \u03bbW (average queue length = arrival rate \u00d7 average wait time). This is a universal law that must hold. In BLIS terms: L = mean <code>still_queued</code> from per-instance metrics; \u03bb = <code>injected_requests / (sim_duration_us / 1e6)</code>; W = mean scheduling delay from <code>scheduling_delay_p99_ms</code> (approximate). Extract from JSON output.</li> <li>Phase structure: Verify that prefill time \u221d prompt tokens and decode time \u221d output tokens by fitting linear models and checking R\u00b2 &gt; 0.95. In BLIS terms: prefill time \u2248 TTFT (time to first token); decode time \u2248 E2E - TTFT. Vary <code>input_distribution</code> mean while holding <code>output_distribution</code> constant, and vice versa.</li> </ul>"},{"location":"standards/experiments/#experiment-classification","title":"Experiment Classification","text":"<p>Every hypothesis must be classified before designing the experiment. The classification determines rigor requirements.</p>"},{"location":"standards/experiments/#type-1-deterministic-experiments","title":"Type 1: Deterministic Experiments","text":"<p>Definition: Verify exact properties \u2014 invariants, conservation laws, error handling boundaries. Same seed = same result, guaranteed.</p> <p>Requirements: - Single seed sufficient (determinism is the point) - Pass/fail is exact \u2014 the invariant holds or it doesn't - Failure is ALWAYS a bug (never noise) - No statistical analysis needed</p> <p>Examples: - H12: Request conservation (INV-1) holds across 10 policy configurations (67 checks) - H13: Same seed produces byte-identical output - H22: Zero KV blocks panics at CLI boundary, not deep in simulation</p> <p>Pass criteria: The invariant holds for every configuration tested. One failure = bug.</p>"},{"location":"standards/experiments/#type-2-statistical-experiments","title":"Type 2: Statistical Experiments","text":"<p>Definition: Compare metrics (TTFT, throughput, distribution uniformity) across configurations. Results vary by seed.</p> <p>Requirements: - Minimum 3 seeds (42, 123, 456) for each configuration - Effect size thresholds:   - Significant: &gt;20% improvement consistent across ALL seeds   - Inconclusive: &lt;10% in any seed   - Equivalent: within 5% across all seeds (for equivalence tests) - Directional consistency: the predicted direction must hold across ALL seeds. One contradicting seed = hypothesis not confirmed - Report: mean, min, max across seeds for primary metric. Include per-seed values for transparency.</p> <p>Subtypes:</p>"},{"location":"standards/experiments/#dominance","title":"Dominance","text":"<p>A is strictly better than B on metric M.</p> <ul> <li>Analysis: Compare metric M for A vs B across all seeds. Compute ratio per seed.</li> <li>Pass: A beats B on M for all seeds, with &gt;20% effect size in every seed.</li> <li>Examples: H3 \u2014 queue-depth TTFT is 1.7-2.8x better than kv-utilization across 3 seeds. H14 \u2014 <code>always-busiest</code> routing produces 4.6x worse TTFT and routes all 500 requests to a single instance.</li> </ul>"},{"location":"standards/experiments/#monotonicity","title":"Monotonicity","text":"<p>Increasing X should monotonically increase/decrease Y.</p> <ul> <li>Analysis: Run at &gt;=3 values of X. Verify Y changes monotonically.</li> <li>Pass: Y is strictly monotonic in X across all seeds. No inversions.</li> <li>Example: H8 \u2014 reducing total KV blocks increases preemption frequency. H9 \u2014 increasing prefix_length decreases TTFT.</li> </ul>"},{"location":"standards/experiments/#equivalence","title":"Equivalence","text":"<p>A ~ B within tolerance (baseline sanity checks).</p> <ul> <li>Analysis: Compare metric M for A vs B. Compute percentage difference per seed.</li> <li>Pass: |A - B| / max(A, B) &lt; 5% across all seeds.</li> <li>Example: H4 \u2014 round-robin ~ least-loaded for uniform workloads at low rates. H23 \u2014 all policies equivalent at near-zero load.</li> </ul>"},{"location":"standards/experiments/#pareto","title":"Pareto","text":"<p>No single configuration dominates all metrics simultaneously.</p> <ul> <li>Analysis: Run N configurations, measure multiple metrics. Identify Pareto-optimal set.</li> <li>Pass: At least 2 configurations are Pareto-optimal (each best on &gt;=1 metric).</li> <li>Example: H17 \u2014 different scorer weights optimize for different objectives (TTFT vs throughput).</li> </ul>"},{"location":"standards/experiments/#hypothesis-formation","title":"Hypothesis Formation","text":"<p>Hypotheses must be conceptual and behavioral, not code-grounded. This is the experimental analogue of behavioral vs structural testing.</p>"},{"location":"standards/experiments/#conceptual-hypotheses-test-system-behavior","title":"Conceptual hypotheses test system behavior","text":"<p>A good hypothesis is an intuitive claim about system behavior: \"burst smoothing should reduce tail latency,\" \"tiered storage should reduce preemptions,\" \"same seed should produce identical output.\" These claims are based on systems thinking, not on reading the implementation.</p>"},{"location":"standards/experiments/#do-not-read-the-code-before-forming-hypotheses","title":"Do NOT read the code before forming hypotheses","text":"<p>Reading the code before hypothesizing is like writing structural tests \u2014 you end up testing the implementation, not the behavior. The value of hypothesis-driven experimentation is that conceptual claims failing against the implementation surfaces design limitations that code-aware experiments would avoid.</p> <p>Evidence: If H5 had read <code>admission.go:45</code> before hypothesizing, the experimenter would have designed a \"correct\" experiment with cap=100K, confirmed a tiny effect, and missed the discovery that the per-input-token cost model makes burst smoothing structurally impossible at practical parameters. The conceptual hypothesis exposed a design limitation that a code-grounded hypothesis would have sidestepped.</p>"},{"location":"standards/experiments/#mechanism-not-plausible-is-a-valid-resolution","title":"\"Mechanism not plausible\" is a valid resolution","text":"<p>When a conceptual hypothesis fails because the implementation doesn't support the assumed mechanism, this is the resolution \"Refuted \u2014 mechanism not plausible.\" This is a design limitation finding, not an experimenter error. The hypothesis did its job \u2014 it revealed a gap between how users think the system works and how it actually works.</p>"},{"location":"standards/experiments/#hypothesis-families","title":"Hypothesis Families","text":"<p>Every hypothesis belongs to a family (what domain is being tested) AND a type (how rigor is assessed). These are orthogonal \u2014 a scheduler invariant can be deterministic or statistical; a cross-policy comparison is always statistical.</p>"},{"location":"standards/experiments/#the-six-families","title":"The six families","text":"Family Tests Hypothesis shape Typical type Examples Workload/arrival Input generation: distributions, rates, burstiness, mix proportions \"Generator X produces arrivals matching distribution D within tolerance T\" Statistical H16, H20 Scheduler invariants (safety/liveness) Conservation, determinism, lifecycle, livelock protection \"For ALL configurations, property P holds\" (universally quantified) Deterministic H12 (conservation), H13 (determinism), H25 Performance-regime (scaling laws) Saturation curves, throughput-latency tradeoffs, horizontal scaling \"Metric M is monotonic/convex in parameter P\" Statistical/Monotonicity H7 (scaling), H8 (KV pressure), H11 (batch formation) Structural model DES model assumptions: phase structure, KV mechanics, signal freshness, prefix caching \"Component C behaves according to model assumption A\" Mixed H3 (signal freshness), H9 (prefix caching), H10 (tiered KV), H26 Robustness/failure-mode Overload, misconfiguration, degenerate inputs, pathological policies \"Under stress condition S, the system exhibits defined behavior B (not undefined state)\" Deterministic or Statistical H5 (token-bucket), H14 (pathological), H21, H22, H24 Cross-policy comparative Policy ordering, Pareto frontiers, robustness to workload shifts \"There EXISTS a workload where policy A beats B on metric M\" (existentially quantified) Statistical/Dominance or Pareto H1, H2, H4, H6, H15, H17, H18, H19, H23"},{"location":"standards/experiments/#family-specific-hypothesis-sentence-patterns","title":"Family-specific hypothesis sentence patterns","text":"<p>Use these templates when generating new hypotheses. Each family has a characteristic sentence shape that ensures testability. See also <code>docs/process/hypothesis.md</code> for the full generation guide.</p> Family Sentence pattern Example Workload/arrival \"Generator G with parameters P should produce distribution D with property X within tolerance T\" \"Gamma sampler with CV=3.5 should produce inter-arrival times with CV within 10% of 3.5 over 10K samples\" Scheduler invariants \"For ALL configurations C, invariant I holds at simulation end\" \"For all routing \u00d7 scheduling \u00d7 admission combinations, injected == completed + queued + running\" Performance-regime \"Metric M should be monotonically non-decreasing/non-increasing in parameter P across range [a, b]\" \"TTFT P99 should be monotonically non-decreasing in offered load from 500 to 5000 req/s\" Structural model \"Component C should behave according to assumption A, verified by observable O\" \"Prefill time should be proportional to input token count (R\u00b2 &gt; 0.95 for linear fit)\" Robustness \"Under stress condition S, the system should exhibit behavior B and NOT exhibit behavior X\" \"Under 10x overload, the system should reject excess requests and NOT deadlock or panic\" Cross-policy \"Under workload W, policy A should produce better metric M than policy B because of mechanism Z\" \"Under mixed-SLO workload, priority-FCFS should produce lower realtime TTFT than FCFS because realtime requests get scheduled first\""},{"location":"standards/experiments/#family-type-matrix","title":"Family \u00d7 Type matrix","text":"Deterministic Statistical/Dominance Statistical/Monotonicity Statistical/Equivalence Statistical/Pareto Workload/arrival Seed reproducibility Distribution match Rate scaling \u2014 \u2014 Scheduler invariants Primary (INV-1 through INV-6) \u2014 \u2014 \u2014 \u2014 Performance-regime \u2014 \u2014 Primary (scaling curves) Baseline sanity Knee behavior Structural model Phase structure Signal freshness Cache effectiveness \u2014 \u2014 Robustness Input validation Overload behavior \u2014 \u2014 \u2014 Cross-policy \u2014 Primary (A vs B) \u2014 Low-load equivalence Multi-scorer tradeoffs"},{"location":"standards/experiments/#family-determines-rigor-requirements","title":"Family determines rigor requirements","text":"<ul> <li>Scheduler invariants: Single seed sufficient. Pass/fail is exact. One failure = bug.</li> <li>Cross-policy comparative: 3+ seeds minimum. Must control confounding variables (ED-1, ED-6).</li> <li>Performance-regime: Sweep points (\u22653 values of the independent variable), not just pairwise comparison.</li> <li>Workload/arrival: Statistical tests on generated distributions. Long runs for accurate rate estimation.</li> <li>Structural model: Code-level verification (RCV-1, RCV-4) is essential \u2014 these test implementation assumptions.</li> <li>Robustness: Must test BOTH the defined behavior AND verify no undefined states (deadlock, panic, data loss).</li> </ul>"},{"location":"standards/experiments/#relationship-to-existing-invariants-and-rules","title":"Relationship to existing invariants and rules","text":"Family Related invariants Related rules Scheduler invariants INV-1 (conservation), INV-2 (lifecycle), INV-3 (clock monotonicity), INV-5 (causality), INV-6 (determinism) R1 (no silent data loss), R5 (transactional mutation) Structural model INV-4 (KV conservation), INV-7 (signal freshness) R2 (sort map keys), R11 (guard division), R17 (signal freshness) Robustness \u2014 R3 (validate CLI flags), R19 (livelock protection), R20 (degenerate inputs) Cross-policy \u2014 R18 (CLI flag precedence)"},{"location":"standards/experiments/#experiment-design-rules","title":"Experiment Design Rules","text":""},{"location":"standards/experiments/#ed-1-controlled-comparison","title":"ED-1: Controlled comparison","text":"<p>Vary exactly one dimension between configurations. Everything else held constant (same model, same instances, same workload, same seed). If the experiment requires varying multiple dimensions, decompose into separate sub-experiments.</p>"},{"location":"standards/experiments/#ed-2-rate-awareness","title":"ED-2: Rate awareness","text":"<p>Many effects are rate-dependent (e.g., signal freshness only matters at high rates). When the hypothesis involves load-dependent behavior: - Run at the target rate where the effect is expected - Also run at a rate where the effect should vanish (to confirm the mechanism, not just the outcome) - Document the rate-dependent transition point if observed</p>"},{"location":"standards/experiments/#ed-3-precondition-verification","title":"ED-3: Precondition verification","text":"<p>Before comparing configurations, verify the experiment preconditions hold. Examples: - Testing SJF vs FCFS? Verify queue depth exceeds batch size (otherwise both produce identical batches). - Testing cache hit benefit? Verify KV blocks are large enough to hold the prefix (otherwise LRU eviction destroys it).</p> <p>Document the precondition check in the experiment script (not just in prose).</p>"},{"location":"standards/experiments/#ed-4-workload-seed-independence","title":"ED-4: Workload seed independence","text":"<p>Resolved (#284): CLI <code>--seed</code> now overrides the workload-spec YAML <code>seed:</code> field when explicitly passed. Behavior: - <code>--seed N --workload-spec w.yaml</code> \u2192 workload uses seed N (CLI override) - <code>--workload-spec w.yaml</code> (no <code>--seed</code>) \u2192 workload uses YAML <code>seed:</code> value (backward compatible) - CLI-generated workloads (<code>--rate</code>, <code>--num-requests</code>) \u2192 <code>--seed</code> controls everything (unchanged)</p> <p>For multi-seed experiments: simply vary <code>--seed</code> on the command line. No need to generate per-seed YAML copies.</p> <p>Note: The YAML <code>seed:</code> field still serves as the default seed for the workload when <code>--seed</code> is not explicitly specified. This enables the \"shareable workload\" pattern \u2014 distributing a YAML file that always produces the same workload by default.</p>"},{"location":"standards/experiments/#ed-5-reproducibility","title":"ED-5: Reproducibility","text":"<p>Every experiment must be reproducible from its artifacts alone: - <code>run.sh</code> must build the binary and run all variants - Exact seed values documented - Exact commit hash recorded (or the experiment is tied to a specific branch/PR) - No manual steps between script invocation and results</p>"},{"location":"standards/experiments/#ed-6-config-diff-against-reference-experiments","title":"ED-6: Config diff against reference experiments","text":"<p>When an experiment reuses calibration data from a prior experiment (e.g., \"H8 found the preemption cliff at 2100 blocks, so we use 2100\"), diff every CLI flag and YAML field between the two experiments. Document any differences. Even a single changed flag (e.g., routing policy) can invalidate the calibration.</p> <p>Evidence: H10 used <code>--routing-policy least-loaded</code> while H8 used the default <code>round-robin</code>. This shifted the preemption cliff, producing zero preemptions where H8 found 11%. The mismatch was not caught until post-publication code review.</p>"},{"location":"standards/experiments/#root-cause-verification","title":"Root Cause Verification","text":"<p>After analyzing results and before finalizing FINDINGS.md, every experiment MUST verify its causal explanations. This step exists because plausible narratives can pass review without being correct.</p>"},{"location":"standards/experiments/#rcv-1-every-causal-claim-must-cite-fileline","title":"RCV-1: Every causal claim must cite <code>file:line</code>","text":"<p>A root cause analysis that says \"the tiered cache increases total capacity\" without citing the code that does this is a hypothesis about the root cause, not a verified root cause. Trace the claim through the code: - Which function implements the claimed behavior? - What are the exact conditions under which it fires? - Does the claimed mechanism actually change the measured metric? - Tracing depth: The citation must trace to the code that directly modifies the measured metric, not just to the constructor or factory that creates the relevant object. Citing <code>NewKVStore</code> and claiming \"this creates a tiered cache with more capacity\" is insufficient \u2014 you must verify that the GPU block count actually changes in the created object.</p> <p>Evidence: H10 claimed \"CPU tier increases total effective capacity\" \u2014 but <code>NewKVStore</code> (<code>kv_store.go:31-36</code>) does not change GPU block count. The actual mechanism was <code>maybeOffload</code> preserving prefix hashes (<code>sim/kv/tiered.go</code>).</p>"},{"location":"standards/experiments/#rcv-2-every-surprise-must-have-a-first-principles-calculation","title":"RCV-2: Every \"surprise\" must have a first-principles calculation","text":"<p>Before labeling a result as \"surprising,\" compute the expected value from the system's parameters. If the result matches the calculation, it is not a surprise \u2014 it is the expected outcome of a mechanism you didn't initially consider.</p> <p>Evidence: H5 labeled 96% rejection as a \"surprise.\" But <code>admission.go:45</code> charges <code>len(req.InputTokens)</code> per request (mean=512). Token demand (1,024,000 tokens/s) exceeds supply (400 tokens/s) by 2,560x. The 96% rejection is the mathematically inevitable steady state.</p>"},{"location":"standards/experiments/#rcv-3-check-the-mechanism-not-just-the-direction","title":"RCV-3: Check the mechanism, not just the direction","text":"<p>Confirming that \"A is better than B\" is necessary but not sufficient. The root cause analysis must explain why through a specific code path. A correct directional result with an incorrect explanation is a ticking time bomb \u2014 the explanation will mislead future experiments.</p> <p>Paradox flag: If the proposed mechanism predicts the opposite direction of what would be intuitive (e.g., \"fewer cache hits improving performance\"), treat this as a red flag. Before accepting a paradoxical explanation, independently verify the underlying data. In H10, the claim \"fewer cache hits \u2192 better TTFT\" survived two rounds because the data (from a buggy analyzer) appeared to support it. The corrected data showed cache hits increased, resolving the paradox. When mechanism and intuition disagree, verify the data first.</p>"},{"location":"standards/experiments/#rcv-4-validate-causal-claims-with-control-experiments","title":"RCV-4: Validate causal claims with control experiments","text":"<p>When a mechanism is proposed (e.g., \"<code>maybeOffload</code> causes the TTFT improvement\"), design a control experiment that disables only that mechanism (e.g., <code>--kv-offload-threshold 1.0</code>). If the effect vanishes, the mechanism is confirmed. If it persists, the explanation is wrong.</p> <p>Evidence: H10 proposed <code>maybeOffload</code> as the mechanism. The control experiment (threshold=1.0) produced output byte-identical to single-tier, confirming <code>maybeOffload</code> as the sole cause. Without this control, the mechanism question (\"does maybeOffload cause the TTFT improvement?\") would have remained unverified.</p>"},{"location":"standards/experiments/#rcv-5-confirmation-bias-guard-devils-advocate","title":"RCV-5: Confirmation bias guard (Devil's Advocate)","text":"<p>Before sending FINDINGS.md to external review, the experimenter must write a Devil's Advocate section: 2-3 sentences arguing the opposite of the conclusion. This is a pre-review self-check that forces consideration of alternative interpretations.</p> <pre><code>## Devil's Advocate\n\n**If this is \"Confirmed,\" argue why it might be Refuted:**\nThe 69x TTFT improvement could be entirely from load shedding (96% rejection)\nrather than burst smoothing. A firewall that blocks all traffic also has great\nlatency for the requests that pass.\n\n**If this is \"Refuted,\" argue why it might be Confirmed:**\nThe calibrated bucket (cap=100K) showed a 4% improvement \u2014 small but consistent\nacross 2 of 3 seeds. This might be a real but tiny burst-smoothing effect masked\nby workload noise.\n</code></pre> <p>The reviewers see both the conclusion AND the counter-argument. This prevents the failure mode where the experimenter writes \"Confirmed\" and the reviewers are anchored by that label.</p> <p>Evidence: H5 was labeled \"Confirmed\" for three rounds. Nobody argued the alternative until Round 3's honest reassessment. A Devil's Advocate section in Round 1 would have surfaced \"could this be load shedding?\" immediately.</p>"},{"location":"standards/experiments/#rcv-6-mandatory-scope-and-limitations","title":"RCV-6: Mandatory Scope and Limitations","text":"<p>Every FINDINGS.md must include a Scope and Limitations section documenting: - Exact operating point tested (blocks, rate, seeds, instances, routing) - Parameters the findings depend on - What was NOT tested that could change the conclusion - Whether the finding generalizes or is specific to the tested configuration</p> <p>Evidence: H10's \"28% TTFT improvement\" is specific to GPU=2100 blocks near the preemption cliff. Without the scope section, this number would be cited as a general property of tiered KV caching.</p>"},{"location":"standards/experiments/#iterative-review-protocol","title":"Iterative Review Protocol","text":"<p>Canonical source: <code>docs/process/convergence.md</code>. If this section diverges, convergence.md is authoritative.</p> <p>Every hypothesis experiment iterates until convergence (max 10 rounds per gate) through three review gates, each using the universal convergence protocol:</p> <ol> <li>Design Review (5 perspectives) \u2014 after experiment design, before implementation</li> <li>Code Review (5 perspectives) \u2014 after implementing run.sh/analyze.py, before execution</li> <li>FINDINGS Review (10 perspectives) \u2014 after documenting results, before finalization</li> </ol> <p>Convergence: Zero CRITICAL and zero IMPORTANT items from any reviewer perspective in the current round. No minimum round count \u2014 convergence in Round 1 is valid if no reviewer flags any CRITICAL or IMPORTANT item. SUGGESTION-level items do not block convergence. See <code>docs/process/convergence.md</code> for the full protocol and severity definitions, and <code>docs/process/hypothesis.md</code> for reviewer prompts and perspective checklists.</p> <p>Why internal agents instead of external LLM reviews: Internal Task agents can read the actual source files, verify <code>file:line</code> citations, and cross-reference analyzer regexes against simulator output format strings \u2014 capabilities external LLM reviews lack. See <code>docs/process/hypothesis.md</code> for the full evidence and comparison table.</p>"},{"location":"standards/experiments/#hypothesis-resolution","title":"Hypothesis Resolution","text":"<p>Every hypothesis resolves to a status (did the prediction hold?) and a resolution (what do we do about it?). These are distinct \u2014 a \"confirmed\" hypothesis can still have a wrong-mechanism resolution that changes user guidance entirely.</p>"},{"location":"standards/experiments/#status-the-prediction","title":"Status (the prediction)","text":"Status Definition Example Confirmed The predicted directional outcome holds across all seeds H13: same seed \u2192 byte-identical output Confirmed with nuance The prediction holds but the mechanism or practical implications differ from expected H5: token-bucket reduces TTFT 69x but via 96% load shedding, not burst smoothing; no practical sweet spot Partially confirmed Some predictions hold, others don't, or the experiment tested something different than intended H14: routing pathological confirmed, scheduling showed double-inversion cancellation Refuted The predicted outcome does not hold across seeds (not yet observed \u2014 the refutation IS the value) Inconclusive Effect is within noise (&lt;10% in any seed) or parameter-dependent H5 exp4: calibrated bucket shows &lt;5% TTFT improvement"},{"location":"standards/experiments/#resolution-what-we-learned-and-what-to-do","title":"Resolution (what we learned and what to do)","text":"Resolution Definition Action Example Clean confirmation Hypothesis holds, mechanism matches prediction Document. No further action. H13, H3, H8 Confirmation with wrong mechanism Prediction holds directionally but the underlying cause differs Correct the explanation. May change user guidance entirely. H5: improvement is load shedding, not burst smoothing Confirmation with bug discovery Prediction holds but experiment surfaces code defects File issues (<code>--label bug</code>). Fix in separate PRs. H12: conservation holds but preemption panics. H14: routing works but 3 detector bugs. H10: tiered KV confirmed but analyzer bug masked preemptions for 2 rounds. Partial confirmation with surprise Some predictions fail; unexpected useful insights emerge Document surprise. May spawn new hypotheses. (use when the experiment finds something valuable but different from what was hypothesized) Refuted \u2014 mechanism not plausible The hypothesis assumed a mechanism that the implementation doesn't support File design issue if the mechanism should exist but doesn't. Document the actual mechanism. H5: hypothesis assumed burst smoothing, but per-input-token cost model (<code>admission.go:45</code>) makes burst smoothing structurally impossible at practical parameters Refuted \u2014 system design flaw Prediction fails because system doesn't work as designed File design issue (<code>--label design</code>). May require architectural change. (not yet observed) Refuted \u2014 wrong mental model Prediction fails because experimenter's assumptions were wrong Correct understanding. Document what the system actually does. (not yet observed) Inconclusive \u2014 parameter-dependent Effect exists at some parameters but not others Document the parameter boundary. May need recalibration. H5 exp4: &lt;5% effect with calibrated bucket Converged to open question Mechanism identified but directional explanation requires different tooling Mark as open. Propose specific tooling needed. (use when remaining questions require code instrumentation, not more experiment sweeps)"},{"location":"standards/experiments/#choosing-status-vs-resolution","title":"Choosing status vs resolution","text":"<p>The status answers \"did the number go the way we predicted?\" The resolution answers \"do we understand why and what to do about it?\" Always report both:</p> <pre><code>**Status:** Confirmed with nuance\n**Resolution:** Confirmation with wrong mechanism \u2014 token-bucket reduces TTFT\nvia load shedding (96% rejection), not burst smoothing. No practical sweet spot\nunder Gamma CV=3.5.\n</code></pre> <p>A common mistake is declaring \"Confirmed\" and stopping. The resolution is where the real value lives.</p>"},{"location":"standards/experiments/#findings-classification","title":"Findings Classification","text":"<p>Every experiment produces individual findings. Each finding MUST be classified independently of the hypothesis status:</p> Finding Type Definition Action Required Confirmation The hypothesis holds; the system works as designed Document in FINDINGS.md. No issues needed. Bug discovery The hypothesis failed due to a code defect File GitHub issue with <code>--label bug</code>. Fix in separate PR. New rule The experiment revealed a pattern that should be checked in all future PRs Add to <code>docs/standards/rules.md</code> with evidence. File issue with <code>--label enhancement</code> if code changes needed. New invariant The experiment revealed a property that must always hold Add to <code>docs/standards/invariants.md</code>. Design limitation The system works as coded but has an undocumented behavioral limitation Document in FINDINGS.md + file issue with <code>--label design</code> for design doc update. Surprise An unexpected result that doesn't fit other categories Document in FINDINGS.md. May spawn new hypotheses. Open question Mechanism identified but explanation incomplete; requires different tooling to resolve Mark explicitly in FINDINGS.md with proposed tooling/experiment."},{"location":"standards/experiments/#the-audit-step","title":"The Audit Step","text":"<p>After analyzing results, EVERY experiment MUST audit findings against <code>docs/standards/</code>:</p> <ol> <li>Do any findings reveal violations of existing rules or principles?</li> <li>Do any findings suggest a new rule, invariant, or principle is needed?</li> <li>Do any findings confirm that existing rules/invariants hold under new conditions?</li> </ol> <p>This audit is what makes experiments a feedback loop into the standards. Example: H3 confirmed that the llm-d default config is robust (confirmation) AND revealed that KV utilization is stale at high rates (design limitation -&gt; new rule R17 + new invariant INV-7 + 3 issues).</p>"},{"location":"standards/experiments/#experiment-artifacts","title":"Experiment Artifacts","text":"<p>Each hypothesis experiment lives in <code>hypotheses/&lt;name&gt;/</code> with:</p> File Purpose <code>run.sh</code> Self-contained script: builds binary, runs all variants, calls analyzer <code>analyze.py</code> Output parser producing formatted comparison tables <code>FINDINGS.md</code> Results, root cause analysis, findings classification, standards audit <code>*.yaml</code> (optional) Custom workload specs for this experiment <p>Scripts must be reproducible \u2014 running <code>./run.sh</code> on the same commit produces deterministic output.</p>"},{"location":"standards/invariants/","title":"BLIS System Invariants","text":"<p>Invariants are properties that must hold at all times during and after simulation. They are verified by invariant tests (see R7) and checked during self-audit (Step 4.75).</p> <p>Hypothesis family mapping: INV-1 through INV-3, INV-5, and INV-6 belong to the Scheduler invariants (safety/liveness) family. INV-4 (KV cache conservation), INV-7 (signal freshness), and INV-8 (work-conserving property) belong to the Structural model family. See <code>docs/standards/experiments.md</code> for hypothesis family definitions.</p>"},{"location":"standards/invariants/#inv-1-request-conservation","title":"INV-1: Request Conservation","text":"<p>Statement: <code>injected_requests == completed_requests + still_queued + still_running + dropped_unservable</code> at simulation end (all levels).</p> <p>Full pipeline: <code>num_requests == injected_requests + rejected_requests</code> (from anomaly counters).</p> <p>Verification: <code>sim/cluster/cluster_test.go</code> \u2014 conservation tests. Conservation fields (<code>still_queued</code>, <code>still_running</code>, <code>injected_requests</code>) are included in CLI JSON output.</p> <p>Evidence: Issue #183 \u2014 a silently-dropped request violated conservation for months.</p> <p>Experimental validation: H12 confirmed conservation across 10 policy configurations (67 invariant checks) \u2014 including round-robin, least-loaded, weighted (multiple scorer configs), SJF, priority-FCFS, token-bucket admission, and always-busiest. H8 confirmed conservation under extreme KV pressure (15 configurations). Full preemption-path validation is blocked by the panic bug (#293).</p>"},{"location":"standards/invariants/#inv-2-request-lifecycle","title":"INV-2: Request Lifecycle","text":"<p>Statement: Requests transition <code>queued -&gt; running -&gt; completed</code>. No invalid transitions. Requests not completed before horizon remain in current state.</p> <p>Verification: State machine assertions in request processing code.</p>"},{"location":"standards/invariants/#inv-3-clock-monotonicity","title":"INV-3: Clock Monotonicity","text":"<p>Statement: Simulation clock never decreases. Every event's timestamp &gt;= the previous event's timestamp.</p> <p>Verification: Clock is advanced in the event loop only via min-heap extraction, which guarantees non-decreasing order.</p>"},{"location":"standards/invariants/#inv-4-kv-cache-conservation","title":"INV-4: KV Cache Conservation","text":"<p>Statement: <code>allocated_blocks + free_blocks = total_blocks</code> at all times.</p> <p>Verification: Checked after every allocation/deallocation. Transactional allocation with rollback on mid-loop failure (R5).</p> <p>Operational note (H8): KV cache pressure exhibits a sharp cliff, not gradual degradation. In H8's workload, performance was identical above ~2200 blocks and collapsed below it (4.7x TTFT P99 increase with just 4.5% fewer blocks). Below ~1000 blocks, the preempt-requeue cycle can livelock (see R19). Capacity planning formula: <code>threshold \u2248 rate / num_instances \u00d7 (input_tokens + output_tokens) / block_size</code>.</p>"},{"location":"standards/invariants/#inv-5-causality","title":"INV-5: Causality","text":"<p>Statement: <code>arrival_time &lt;= enqueue_time &lt;= schedule_time &lt;= completion_time</code> for every request.</p> <p>Verification: Per-request metric timestamps recorded at each lifecycle stage. Invariant tests verify ordering for all completed requests.</p>"},{"location":"standards/invariants/#inv-6-determinism","title":"INV-6: Determinism","text":"<p>Statement: Same seed must produce byte-identical stdout across runs.</p> <p>Verification: Run same configuration twice with same seed; diff stdout. Wall-clock timing goes to stderr (not stdout).</p> <p>Common violation sources: - Go map iteration feeding output ordering (R2) - Floating-point accumulation order dependencies - Wall-clock-dependent randomness (must use PartitionedRNG) - Stateful scorers with non-deterministic internal state</p>"},{"location":"standards/invariants/#inv-7-signal-freshness-hierarchy","title":"INV-7: Signal Freshness Hierarchy","text":"<p>Statement: Routing snapshot signals have tiered freshness due to DES event ordering. Cluster events at tick T drain before instance events at tick T.</p> Signal Owner Freshness Updated By PendingRequests Cluster Synchronous <code>RoutingDecisionEvent.Execute()</code> QueueDepth Instance Stale within tick <code>QueuedEvent.Execute()</code> BatchSize Instance Stale within tick <code>StepEvent.Execute()</code> KVUtilization Instance Stale across batch steps <code>FormBatch()</code> -&gt; <code>AllocateKVBlocks()</code> CacheHitRate Instance Stale across batch steps <code>FormBatch()</code> <p>Design implication: <code>EffectiveLoad()</code> = <code>QueueDepth + BatchSize + PendingRequests</code> compensates for Tier 2 staleness by including the Tier 1 PendingRequests term. KVUtilization has no analogous compensation.</p> <p>Verification: H3 hypothesis experiment (<code>hypotheses/h3-signal-freshness/</code>).</p> <p>Evidence: Issues #282, #283. At rate=5000, kv-utilization-only routing produces 200x worse distribution uniformity than queue-depth.</p>"},{"location":"standards/invariants/#inv-8-work-conserving-property","title":"INV-8: Work-Conserving Property","text":"<p>Statement: After every step completion, if <code>WaitQ.Len() &gt; 0</code>, a <code>StepEvent</code> must exist in the event queue. The simulator must not idle while there is work waiting.</p> <p>Verification: <code>sim/simulator_test.go</code> \u2014 <code>TestWorkConserving_StepRestartsWhenWaitQNonEmpty</code>. Deterministic test with <code>MaxRunningReqs=1</code>, two requests arriving simultaneously. Without the property, the second request is stranded forever (no arrival to trigger a new StepEvent). With the property, both complete.</p> <p>Evidence: H-MMK experiment (PR #325) \u2014 without the work-conserving fix, W_q error was 151,000% at \u03c1=0.3. After fix, error dropped to 47% (remaining gap is discrete step processing, not a bug).</p> <p>Code location: Search for <code>// Work-conserving:</code> comment in <code>sim/simulator.go</code> \u2014 the <code>else</code> branch of <code>len(remaining) &gt; 0</code> checks <code>WaitQ.Len() &gt; 0</code> and schedules a new <code>StepEvent</code>.</p> <p>Hypothesis family: Structural model (same as INV-4, INV-7).</p>"},{"location":"standards/principles/","title":"BLIS Engineering Principles","text":"<p>Principles guide design decisions. The antipattern rules are specific, checkable manifestations of these principles. The invariants are properties that must always hold.</p>"},{"location":"standards/principles/#separation-of-concerns","title":"Separation of Concerns","text":"<ul> <li><code>sim/</code> is a library \u2014 never call <code>os.Exit</code>, <code>logrus.Fatalf</code>, or terminate the process. Return errors. Only <code>cmd/</code> may terminate. (Enforced by R6)</li> <li>Cluster-level policies (admission, routing) receive <code>*RouterState</code> with global view. Instance-level policies (priority, scheduler) receive only local data. Never leak cluster state to instance-level code.</li> <li>Bridge types (<code>RouterState</code>, <code>RoutingSnapshot</code>) live in <code>sim/</code> to avoid import cycles.</li> <li>Unidirectional dependency: <code>cmd/ -&gt; sim/cluster/ -&gt; sim/</code> and <code>sim/cluster/ -&gt; sim/trace/</code>. <code>sim/</code> never imports subpackages.</li> </ul>"},{"location":"standards/principles/#interface-design","title":"Interface Design","text":"<ul> <li>Single-method interfaces where possible (<code>AdmissionPolicy</code>, <code>RoutingPolicy</code>, <code>PriorityPolicy</code>, <code>InstanceScheduler</code>).</li> <li>Query methods must be pure \u2014 no side effects, no state mutation, no destructive reads. Separate <code>Get()</code> and <code>Consume()</code> for query-and-clear.</li> <li>Factory functions must validate inputs: <code>IsValid*()</code> check + switch/case + panic on unknown.</li> <li>Interfaces defined by behavioral contract, not one implementation's data model. (Enforced by R13)</li> <li>Methods operate within a single module's responsibility. (Enforced by R14)</li> </ul>"},{"location":"standards/principles/#configuration-design","title":"Configuration Design","text":"<ul> <li>Group configuration by module. (Enforced by R16)</li> <li>Each module's config independently specifiable and validatable.</li> </ul>"},{"location":"standards/principles/#canonical-constructors","title":"Canonical Constructors","text":"<ul> <li>Every struct constructed in multiple places needs a canonical constructor. Struct literals appear in exactly one place. (Enforced by R4)</li> <li>Before adding a field, grep for ALL construction sites.</li> </ul>"},{"location":"standards/principles/#output-channel-separation","title":"Output Channel Separation","text":"<ul> <li>stdout (deterministic): simulation results \u2014 metrics JSON, fitness scores, anomaly counters, KV cache metrics, per-SLO metrics, trace summaries. Use <code>fmt.Println</code>/<code>fmt.Printf</code>.</li> <li>stderr (diagnostic): configuration echoes, progress markers, warnings, errors. Use <code>logrus.*</code>, controlled by <code>--log</code>.</li> <li>Rule of thumb: if a user piping to a file would want to capture it, use <code>fmt</code>. If it's debugging context, use <code>logrus</code>.</li> </ul>"},{"location":"standards/principles/#error-handling-boundaries","title":"Error Handling Boundaries","text":"Layer Strategy Example CLI (<code>cmd/</code>) <code>logrus.Fatalf</code> for user errors Invalid <code>--rate</code> value Library (<code>sim/</code>) <code>panic()</code> for invariant violations Unknown policy name in factory Library (<code>sim/</code>) <code>error</code> return for recoverable failures File I/O, parse errors Runtime (<code>sim/</code>) <code>bool</code> return for expected conditions KV allocation failure -&gt; preemption <p>Never use <code>continue</code> in an error path without propagating, counting, or documenting why it's safe. (Enforced by R1)</p>"},{"location":"standards/principles/#bddtdd-development","title":"BDD/TDD Development","text":"<ol> <li>Write behavioral contracts first (GIVEN/WHEN/THEN)</li> <li>Implement tests before code</li> <li>Use table-driven tests</li> <li>Test laws, not just values \u2014 invariant tests alongside golden tests (Enforced by R7)</li> <li>Refactor survival test: \"Would this test still pass if the implementation were completely rewritten but the behavior preserved?\"</li> <li>THEN clauses drive test quality \u2014 structural THEN produces structural test</li> </ol> <p>Prohibited assertion patterns (structural \u2014 break on refactor): - Type assertions: <code>policy.(*ConcreteType)</code> - Internal field access: <code>obj.internalField</code> - Exact formula reproduction: <code>assert.Equal(score, 0.6*cache + 0.4*load)</code></p> <p>Required assertion patterns (behavioral \u2014 survive refactor): - Observable output: <code>assert.Equal(policy.Compute(req, clock), 0.0)</code> - Invariant verification: <code>assert.Equal(completed+queued+running+dropped, injected)</code> - Ordering/ranking: <code>assert.True(scoreA &gt; scoreB)</code></p>"},{"location":"standards/principles/#test-suite-performance","title":"Test Suite Performance","text":"<p>As the test suite grows (invariant tests, golden tests, hypothesis-promoted regression tests), keep total <code>go test ./...</code> time manageable:</p> <ul> <li>Individual test budget: No single test should exceed 5 seconds without using <code>testing.Short()</code> to provide a fast-path skip. Tests that run full cluster simulations (e.g., 10K requests across 8 instances) should check <code>testing.Short()</code> and reduce to a minimal configuration.</li> <li>CI target: Total <code>go test ./...</code> should complete in under 60 seconds. If it exceeds this, audit for tests that can use smaller configurations without losing behavioral coverage.</li> <li>Benchmark isolation: Performance benchmarks (<code>Benchmark*</code> functions) run only with <code>go test -bench=.</code>, never in the default <code>go test ./...</code> path. This is Go's default behavior \u2014 just don't put benchmark assertions in regular tests.</li> </ul>"},{"location":"standards/principles/#documentation-single-source-of-truth","title":"Documentation Single Source of Truth","text":"<p>Every piece of documentation lives in exactly one canonical location. Other files may contain working copies (summaries for quick reference) with explicit canonical-source headers.</p> <p>The canonical-source pattern:</p> <p>Canonical source: <code>docs/standards/rules.md</code>. If this section diverges, rules.md is authoritative.</p> <p>When updating any standard, invariant, rule, or recipe: 1. Update the canonical source FIRST 2. Then update any working copies that reference it 3. If you can't update working copies immediately, the canonical-source header ensures readers know which version to trust</p> <p>Single-source-of-truth map:</p> Content Canonical Source Working Copies Antipattern rules (R1-R20) <code>docs/standards/rules.md</code> CLAUDE.md (table), CONTRIBUTING.md (checklist) System invariants (INV-1\u2013INV-8) <code>docs/standards/invariants.md</code> CLAUDE.md (summary), <code>docs/design/core-engine.md</code> (formulas), <code>docs/design/architecture.md</code> (signal freshness) Engineering principles <code>docs/standards/principles.md</code> CLAUDE.md (summary) Extension recipes (policies, scorers, KV tiers) <code>docs/extension-recipes.md</code> \u2014 Design process <code>docs/process/design.md</code> CONTRIBUTING.md (summary) Macro-plan process <code>docs/process/macro-plan.md</code> CONTRIBUTING.md (summary) File organization and architecture CLAUDE.md (File Organization tree) README.md (Project Structure tree) Hypothesis catalog and specifications <code>docs/plans/research.md</code> \u2014 Experiment status and coverage <code>hypotheses/README.md</code> \u2014 Experiment standards <code>docs/standards/experiments.md</code> \u2014 (note: review protocol subsection references <code>docs/process/convergence.md</code> as canonical) Convergence protocol <code>docs/process/convergence.md</code> <code>docs/process/hypothesis.md</code> (summary), <code>docs/process/pr-workflow.md</code> (summary), <code>docs/standards/experiments.md</code> (review protocol summary), <code>.claude/skills/convergence-review/SKILL.md</code> (protocol copy) Hypothesis experiment workflow <code>docs/process/hypothesis.md</code> CONTRIBUTING.md (summary), hypotheses/README.md (step list), <code>.claude/skills/hypothesis-experiment/SKILL.md</code> (workflow steps), <code>.claude/skills/hypothesis-experiment/review-prompts.md</code> (perspective prompts) PR workflow <code>docs/process/pr-workflow.md</code> CONTRIBUTING.md (summary), <code>.claude/skills/convergence-review/pr-prompts.md</code> (perspective prompts)"},{"location":"standards/rules/","title":"BLIS Antipattern Rules","text":"<p>Every rule traces to a real bug, design failure, or hypothesis finding. Rules are enforced at three checkpoints: - PR template \u2014 checklist before merge - Micro-plan Phase 8 \u2014 checklist before implementation - Self-audit Step 4.75 \u2014 deliberate critical thinking before commit</p> <p>For the full process, see docs/process/pr-workflow.md.</p>"},{"location":"standards/rules/#priority-tiers","title":"Priority Tiers","text":"<p>New contributors: focus on Critical rules first. These protect correctness \u2014 violating them produces wrong results or crashes. Important rules protect code quality and maintainability. Hygiene rules keep the codebase clean over time.</p> Tier Rules Why Critical (correctness) R1, R4, R5, R6, R11, R19 Violations produce silent data loss, panics, conservation invariant breaks, or infinite loops Important (quality) R2, R3, R7, R8, R9, R10, R13, R14, R17, R18, R20 Violations produce non-determinism, validation gaps, silent misconfig, interface debt, or undetected anomalies Hygiene (maintenance) R12, R15, R16 Violations produce stale references, config sprawl, or misleading test baselines <p>All 20 rules apply to every PR. The tiers help you prioritize during review \u2014 check Critical rules first.</p>"},{"location":"standards/rules/#rules","title":"Rules","text":""},{"location":"standards/rules/#r1-no-silent-data-loss","title":"R1: No silent data loss","text":"<p>Every error path must either return an error, panic with context, or increment a counter. A <code>continue</code> or early <code>return</code> that silently drops a request, metric, or allocation is a correctness bug.</p> <p>Evidence: Issue #183 \u2014 a KV allocation failure silently dropped a request. The golden test perpetuated the bug for months because it captured \"499 completions\" as the expected value.</p> <p>Additional evidence: H14 hypothesis experiment \u2014 HOL blocking detector silently returns 0 instead of flagging the most extreme imbalance case when <code>always-busiest</code> routes all traffic to one instance (bug #291).</p> <p>Check: For every <code>continue</code> or early <code>return</code> in new code, verify the error is propagated, counted, or documented as safe.</p> <p>Enforced: PR template, micro-plan Phase 8, self-audit dimension 9.</p>"},{"location":"standards/rules/#r2-sort-map-keys-before-float-accumulation","title":"R2: Sort map keys before float accumulation","text":"<p>Go map iteration is non-deterministic. Any <code>for k, v := range someMap</code> that feeds a running sum (<code>total += v</code>) or determines output ordering must sort keys first. Unsorted iteration violates the determinism invariant (INV-6).</p> <p>Evidence: Five sites iterated Go maps to accumulate floats or determine output ordering, violating determinism.</p> <p>Check: For every <code>range</code> over a map, check if the loop body accumulates floats or produces ordered output. If so, sort keys first.</p> <p>Enforced: PR template, micro-plan Phase 8, self-audit dimension 3.</p>"},{"location":"standards/rules/#r3-validate-all-numeric-cli-flags","title":"R3: Validate ALL numeric CLI flags","text":"<p>Every numeric flag (<code>--rate</code>, <code>--fitness-weights</code>, <code>--kv-cpu-blocks</code>, etc.) must be validated for: zero, negative, NaN, Inf, and empty string. Missing validation causes infinite loops (Rate=0) or wrong results (NaN weights).</p> <p>Evidence: <code>--rate 0</code> caused an infinite loop deep in the simulation. <code>--snapshot-refresh-interval</code> was added without validation (#281).</p> <p>Check: For every new CLI flag, add validation in <code>cmd/root.go</code> with <code>logrus.Fatalf</code> for invalid values.</p> <p>Enforced: PR template, micro-plan Phase 8, self-audit dimension 6.</p>"},{"location":"standards/rules/#r4-construction-site-audit","title":"R4: Construction site audit","text":"<p>Before adding a field to a struct, find every place that struct is constructed as a literal. If there are multiple sites, either add a canonical constructor or update every site. Missing a site causes silent field-zero bugs.</p> <p>Evidence: Issue #181 \u2014 adding <code>InstanceID</code> to per-request metrics required changes in 4 files. Three construction sites for <code>RequestMetrics</code> existed, and one was missed initially.</p> <p>Check: <code>grep 'StructName{' across the codebase</code>. List every site. Update all or refactor to canonical constructor.</p> <p>Enforced: PR template, micro-plan Phase 0 + Phase 8, self-audit dimension 8.</p>"},{"location":"standards/rules/#r5-transactional-state-mutation","title":"R5: Transactional state mutation","text":"<p>Any loop that allocates resources (blocks, slots, counters) must handle mid-loop failure by rolling back all mutations from previous iterations. A partial allocation that returns <code>false</code> without cleanup violates conservation invariants.</p> <p>Evidence: KV block allocation (<code>AllocateKVBlocks</code>) had a mid-loop failure path that didn't roll back previously allocated blocks, violating KV conservation (INV-4).</p> <p>Additional evidence: H12 hypothesis experiment \u2014 preemption loop in <code>sim/simulator.go:383</code> accesses <code>RunningBatch.Requests[len-1]</code> without bounds check. When all running requests are evicted and the batch is empty, the code panics with index out of range (bug #293).</p> <p>Check: For every loop that mutates state, verify the failure path rolls back all mutations.</p> <p>Enforced: PR template, micro-plan Phase 8, self-audit dimension 9.</p>"},{"location":"standards/rules/#r6-no-logrusfatalf-in-library-code","title":"R6: No logrus.Fatalf in library code","text":"<p>The <code>sim/</code> package tree must never terminate the process \u2014 return errors so callers can handle them. Only <code>cmd/</code> may terminate. This enables embedding, testing, and adapters.</p> <p>Evidence: Library code that called <code>logrus.Fatalf</code> prevented test isolation and made the simulator non-embeddable.</p> <p>Check: <code>grep -r 'logrus.Fatal\\|os.Exit' sim/</code> must return zero results.</p> <p>Enforced: PR template, micro-plan Phase 8.</p>"},{"location":"standards/rules/#r7-invariant-tests-alongside-golden-tests","title":"R7: Invariant tests alongside golden tests","text":"<p>Golden tests (comparing against known-good output) are regression freezes, not correctness checks. If a bug exists when the golden values are captured, the golden test perpetuates the bug. Every subsystem that has golden tests must also have invariant tests that verify conservation laws, causality, and determinism.</p> <p>Evidence: Issue #183 \u2014 the codellama golden dataset expected 499 completions because one request was silently dropped. A conservation invariant test would have caught it on day one.</p> <p>Check: For every golden test, ask: \"If this expected value were wrong, would any other test catch it?\" If no, add an invariant test.</p> <p>Enforced: PR template, micro-plan Phase 6 + Phase 8, self-audit dimension 7.</p>"},{"location":"standards/rules/#r8-no-exported-mutable-maps","title":"R8: No exported mutable maps","text":"<p>Validation lookup maps (e.g., <code>validRoutingPolicies</code>) must be unexported. Expose through <code>IsValid*()</code> accessor functions. Exported maps allow callers to mutate global state, breaking encapsulation and enabling hard-to-trace bugs.</p> <p>Evidence: Exported mutable maps were found during hardening audit \u2014 callers could silently add entries to validation maps.</p> <p>Check: <code>grep -r 'var [A-Z].*map\\[' sim/</code> must return zero mutable map results.</p> <p>Enforced: PR template, micro-plan Phase 8.</p>"},{"location":"standards/rules/#r9-pointer-types-for-yaml-zero-value-ambiguity","title":"R9: Pointer types for YAML zero-value ambiguity","text":"<p>YAML config structs must use <code>*float64</code> (pointer) for fields where zero is a valid user-provided value, to distinguish \"not set\" (nil) from \"set to zero\" (0.0). Using bare <code>float64</code> causes silent misconfiguration when users intentionally set a value to zero.</p> <p>Evidence: YAML fields with bare <code>float64</code> couldn't distinguish \"user set this to 0\" from \"user didn't set this.\"</p> <p>Check: For every new YAML config field where zero is meaningful, use a pointer type.</p> <p>Enforced: Micro-plan Phase 8.</p>"},{"location":"standards/rules/#r10-strict-yaml-parsing","title":"R10: Strict YAML parsing","text":"<p>Use <code>yaml.KnownFields(true)</code> or equivalent strict parsing for all YAML config loading. Typos in field names must cause parse errors, not silent acceptance of malformed config.</p> <p>Evidence: YAML typos in field names were silently accepted, producing default behavior instead of the user's intended configuration.</p> <p>Check: Every <code>yaml.Unmarshal</code> or decoder usage must enable strict/known-fields mode.</p> <p>Enforced: Micro-plan Phase 8.</p>"},{"location":"standards/rules/#r11-guard-division-in-runtime-computation","title":"R11: Guard division in runtime computation","text":"<p>Any division where the denominator derives from runtime state (batch size, block count, request count, bandwidth) must guard against zero. CLI validation (R3) catches input zeros at the boundary; this rule catches intermediate zeros that arise during simulation.</p> <p>Evidence: <code>utilization = usedBlocks / totalBlocks</code> when no blocks are configured; <code>avgLatency = sum / count</code> when count is zero.</p> <p>Check: For every division, verify the denominator is either (a) guarded by an explicit zero check, or (b) proven non-zero by a documented invariant.</p> <p>Enforced: Micro-plan Phase 8.</p>"},{"location":"standards/rules/#r12-golden-dataset-regenerated-when-output-changes","title":"R12: Golden dataset regenerated when output changes","text":"<p>When a PR changes output format, metrics, or default behavior, the golden dataset must be regenerated and the regeneration command documented. Golden tests that pass with stale expected values provide false confidence.</p> <p>Evidence: Present in CONTRIBUTING.md and PR template but not in CLAUDE.md's numbered rules \u2014 an inconsistency this consolidation resolves.</p> <p>Check: If <code>go test ./sim/... -run Golden</code> fails after your changes, regenerate and document the command.</p> <p>Enforced: PR template, micro-plan Phase 8.</p>"},{"location":"standards/rules/#r13-interfaces-accommodate-multiple-implementations","title":"R13: Interfaces accommodate multiple implementations","text":"<p>New interfaces must accommodate at least two implementations (even if only one exists today). No methods that only make sense for one backend.</p> <p>Evidence: <code>KVStore</code> interface has methods exposing block-level semantics. A distributed KV cache like LMCache thinks in tokens and layers, not blocks. The interface encodes vLLM's implementation model rather than an abstract behavioral contract.</p> <p>Check: For every new interface, ask: \"Could a second backend implement this without dummy methods?\"</p> <p>Enforced: Micro-plan Phase 8.</p> <p>Previously: principle in CLAUDE.md \"Interface design\" section. Promoted to numbered rule for checkability.</p>"},{"location":"standards/rules/#r14-no-multi-module-methods","title":"R14: No multi-module methods","text":"<p>No method should span multiple module responsibilities (scheduling + latency estimation + metrics in one function). Extract each concern into its module's interface.</p> <p>Evidence: <code>Simulator.Step()</code> is 134 lines mixing scheduling, latency estimation, token generation, completion, and metrics. Impossible to swap the latency model without modifying this method.</p> <p>Check: If a method touches &gt;1 module's concern, extract each concern.</p> <p>Enforced: Micro-plan Phase 8.</p> <p>Previously: principle in CLAUDE.md \"Interface design\" section. Promoted to numbered rule for checkability.</p>"},{"location":"standards/rules/#r15-resolve-stale-pr-references","title":"R15: Resolve stale PR references","text":"<p>After completing a PR, grep for references to that PR number (<code>planned for PR N</code>, <code>TODO.*PR N</code>) in the codebase. Resolve all stale references.</p> <p>Evidence: Multiple stale comments referencing completed PRs accumulated over time, misleading future developers about what was implemented vs planned.</p> <p>Check: <code>grep -rn 'planned for PR\\|TODO.*PR' --include='*.go' --include='*.md'</code> for the current PR number.</p> <p>Enforced: Micro-plan Phase 8.</p>"},{"location":"standards/rules/#r16-group-configuration-by-module","title":"R16: Group configuration by module","text":"<p>Configuration parameters must be grouped by module \u2014 not added to a monolithic config struct mixing unrelated concerns. Each module's config should be independently specifiable and validatable.</p> <p>Evidence: <code>SimConfig</code> previously combined hardware identity, model parameters, simulation parameters, and policy choices in 23 flat fields. Resolved in #350: <code>SimConfig</code> now embeds 6 module-scoped sub-configs (<code>KVCacheConfig</code>, <code>BatchConfig</code>, <code>LatencyCoeffs</code>, <code>ModelHardwareConfig</code>, <code>PolicyConfig</code>, <code>WorkloadConfig</code>). Factory signatures accept the narrowest sub-config (e.g., <code>NewKVStore(KVCacheConfig)</code>).</p> <p>Check: New config parameters go into the appropriate sub-config in <code>sim/config.go</code>, not directly into <code>SimConfig</code>.</p> <p>Enforced: Micro-plan Phase 8.</p> <p>Previously: principle in CLAUDE.md \"Configuration design\" section. Promoted to numbered rule for checkability.</p>"},{"location":"standards/rules/#r17-document-signal-freshness-for-routing-inputs","title":"R17: Document signal freshness for routing inputs","text":"<p>Routing snapshot signals have different freshness guarantees due to DES event ordering. Scorer authors must understand which signals are synchronously fresh and which are stale. Any scorer intended for high-rate routing must either use a synchronously-fresh signal or be combined with one that does.</p> <p>Evidence: H3 hypothesis experiment (#279) \u2014 kv-utilization scorer produced 200x worse distribution uniformity than queue-depth at rate=5000. See issues #282, #283.</p> <p>Freshness hierarchy: - Tier 1 \u2014 Synchronously fresh (cluster-owned): PendingRequests - Tier 2 \u2014 Stale within tick (instance-owned): QueueDepth, BatchSize - Tier 3 \u2014 Stale across batch steps (instance-owned): KVUtilization, CacheHitRate</p> <p>Check: When writing a new scorer, identify which snapshot fields it reads and their freshness tier. If using only Tier 3 signals, document why or combine with a Tier 1 scorer.</p> <p>Enforced: Design review, scorer implementation review.</p>"},{"location":"standards/rules/#r18-cli-flag-precedence-over-defaults","title":"R18: CLI flag precedence over defaults","text":"<p>When the CLI binary loads default values from <code>defaults.yaml</code>, it must not silently overwrite user-provided flag values. Always check <code>cmd.Flags().Changed(\"&lt;flag&gt;\")</code> before applying a default. A user who explicitly passes <code>--total-kv-blocks 50</code> must get 50, not the model's default of 132,139.</p> <p>Evidence: H9 hypothesis experiment \u2014 <code>GetCoefficients()</code> unconditionally overwrote <code>totalKVBlocks</code> with the model default, silently destroying the CLI flag value. The entire H9 Experiment 3 (cache capacity independence) produced invalid results. Bug #285, fix cbb0de7.</p> <p>Check: For every assignment from <code>defaults.yaml</code> to a CLI-parsed variable, verify <code>cmd.Flags().Changed()</code> is checked first. Grep for <code>GetCoefficients</code> and <code>defaults.yaml</code> assignment patterns.</p> <p>Enforced: PR template, micro-plan Phase 8, self-audit dimension 6.</p>"},{"location":"standards/rules/#r19-livelock-protection-for-unbounded-retry-loops","title":"R19: Livelock protection for unbounded retry loops","text":"<p>Loops where the exit condition depends on resource availability that may never be satisfied (e.g., preempt \u2192 requeue \u2192 schedule \u2192 preempt) must have a circuit breaker: maximum iteration count, progress assertion, or bounded retry with error escalation. An infinite loop in a deterministic simulator is indistinguishable from a hang.</p> <p>Evidence: H8 hypothesis experiment \u2014 with total KV blocks below ~1000 (insufficient for any single request), the preempt-requeue cycle ran indefinitely with no termination condition, no max-retry limit, and no progress check.</p> <p>Check: For every loop that retries an operation after a resource failure, verify there is an explicit bound or progress check. Pay special attention to preemption, eviction, and reallocation loops.</p> <p>Enforced: PR template, micro-plan Phase 8, self-audit dimension 4.</p>"},{"location":"standards/rules/#r20-degenerate-input-handling-in-detectors-and-analyzers","title":"R20: Degenerate input handling in detectors and analyzers","text":"<p>Anomaly detectors and metric analyzers must explicitly handle degenerate inputs: empty sample sets, single-instance concentration, all-zero distributions, and cross-class comparisons. The degenerate case is often the most important one to detect \u2014 a detector that returns \"no anomaly\" when all traffic hits one instance is worse than useless.</p> <p>Evidence: H14 hypothesis experiment \u2014 two detector failures: (1) HOL blocking detector requires \u22652 instances with samples, but <code>always-busiest</code> routes ALL traffic to one instance, leaving 3 empty \u2014 detector returns 0 for the most extreme HOL case (bug #291). (2) Priority inversion detector uses a 2x threshold that conflates workload heterogeneity with scheduling unfairness \u2014 7,463 false positives with normal configs (bug #292).</p> <p>Check: For every detector or analyzer, identify what happens when one or more inputs are empty, zero, or maximally skewed. Write tests for these degenerate cases.</p> <p>Enforced: PR template, micro-plan Phase 8, self-audit dimension 9.</p>"},{"location":"standards/rules/#quick-reference-checklist","title":"Quick Reference Checklist","text":"<p>For PR authors \u2014 check each rule before submitting:</p> <ul> <li> R1: No silent <code>continue</code>/<code>return</code> dropping data</li> <li> R2: Map keys sorted before float accumulation or ordered output</li> <li> R3: Every new CLI flag validated (zero, negative, NaN, Inf)</li> <li> R4: All struct construction sites audited for new fields</li> <li> R5: Resource allocation loops handle mid-loop failure with rollback</li> <li> R6: No <code>logrus.Fatalf</code> or <code>os.Exit</code> in <code>sim/</code> packages</li> <li> R7: Invariant tests alongside any golden tests</li> <li> R8: No exported mutable maps</li> <li> R9: <code>*float64</code> for YAML fields where zero is valid</li> <li> R10: YAML strict parsing (<code>KnownFields(true)</code>)</li> <li> R11: Division by runtime-derived denominators guarded</li> <li> R12: Golden dataset regenerated if output changed</li> <li> R13: New interfaces work for 2+ implementations</li> <li> R14: No method spans multiple module responsibilities</li> <li> R15: Stale PR references resolved</li> <li> R16: Config params grouped by module</li> <li> R17: Routing scorer signals documented for freshness tier</li> <li> R18: CLI flag values not silently overwritten by defaults.yaml</li> <li> R19: Unbounded retry/requeue loops have circuit breakers</li> <li> R20: Detectors and analyzers handle degenerate inputs (empty, skewed, zero)</li> </ul>"},{"location":"standards/rules/#rule-lifecycle","title":"Rule Lifecycle","text":"<p>Rules are born from real bugs and live as long as they prevent real bugs. As the codebase evolves, some rules may become automated, consolidated, or no longer applicable.</p>"},{"location":"standards/rules/#lifecycle-states","title":"Lifecycle States","text":"State Meaning Action Active Rule prevents a class of bugs that can still occur Check in every PR review Automated Rule is enforced by CI (linter, test, build) Note the enforcement mechanism; keep for documentation but skip manual checks Consolidated Rule merged into a broader rule Redirect to the parent rule; remove from checklist Retired The class of bugs is no longer possible (e.g., the vulnerable code path was removed) Move to a \"Retired Rules\" appendix with rationale"},{"location":"standards/rules/#when-to-consolidate","title":"When to Consolidate","text":"<p>If two rules address the same root principle and checking one always catches the other, consolidate them. Example: if a linter rule were added that caught all R2 violations (unsorted map iteration), R2 could move to \"Automated\" state.</p>"},{"location":"standards/rules/#quarterly-review","title":"Quarterly Review","text":"<p>Every ~10 PRs or quarterly (whichever comes first), scan the rule list: 1. Can any rule be automated by a linter or CI check? 2. Are any two rules always checked together and catching the same class of bugs? 3. Has the code path that motivated any rule been removed?</p> <p>File an issue for each proposed state change. Do not retire rules silently.</p>"},{"location":"standards/rules/#current-state","title":"Current State","text":"<p>All 20 rules (R1-R20) are Active as of 2026-02-26. No rules have been automated, consolidated, or retired.</p>"},{"location":"templates/design-guidelines/","title":"BLIS Design Guidelines: Principles for Robust Simulation Design and Modular Extension","text":"<p>Date: 2026-02-18 Status: Draft (pending review) Species: System Overview</p>"},{"location":"templates/design-guidelines/#1-purpose-scope","title":"1. Purpose &amp; Scope","text":"<p>This document serves two audiences: 1. Design doc authors (human or Claude) \u2014 guidance on writing design docs that stay durable and useful across the project lifecycle 2. Module developers \u2014 guidance on extending BLIS with new modules that fit the architecture and enable parallel development</p> <p>What this document IS: - A target architecture specification that BLIS will be refactored toward - A set of principles grounded in DES methodology and project experience (12 completed PRs, 20+ issues) - A reference for evaluating whether a design doc or a new module meets BLIS's quality bar</p> <p>What this document is NOT: - Not an implementation plan (refactoring happens in separate PRs, each following <code>docs/process/pr-workflow.md</code>) - Not a replacement for CLAUDE.md (which captures engineering rules and code-level patterns) - Not a replacement for the micro-plan template (which captures PR-level planning structure)</p>"},{"location":"templates/design-guidelines/#relationship-to-existing-docs","title":"Relationship to Existing Docs","text":"<pre><code>Design Guidelines (this doc)          \u2190 Principles, target architecture, extension framework\n    \u2193 informs\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Design Docs  \u2190\u2192  Macro Plan                    \u2502\n\u2502       \u2193                \u2193                        \u2502\n\u2502  Micro Plans     Micro Plans                    \u2502\n\u2502       \u2193                \u2193                        \u2502\n\u2502  CLAUDE.md (updated by each PR)                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Scenario Path Large multi-PR feature Design doc \u2192 Macro plan \u2192 Micro plans per PR Single-PR feature Design doc \u2192 Micro plan directly Refactoring / bug fix Issue or design doc \u2192 Micro plan directly Macro plan PR Macro plan section \u2192 Micro plan <p>Key distinction: This document describes the target state and design principles. CLAUDE.md describes the current state and implementation rules. Where they diverge, this document is aspirational and CLAUDE.md is authoritative for today's code.</p>"},{"location":"templates/design-guidelines/#2-des-design-foundations","title":"2. DES Design Foundations","text":"<p>BLIS is a discrete-event simulator. Design docs for BLIS should be informed by established DES methodology (Banks et al., Discrete-Event System Simulation; Misra, Distributed Discrete-Event Simulation, 1986).</p> <p>Core Principle: Abstraction must be justified. Every state variable, event type, and random input should be defensible in terms of the questions the simulator will answer \u2014 not fidelity for its own sake.</p>"},{"location":"templates/design-guidelines/#21-model-scoping-banks-et-al","title":"2.1 Model Scoping (Banks et al.)","text":"<p>Before including a component in BLIS, evaluate it against these six criteria:</p> <ol> <li>Will including it significantly affect the accuracy of results for the target analysis questions?</li> <li>What level of accuracy is actually required for the analysis to be useful?</li> <li>Can the component's data requirements be satisfied (alpha/beta coefficients, hardware specs, workload traces)?</li> <li>What is the cost of inclusion \u2014 code complexity, maintenance burden, configuration surface?</li> <li>What breaks if we omit it? (Sensitivity analysis \u2014 if removing it changes results by &lt;5%, defer it)</li> <li>What is the simplest version that answers the same questions? (Start coarse, refine only with evidence)</li> </ol> <p>These criteria operationalize \"YAGNI\" for simulation design. Example: the PR12 pre-design chose synchronous transfer latency over event-based transfers because it answered the same questions with ~100 fewer LOC.</p>"},{"location":"templates/design-guidelines/#22-event-design","title":"2.2 Event Design","text":"<ul> <li>Events must be minimal and atomic \u2014 each event corresponds to exactly one state change.</li> <li>Classify new events as exogenous (arrivals, environment changes \u2014 driven by workload input) or endogenous (completions, scheduling decisions, scaling actions \u2014 driven by internal state transitions). This classification must appear in design docs.</li> <li>New events must specify their priority constant for tie-breaking within BLIS's <code>(timestamp, priority, seqID)</code> ordering scheme.</li> </ul>"},{"location":"templates/design-guidelines/#23-state-vs-statistics-separation","title":"2.3 State vs. Statistics Separation","text":"<ul> <li>State variables evolve the system (queues, caches, clocks, request lifecycle) \u2014 they are inputs to event handlers.</li> <li>Statistics are derived from state trajectories (TTFT distributions, throughput, utilization) \u2014 they are outputs for analysis.</li> <li>These must be decoupled. A module that mixes state mutation and metric computation in the same method is violating this principle.</li> </ul>"},{"location":"templates/design-guidelines/#24-verification-and-validation","title":"2.4 Verification and Validation","text":"<ul> <li>Verification: the code correctly implements the conceptual model (behavioral tests, invariant tests, golden tests).</li> <li>Validation: the model accurately represents real-system behavior (calibration against real servers, sensitivity analysis, comparison with published benchmarks).</li> <li>A verified simulator can be an invalid model. (BLIS issue #183: golden tests verified the code did what it did, but an invariant test would have caught the dropped request.)</li> <li>Design docs must specify both: how will correctness be verified (which invariants?) AND how will fidelity be validated (against what real-system data?).</li> </ul>"},{"location":"templates/design-guidelines/#25-randomness-as-first-class-concern","title":"2.5 Randomness as First-Class Concern","text":"<ul> <li>All randomness flows through <code>PartitionedRNG</code> with named subsystems.</li> <li>New modules that introduce randomness must declare their subsystem name and justify that their random draws don't interfere with existing streams.</li> <li>Design docs should consider whether the feature enables common random numbers experiments (paired comparison of two configurations using the same random stream).</li> </ul>"},{"location":"templates/design-guidelines/#26-des-design-review-checklist","title":"2.6 DES Design Review Checklist","text":"<p>Every BLIS design doc must answer these questions:</p> Question Principle What analysis questions does this design help answer? Model scoping What is modeled, simplified, and deliberately omitted? (Table format) Model scoping What events are introduced or modified? Exogenous or endogenous? Event design How do new events interact with existing tie-breaking rules? Event design What new state is introduced? Who owns it? State/statistics separation What new metrics are derived? Collected incrementally or on demand? State/statistics separation How will correctness be verified? (Which invariants?) Verification How will fidelity be validated? (Against what data?) Validation Does this introduce new randomness? Which PartitionedRNG subsystem? Randomness What is the simplest version that answers the same questions? Model scoping"},{"location":"templates/design-guidelines/#3-design-doc-guidelines","title":"3. Design Doc Guidelines","text":""},{"location":"templates/design-guidelines/#31-the-staleness-test","title":"3.1 The Staleness Test","text":"<p>Before including any content in a design doc, apply this test:</p> <p>\"If the implementation changes this detail during micro-planning, will the design doc silently mislead future readers?\"</p> <ul> <li>Durable content (include): invariants, modeling decisions, fidelity trade-offs, extension points described behaviorally, decision rationale with alternatives considered.</li> <li>Fragile content (exclude): Go struct field lists, method implementations, file paths with line numbers, specific parameter names.</li> </ul> <p>The dividing line: describe what crosses a boundary and why, not how the boundary is implemented.</p>"},{"location":"templates/design-guidelines/#32-four-design-doc-species","title":"3.2 Four Design Doc Species","text":"<p>Not all design docs serve the same purpose. Choose the right species based on scope:</p> Species When to Use Structure Example Decision Record Single-PR architectural choices that need trade-off analysis Numbered decisions, each with Problem / Decision / Rationale / Alternatives PR12 pre-design (9 decisions) Specification New subsystem with precise behavioral requirements Behavioral contracts, math/formulas, input/output schemas, validation criteria Workload generator design Problem Analysis Refactoring motivated by identified friction or bugs Extension scenario analysis, antipattern catalog with evidence, phased fix plan Hardening design System Overview Multi-PR feature spanning multiple modules Concept model, module interactions, invariants, phased roadmap Evolutionary policy optimization design <p>A design doc should declare its species at the top so readers know what to expect.</p>"},{"location":"templates/design-guidelines/#33-required-sections-all-species","title":"3.3 Required Sections (All Species)","text":"<p>Every BLIS design doc, regardless of species, must include:</p> <ol> <li>Motivation \u2014 What problem does this solve? What can't users do today? (2-5 sentences, no jargon)</li> <li>Scope \u2014 What's in, what's explicitly out, what's deferred to later</li> <li>Modeling Decisions \u2014 What is modeled, simplified, and omitted (table format per Section 2.1)</li> <li>Invariants \u2014 What must always hold after this design is implemented? What must never happen? (Named: INV-1, INV-2, ...)</li> <li>Decisions with Trade-offs \u2014 For each non-obvious choice: what alternatives were considered, why this one won, what breaks if it's wrong</li> <li>Extension Points \u2014 Where do future extensions plug in? What is the default behavior? What would a non-default look like?</li> <li>Validation Strategy \u2014 How will correctness be verified (invariants) and fidelity be validated (calibration, comparison)?</li> <li>DES Checklist \u2014 Completed checklist from Section 2.6</li> </ol>"},{"location":"templates/design-guidelines/#34-prohibited-content","title":"3.4 Prohibited Content","text":"<p>Do NOT include in design docs (with rationale from project experience):</p> Content Why Not What to Write Instead Go struct definitions with field lists Diverged within 2 PRs in the original design doc \u2014 \"aspirational signatures\" Describe what data crosses the boundary and its semantics Method implementations Changed during micro-planning in every PR Describe the behavioral contract (GIVEN/WHEN/THEN) File paths with line numbers Stale after any refactoring Name the module and its responsibility Specific parameter/field names Renamed during implementation Describe the concept (\"load metric combining queue depth and batch size\") Interface signatures in Go syntax Froze prematurely in original design doc; actual interfaces were simpler Describe the interface's contract: single method? what it observes? what it returns? <p>Exception: Decision Records (species 1) may include brief code snippets when the decision IS about a specific implementation choice (e.g., \"use <code>math.Ceil</code> not integer division\"). Keep these minimal.</p>"},{"location":"templates/design-guidelines/#35-abstraction-levels-across-document-tiers","title":"3.5 Abstraction Levels Across Document Tiers","text":"Content Type Design Doc Macro Plan Micro Plan System invariants Define (named: INV-1, ...) Reference Refine to GIVEN/WHEN/THEN Modeling decisions (modeled/simplified/omitted) Define with justification Summarize N/A Module boundaries (behavioral) Define contract Reference + annotate per-PR Implement Interface signatures (Go code) No Frozen post-freeze PR Full code File paths No Inventory + per-PR Exact <code>file:line</code> Fidelity trade-offs Define with alternatives Reference Deviation log if changed"},{"location":"templates/design-guidelines/#4-module-architecture-principles","title":"4. Module Architecture Principles","text":""},{"location":"templates/design-guidelines/#41-two-layer-architecture","title":"4.1 Two-Layer Architecture","text":"<p>BLIS is organized as two layers:</p> <p>Layer 1: Simulation Kernel \u2014 domain-agnostic DES infrastructure - Event queue (min-heap with deterministic tie-breaking) - Clock management (next-event time advance) - Randomness (PartitionedRNG with named subsystems) - Statistics collection (accumulators decoupled from state) - Experiment control (seed, horizon, configuration)</p> <p>Layer 2: Domain Modules \u2014 inference-platform-specific model logic, each behind an interface</p> <p>The kernel provides the execution substrate. Domain modules define what is being simulated. The kernel never contains inference-specific logic; domain modules never manage the event queue or clock directly.</p>"},{"location":"templates/design-guidelines/#42-domain-module-map","title":"4.2 Domain Module Map","text":"<p>BLIS models an extensible distributed inference platform \u2014 not any single system. llm-d, vLLM, SGLang, Mooncake, and LMCache are all target systems whose behaviors should be expressible through BLIS's module composition.</p> <pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502            Cluster Orchestrator              \u2502\n                    \u2502  (shared clock, event dispatch, aggregation) \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502          \u2502          \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 Admission\u2502 \u2502 Router  \u2502 \u2502AutoScaler\u2502\n                    \u2502 Policy   \u2502 \u2502 Policy  \u2502 \u2502 Policy   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502                       \u2502                       \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Instance 0  \u2502        \u2502 Instance 1  \u2502        \u2502 Instance N  \u2502\n        \u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502        \u2502             \u2502        \u2502             \u2502\n        \u2502\u2502 Scheduler  \u2502\u2502        \u2502   . . .     \u2502        \u2502   . . .     \u2502\n        \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502        \u2502             \u2502        \u2502             \u2502\n        \u2502\u2502 Latency   \u2502\u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502\u2502 Model     \u2502\u2502\n        \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\n        \u2502\u2502 KV Cache  \u2502\u2502\n        \u2502\u2502 Manager   \u2502\u2502\n        \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\n        \u2502\u2502 Batch     \u2502\u2502\n        \u2502\u2502 Formation \u2502\u2502\n        \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Module Responsibility Interface Today Status Admission Accept/reject requests at cluster entry <code>AdmissionPolicy</code> (single method) Implemented, frozen Router Select target instance for admitted requests <code>RoutingPolicy</code> (single method) Implemented, frozen Scheduler Order queued requests within an instance <code>InstanceScheduler</code> (single method) Implemented, frozen Priority Compute request priority for scheduler <code>PriorityPolicy</code> (single method) Implemented, frozen KV Cache Manager Allocate/release/cache KV blocks <code>KVStore</code> (11 methods) Implemented AutoScaler Add/remove instances based on load signals <code>AutoScalePolicy</code> (planned) Target \u2014 PR11 Latency Model Estimate step execution time <code>LatencyModel</code> (5 methods) Implemented \u2014 <code>NewLatencyModel</code> factory Batch Formation Select requests from queue for next step <code>BatchFormation</code> (1 method: <code>FormBatch</code>) Implemented \u2014 <code>NewBatchFormation</code> factory Workload Generator Produce request streams from specs/traces <code>GenerateRequests()</code> function Implemented Trace Recorder Record decisions for analysis <code>SimulationTrace</code> Implemented Metrics Collector Aggregate per-request and system-level metrics <code>CollectRawMetrics()</code> function Implemented <p>\"Target\" means: the module exists as embedded logic today but lacks an interface boundary. Refactoring PRs will extract the interface. The guidelines define what that interface contract should look like.</p>"},{"location":"templates/design-guidelines/#43-module-contract-template","title":"4.3 Module Contract Template","text":"<p>Every module (current or target) is defined by this contract:</p> <ol> <li>Observes \u2014 what state does this module read? (Its inputs)</li> <li>Controls \u2014 what decisions does this module make? (Its outputs)</li> <li>Owns \u2014 what mutable state does this module exclusively manage?</li> <li>Invariants \u2014 what must always hold for this module?</li> <li>Events \u2014 what events does this module produce or consume? (Exogenous/endogenous classification)</li> <li>Extension friction \u2014 how many files must change to add one more variant of this module?</li> </ol> <p>Example \u2014 Router module contract:</p> Aspect Contract Observes Request metadata (tokens, prefix, SLO class, tenant), per-instance snapshots (queue depth, batch size, KV utilization, cache hit rate, pending requests), cluster clock Controls Target instance selection, priority hint for downstream scheduler Owns No mutable state (stateless policy; any affinity tracking is internal to the policy instance) Invariants Must select from non-empty snapshot list. Must return a valid instance ID. Selection must be deterministic given same inputs and RNG state. Events Consumes: <code>AdmissionDecisionEvent</code> (admitted=true). Produces: <code>RoutingDecisionEvent</code>. Extension friction 3 files to add a new routing algorithm (policy file, bundle.go registration, cmd/root.go validation message)"},{"location":"templates/design-guidelines/#44-real-system-correspondence","title":"4.4 Real-System Correspondence","text":"<p>BLIS modules map to real inference system components, but the mapping is many-to-many \u2014 BLIS must be able to express behaviors from multiple real systems:</p> BLIS Module llm-d vLLM SGLang Mooncake Router Endpoint Picker N/A (single-instance) N/A (single-instance) Global scheduler Scheduler N/A (engine-internal) <code>Scheduler</code> class <code>Scheduler</code> class Prefill/decode scheduler KV Cache Manager N/A (engine-internal) <code>BlockManager</code> <code>RadixCache</code> Distributed KV pool Latency Model N/A (real latency) N/A (real latency) N/A (real latency) N/A (real latency) AutoScaler HPA / custom N/A N/A N/A Batch Formation N/A (engine-internal) Continuous batching Chunked prefill Disaggregated batching <p>The design implication: module interfaces must be abstract enough to express all these variants, but concrete enough to capture the behavioral differences that matter for analysis. The modeling decisions table (Section 2.1) determines where on this spectrum each module sits.</p>"},{"location":"templates/design-guidelines/#45-the-touch-point-rule","title":"4.5 The Touch-Point Rule","text":"<p>When a design doc introduces a new module boundary, it must specify the expected touch-point count for adding one more variant. The following are reference targets based on what works well in the current codebase:</p> Extension Type Reference Target Current Reality New policy template ~3 files 3 files (meets target) New KV cache tier ~4 files 4-5 files (acceptable) New config parameter ~2 files 2 files (meets target \u2014 post-#381) New observable metric ~3 files 6 files (exceeds \u2014 known friction) New latency model backend ~2 files 2 files (meets target) New batch formation strategy ~2 files 2 files (meets target) <p>If a design exceeds the reference target, the design doc must acknowledge the friction and explain whether it's acceptable (justified complexity) or whether structural improvement should happen first or concurrently. The goal is awareness, not rigidity \u2014 some modules genuinely require more touch points, but that should be a conscious choice, not an accident.</p>"},{"location":"templates/design-guidelines/#46-parallel-development-enablement","title":"4.6 Parallel Development Enablement","text":"<p>Module boundaries enable parallel development when:</p> <ol> <li>Interfaces are stable \u2014 frozen after a designated PR (as done with policy interfaces after PR8)</li> <li>Contracts are testable independently \u2014 each module can be tested with mock implementations of adjacent modules</li> <li>No shared mutable state \u2014 modules communicate through defined inputs/outputs, not through shared globals or reaching through struct fields</li> <li>Bridge types at boundaries \u2014 when two modules in different packages need shared types, bridge types live in the lower-level package (e.g., <code>RouterState</code> in <code>sim/</code> not <code>sim/cluster/</code>)</li> </ol> <p>A design doc for a new module must demonstrate that two developers could work on different implementations of that module's interface simultaneously, with only behavioral contract tests to keep them aligned.</p>"},{"location":"templates/design-guidelines/#5-extension-framework","title":"5. Extension Framework","text":""},{"location":"templates/design-guidelines/#51-extension-taxonomy","title":"5.1 Extension Taxonomy","text":"<p>There are four fundamentally different ways to extend BLIS:</p> Type What It Is Example Scope Policy Template New algorithm behind an existing interface New routing algorithm (e.g., power-of-two-choices) Single file + registration Subsystem Module New module with its own interface, events, and state AutoScaler, P/D disaggregation New interface + integration Backend Swap Alternative implementation of an internal module SGLang latency model alongside vLLM, RadixCache alongside block-based KV New implementation behind existing interface Tier Composition Layering an existing module with additional behavior NVMe KV tier wrapping GPU+CPU tiers, network latency wrapping base latency Decorator/delegation over existing module <p>Understanding which type an extension is determines which recipe to follow.</p>"},{"location":"templates/design-guidelines/#52-recipe-policy-template","title":"5.2 Recipe: Policy Template","text":"<p>Adding a new algorithm behind a frozen interface.</p> <p>This is the lightest extension type. The interface already exists, the factory already exists, the CLI integration pattern is established.</p> <p>Prerequisites: The target policy interface must exist and be stable.</p> <p>Contract the new template must satisfy: - Implements the interface (single method) - Deterministic given same inputs and RNG state - No side effects beyond its owned state (if stateful) - Handles edge cases defined by the interface (e.g., empty snapshot list for routing policies)</p> <p>Steps: Documented in CLAUDE.md \"Adding New Policy Templates\" \u2014 these guidelines don't duplicate, they reference.</p> <p>Parallel development: Multiple policy templates for the same interface can be developed simultaneously by different contributors with zero coordination, since they share only the interface contract.</p>"},{"location":"templates/design-guidelines/#53-recipe-subsystem-module","title":"5.3 Recipe: Subsystem Module","text":"<p>Adding an entirely new module with its own behavioral contract.</p> <p>This is the most architecturally significant extension type. Examples: AutoScaler (PR11), framework adapters (PR15).</p> <p>Design doc must define:</p> <ol> <li>Module contract (per Section 4.3 template \u2014 observes, controls, owns, invariants, events, extension friction)</li> <li>Interface design \u2014 prefer single-method interfaces where possible. If multiple methods are needed, justify each one. The interface must be testable with a mock.</li> <li>Event integration \u2014 what new event types are added to the cluster event queue? What are their priority constants? How do they interact with existing events?</li> <li>State ownership \u2014 what new mutable state does this module introduce? Who creates it, who reads it, who mutates it? No shared mutable state across module boundaries.</li> <li>Failure modes \u2014 what happens when the module fails? (Error return? Panic? Graceful degradation?) Which error handling boundary applies? (See CLAUDE.md Engineering Principles)</li> <li>Default behavior \u2014 what does BLIS do when this module is not configured? (A no-op default must exist so existing workflows are unaffected)</li> <li>Configuration surface \u2014 what CLI flags / YAML config does this add? Validated how?</li> </ol> <p>Design doc must demonstrate: - The module can be tested in isolation with mocked dependencies - Adding the module doesn't change behavior of existing tests (no-op default) - The extension friction for adding a second implementation is within reference targets from Section 4.5</p> <p>Parallel development: Once the interface is agreed and frozen, the module implementation and its integration into the cluster orchestrator can proceed independently.</p>"},{"location":"templates/design-guidelines/#54-recipe-backend-swap","title":"5.4 Recipe: Backend Swap","text":"<p>Alternative implementation of an internal module that currently has no interface.</p> <p>This is the extension type that requires refactoring first. Examples: SGLang latency model, continuous-vs-chunked batching.</p> <p>Two-phase approach:</p> <p>Phase A \u2014 Extract interface (refactoring PR): - Identify the hardcoded logic (e.g., <code>Step()</code> calling the blackbox latency estimator directly) - Define an interface that captures the behavioral contract of the existing implementation - Extract the existing implementation behind the new interface - Verify: all existing tests pass, no behavior change, the factory returns the existing implementation by default</p> <p>Phase B \u2014 Add alternative (extension PR): - Implement the new backend behind the extracted interface - Add configuration to select between backends (CLI flag or YAML) - Add behavioral tests for the new backend - Verify: existing tests still pass when default backend is selected</p> <p>Design doc must cover both phases. Phase A is often the harder design challenge \u2014 getting the interface abstraction right so that it accommodates both the existing and new backends without over-generalizing.</p> <p>Key principle: The interface should capture what the module does (behavioral contract), not how one particular backend does it. If the interface has methods that only make sense for one backend, it's too specific.</p> <p>Parallel development: After Phase A merges, multiple backends can be developed simultaneously.</p>"},{"location":"templates/design-guidelines/#55-recipe-tier-composition","title":"5.5 Recipe: Tier Composition","text":"<p>Layering additional behavior onto an existing module via delegation.</p> <p>This is the pattern used by <code>TieredKVCache</code> (wraps <code>KVCacheState</code>) and could be used for network latency (wraps base latency model), caching layers, or monitoring wrappers.</p> <p>Design doc must define: - Which existing interface is being composed - What new behavior the wrapper adds (offloading, caching, monitoring, latency injection) - How metrics aggregate across tiers (the wrapper must expose the same metrics interface as the inner module, combining results appropriately) - How configuration selects the composition (e.g., <code>--kv-cpu-blocks &gt; 0</code> triggers tiered wrapping)</p> <p>Key principle: The wrapper must satisfy the same interface contract as the inner module. Any caller that works with the inner module must work identically with the wrapper (Liskov substitution).</p> <p>Parallel development: Wrappers are naturally parallelizable \u2014 different tiers or decorators can be developed independently as long as they compose through the same interface.</p>"},{"location":"templates/design-guidelines/#56-extension-checklist","title":"5.6 Extension Checklist","text":"<p>Before submitting a design doc for any extension, verify:</p> <ul> <li> Extension type identified (policy template / subsystem module / backend swap / tier composition)</li> <li> Correct recipe followed</li> <li> Module contract defined (observes / controls / owns / invariants / events / friction)</li> <li> No-op default exists (existing behavior unchanged when extension not configured)</li> <li> Interface testable with mocks (no concrete dependencies leaked through interface)</li> <li> Parallel development path described (what can proceed independently after interface freeze?)</li> <li> Touch-point count specified for adding one more variant</li> <li> DES checklist from Section 2.6 completed</li> <li> New randomness declared (PartitionedRNG subsystem name)</li> <li> Event priority constants assigned (if new events introduced)</li> </ul>"},{"location":"templates/design-guidelines/#6-anti-patterns-with-evidence","title":"6. Anti-Patterns with Evidence","text":"<p>Every anti-pattern in this section traces to a real bug, a real friction point, or a real design doc failure from BLIS's development history.</p>"},{"location":"templates/design-guidelines/#61-design-doc-anti-patterns","title":"6.1 Design Doc Anti-Patterns","text":"Anti-Pattern What Happened Lesson The Type Catalog The original design doc contained ~600 lines of Go struct definitions. Within 2 PRs, <code>RouterState</code> went from a 5-section struct to <code>Snapshots + Clock</code>. <code>InstanceScheduler</code> went from 3 methods to 1. The macro plan called these \"aspirational signatures that diverged during implementation.\" Describe module boundaries behaviorally, not as Go types. Types change; contracts persist. Fidelity for Its Own Sake The original design doc specified <code>ShadowKVModel</code> with <code>PrefixHashes</code>, <code>EstimatedUtilization</code>, <code>EvictionQueue</code> \u2014 none of which were needed for the analysis questions BLIS answers today. Apply Banks et al.'s six model scoping criteria. If you can't name the analysis question a component answers, defer it. Silent Staleness The design doc's <code>RoutingDecision</code> struct had 8 fields. The implemented version has 4. No mechanism flagged the divergence. Meanwhile, micro-plan deviation logs caught discrepancies with the macro plan because there was an explicit comparison step. Higher-level docs need an explicit freshness mechanism. Design docs should version their decisions and mark which are implemented (e.g., a Decision Status column: Proposed / Implemented / Superseded). Missing Trade-off Rationale Several design doc decisions had no alternatives listed. When implementation revealed a better approach, there was no record of why the original choice was made. Every non-obvious decision must list alternatives considered and why they were rejected. This is what makes the PR12 pre-design valuable \u2014 each of its 9 decisions has explicit rationale."},{"location":"templates/design-guidelines/#62-module-architecture-anti-patterns","title":"6.2 Module Architecture Anti-Patterns","text":"Anti-Pattern What Happened Lesson Shotgun Surgery Adding <code>InstanceID</code> to per-request metrics (#181) required changes in 4 files. Three construction sites for <code>RequestMetrics</code> existed, and one was missed initially. Use canonical constructors. Design docs for new types must specify whether a canonical constructor is needed. Destructive Read <code>KVStore.PendingTransferLatency()</code> both queried and cleared the accumulated latency. Callers couldn't distinguish \"no latency\" from \"already consumed.\" Identified as blocking LMCache integration. Query methods must be pure. If a method needs to both query and clear state, provide separate <code>Get()</code> and <code>Consume()</code> methods. Interface Leaking Implementation <code>KVStore</code> interface has 11 methods, several exposing block-level semantics. A distributed KV cache like LMCache doesn't think in blocks \u2014 it thinks in tokens and layers. The interface encodes vLLM's implementation model, not the abstract behavioral contract. (#246) Design interfaces around the behavioral contract (allocate space, check cache, release space), not around one implementation's data model. Monolith Method <code>Simulator.Step()</code> was 152 lines mixing 4 concerns. Decomposed into named phase methods (<code>scheduleBatch</code>, <code>executeBatchStep</code>, <code>processCompletions</code>, <code>scheduleNextStep</code>). Each module's logic should be callable through its interface. When a method contains logic for multiple modules, extract each into its module's interface method. Config Mixing Concerns <code>SimConfig</code> combined 23 fields from 8 concerns. Decomposed into 6 embedded sub-configs with canonical constructors (<code>NewKVCacheConfig</code>, etc.). Adding a field now touches 2 files; the compiler catches all call sites. Group configuration by module. Each module's config should be independently specifiable and validatable."},{"location":"templates/design-guidelines/#63-des-specific-anti-patterns","title":"6.3 DES-Specific Anti-Patterns","text":"Anti-Pattern What Happened Lesson Golden Tests Without Invariant Tests The golden dataset test for codellama expected 499 completions. The code silently dropped one request on KV allocation failure (#183). The golden test encoded the bug as the expected value for months. Golden tests answer \"did the output change?\" Invariant tests answer \"is the output correct?\" Both are needed (Banks et al.: verification \u2260 validation). Non-Deterministic Map Iteration Five sites iterated Go maps to accumulate floats or determine output ordering. Go map iteration is randomized, violating the determinism invariant. All randomness must flow through PartitionedRNG. Map iteration is a hidden source of non-determinism. Sort keys before any iteration that affects output. Mixing Exogenous and Endogenous The cluster workload generator (exogenous) was tightly coupled to the cluster simulator (endogenous). Impossible to replay the same workload through different configurations without re-generating. PR10 decoupled them. Exogenous inputs must be separable from endogenous logic. This enables the fundamental simulation experiment: same input, different configuration, compare results."},{"location":"templates/design-guidelines/#64-the-meta-lesson","title":"6.4 The Meta-Lesson","text":"<p>All these anti-patterns share one root cause: the design was expressed in terms of implementation rather than behavior. When a design doc specifies Go structs instead of behavioral contracts, the implementation becomes the specification. When a module boundary is defined by its internal data model instead of its observable behavior, the boundary can't accommodate a second implementation.</p> <p>Describe what a module does and what it guarantees, not how it's built.</p> <p>If a design doc follows this principle, its content stays durable, its modules stay extensible, its interfaces accommodate multiple backends, and parallel development is naturally enabled \u2014 because contributors agree on behavior, not on implementation.</p>"},{"location":"templates/design-guidelines/#references","title":"References","text":"<ol> <li>Banks, J., Carson, J. S., Nelson, B. L., &amp; Nicol, D. M. Discrete-Event System Simulation (5th ed.). Pearson. \u2014 Foundational text on DES methodology, model scoping, input modeling, output analysis, V&amp;V.</li> <li>Misra, J. \"Distributed Discrete-Event Simulation.\" Computing Surveys, Vol. 18, No. 1, March 1986. \u2014 Formal correctness proofs for DES, causal ordering, simulation as formal property.</li> <li>BLIS Issue #183 \u2014 Golden dataset encoded a silently-dropped request. Conservation invariant test would have caught it on day one.</li> <li>BLIS PR12 Pre-Design (<code>docs/plans/archive/pr12-architectural-predesign.md</code>) \u2014 Gold standard for decision records: 9 decisions, each with trade-off analysis.</li> <li>BLIS Hardening Design (<code>docs/plans/archive/2026-02-18-hardening-antipattern-refactoring-design.md</code>) \u2014 Extension scenario analysis identifying friction points for autoscaling, LMCache, heterogeneous HW, new engines.</li> </ol>"},{"location":"templates/hypothesis/","title":"Hypothesis Experiment Template","text":"<p>For Claude: Use this template when creating a new hypothesis experiment in <code>hypotheses/&lt;name&gt;/</code>.</p>"},{"location":"templates/hypothesis/#findingsmd-structure","title":"FINDINGS.md Structure","text":"<p>Every experiment's <code>FINDINGS.md</code> MUST contain these sections:</p> <pre><code># &lt;Hypothesis Name&gt;\n\n**Status:** Confirmed | Confirmed with nuance | Partially confirmed | Refuted | Inconclusive\n**Resolution:** &lt;one of: Clean confirmation | Confirmation with wrong mechanism | Confirmation with bug discovery | Partial confirmation with surprise | Refuted \u2014 mechanism not plausible | Refuted \u2014 system design flaw | Refuted \u2014 wrong mental model | Inconclusive \u2014 parameter-dependent | Converged to open question&gt;\n**Family:** &lt;one of: Workload/arrival | Scheduler invariants | Performance-regime | Structural model | Robustness/failure-mode | Cross-policy comparative&gt;\n**VV&amp;UQ:** &lt;one of: Verification | Validation | UQ&gt;\n**Tier:** &lt;tier number \u2014 see hypotheses/README.md for definitions&gt;\n**Type:** Deterministic | Statistical (&lt;subtype&gt;)\n**Date:** YYYY-MM-DD\n**Rounds:** &lt;number of experiment-review rounds to convergence&gt;\n\n## Hypothesis\n\n&gt; &lt;Quoted hypothesis statement \u2014 intuitive claim about system behavior&gt;\n\n## Experiment Design\n\n**Classification:** &lt;Deterministic | Statistical/Dominance | Statistical/Monotonicity | Statistical/Equivalence | Statistical/Pareto&gt;\n\n**Configurations compared:**\n- A: &lt;description + exact CLI flags&gt;\n- B: &lt;description + exact CLI flags&gt;\n\n**Controlled variables:** &lt;what is held constant&gt;\n**Varied variable:** &lt;what differs between A and B&gt;\n**Seeds:** &lt;list of seeds used&gt;\n**Preconditions verified:** &lt;what was checked before running&gt;\n\n## Results\n\n&lt;Comparison tables with per-seed values&gt;\n\n## Root Cause Analysis\n\n&lt;Why the results are what they are \u2014 trace through the code/architecture.\nEvery causal claim MUST cite file:line (RCV-1).\nEvery \"surprise\" MUST include a first-principles calculation (RCV-2).\nMust explain the mechanism AND its direction (RCV-3).\nIf a mechanism is proposed, describe the control experiment that would confirm it (RCV-4).&gt;\n\n## Devil's Advocate (RCV-5)\n\n&lt;Before sending to review, argue the OPPOSITE of your conclusion.&gt;\n\n**If this is \"Confirmed,\" argue why it might be Refuted:**\n&lt;2-3 sentences&gt;\n\n**If this is \"Refuted,\" argue why it might be Confirmed:**\n&lt;2-3 sentences&gt;\n\n## Findings Classification\n\n| Finding | Type | Action |\n|---------|------|--------|\n| &lt;finding 1&gt; | Confirmation / Bug / New rule / New invariant / Design limitation / Surprise / Open question | &lt;issue number or \"documented here\"&gt; |\n\n## Standards Audit\n\nFindings checked against docs/standards/:\n- [ ] Any violations of existing rules? &lt;list or \"none found\"&gt;\n- [ ] Any new rules needed? &lt;list or \"none\"&gt;\n- [ ] Any new invariants needed? &lt;list or \"none\"&gt;\n- [ ] Any existing rules/invariants confirmed? &lt;list or \"none\"&gt;\n\n## Scope and Limitations (RCV-6)\n\n- **Operating point tested:** &lt;blocks, rate, seeds, instances, routing, etc.&gt;\n- **Parameters findings depend on:** &lt;what must be true for these results to hold&gt;\n- **What was NOT tested:** &lt;parameter ranges, workloads, configs not covered&gt;\n- **Generalizability:** &lt;does this finding generalize, or is it specific to this config?&gt;\n- **Uncertainty quantification:** &lt;for any threshold or boundary finding, report confidence intervals. For any \"confirmed\" result, estimate the probability of holding under parameter variation. If UQ was not performed, state \"UQ not performed \u2014 single operating point.\"&gt;\n\n## Evidence Quality\n\n| Metric | Value | Confidence |\n|--------|-------|------------|\n| &lt;primary metric&gt; | &lt;value&gt; | High / Medium / Low \u2014 &lt;why&gt; |\n| Sample size | &lt;seeds \u00d7 configs \u00d7 requests&gt; | &lt;assessment&gt; |\n| Mechanism | &lt;proposed mechanism&gt; | &lt;confidence + whether control confirms&gt; |\n\n## Implications for Users\n\n&lt;Practical guidance derived from this experiment&gt;\n\n## Reproducing\n\ncd hypotheses/&lt;name&gt;\n./run.sh\n</code></pre>"},{"location":"templates/hypothesis/#runsh-structure","title":"run.sh Structure","text":"<pre><code>#!/bin/bash\n# &lt;Hypothesis name&gt;\n# &lt;One-line description&gt;\n# Usage: ./run.sh [--rebuild]\n\nset -euo pipefail\nSCRIPT_DIR=\"$(cd \"$(dirname \"$0\")\" &amp;&amp; pwd)\"\nsource \"$SCRIPT_DIR/../lib/harness.sh\"\n\nsetup_experiment \"${1:-}\"\n\n# -- Experiment sections -----------------------------------------------\n# Each experiment: use blis_run with appropriate timeout tier.\n# NOTE: blis_run (not run_sim) \u2014 define your own run_sim() wrapper if needed.\n#\n# Example (basic):\n#   blis_run $TIMEOUT_STANDARD \"$RESULTS_DIR/config_a.txt\" \\\n#       --model \"$MODEL\" --num-instances 4 --seed 42 \\\n#       --workload-spec \"$WORKLOAD_YAML\" --log error\n#\n# Example (with stderr capture for robustness experiments):\n#   blis_run $TIMEOUT_STANDARD \"$RESULTS_DIR/config_a.txt\" \\\n#       --stderr \"$RESULTS_DIR/config_a_stderr.txt\" \\\n#       --model \"$MODEL\" --num-instances 4 --seed 42 --log error\n#\n# Example (with per-request JSON):\n#   blis_run $TIMEOUT_STANDARD \"$RESULTS_DIR/config_a.txt\" \\\n#       --model \"$MODEL\" --num-instances 4 --seed 42 --log error \\\n#       --results-path \"$RESULTS_DIR/config_a_results.json\"\n#\n# Example (robustness/stress \u2014 non-zero exit expected, use || true under set -e):\n#   blis_run $TIMEOUT_EXTENDED \"$RESULTS_DIR/stress.txt\" \\\n#       --stderr \"$RESULTS_DIR/stress_stderr.txt\" \\\n#       --model \"$MODEL\" --num-instances 4 --seed 42 --log error || true\n#\n# For KV-constrained experiments, add pre-flight check (advisory, never aborts):\n#   preflight_kv_check 800 16 512  # total_blocks, block_size (default: 16), max_input\n# ----------------------------------------------------------------------\n</code></pre>"},{"location":"templates/hypothesis/#analyzepy-structure","title":"analyze.py Structure","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Analysis script for &lt;hypothesis name&gt;.\n\nParses BLIS multi-block output and produces comparison tables.\n\"\"\"\nimport sys\nfrom pathlib import Path\n\n# Import shared helpers\nsys.path.insert(0, str(Path(__file__).resolve().parent.parent / \"lib\"))\nfrom analyze_helpers import parse_blis_output, check_for_timeout\n\n# -- Analysis code --------------------------------------------------------\n# Use parse_blis_output(filepath) to get metrics dict.\n# The dict includes a 'timed_out' flag \u2014 check it before computing ratios.\n#\n# Example:\n#   metrics = parse_blis_output(sys.argv[1])\n#   if metrics[\"timed_out\"]:\n#       print(f\"  SKIPPED (timeout)\", file=sys.stderr)\n#   else:\n#       print(f\"  TTFT mean: {metrics['ttft_mean']:.2f} ms\")\n# -------------------------------------------------------------------------\n</code></pre>"},{"location":"templates/macro-plan/","title":"Macro Plan","text":"<p>You are operating inside a real repository with full code access.</p> <p>You are tasked with producing a MACRO-LEVEL DESIGN PLAN for a:</p> <p>\"Major Feature Expansion with Architectural Changes.\"</p> <p>This is a program-level plan that defines objectives, a concept model, architectural evolution, and an ordered PR series.</p> <p>This is NOT implementation planning. This is NOT a micro-level design. This is NOT speculative architecture.</p> <p>You MUST inspect the real codebase before proposing changes.</p> <p>====================================================================== PREREQUISITE \u2014 DESIGN GUIDELINES ======================================================================</p> <p>Before writing a macro plan, read and internalize:</p> <ul> <li><code>docs/templates/design-guidelines.md</code> \u2014 BLIS design guidelines   covering DES foundations, module architecture, and extension framework.</li> </ul> <p>The macro plan must be consistent with these guidelines. Specifically: - Building blocks must use the MODULE CONTRACT TEMPLATE from Section 4.3   (observes / controls / owns / invariants / events / extension friction) - New events must be classified as EXOGENOUS or ENDOGENOUS (Section 2.2) - Modeling decisions must apply the SIX SCOPING CRITERIA (Section 2.1) - PRs must identify their EXTENSION TYPE (Section 5.1): policy template,   subsystem module, backend swap, or tier composition - Building blocks should map to REAL SYSTEM COMPONENTS (Section 4.4)</p> <p>If no design doc exists for the feature being planned, one must be created (per the guidelines) before or alongside the macro plan.</p> <p>====================================================================== ABSTRACTION LEVEL RULE (NON-NEGOTIABLE) ======================================================================</p> <p>The macro plan describes WHAT to build and in WHAT ORDER, not HOW to implement each piece. Enforce these boundaries strictly:</p> <p>ALLOWED in macro plan:   - Behavioral descriptions of module contracts (prose)   - Frozen interface signatures (Go code ONLY for interfaces whose     freeze PR has already merged \u2014 these are facts, not aspirations)   - File path inventories (which packages, which files, per PR)   - LOC estimates per PR (sizing heuristics)   - CLI flag names and configuration surface area   - Brief YAML/config examples   - Architecture diagrams (text-based)</p> <p>PROHIBITED in macro plan:   - Method implementations (belongs in micro plan)   - Struct field lists (belongs in micro plan)   - Pre-freeze interface signatures in Go syntax (describe behaviorally:     \"a single-method interface that selects a target instance given     request metadata and per-instance snapshots\")   - Factory function code (belongs in micro plan)   - Test code (belongs in micro plan)</p> <p>THE TEST: Is this content a FACT about merged code, or an ASPIRATION about code to be written? Facts are allowed. Aspirations must be described behaviorally, not as Go code.</p> <p>WHY THIS MATTERS: The original macro plan had ~200 lines of Go code including full method implementations (TokenBucket.Admit(), PartitionedRNG.ForSubsystem()). These diverged during micro-planning and became misleading. Behavioral descriptions survive intact because they describe WHAT, not HOW.</p> <p>====================================================================== PHASE 0 \u2014 REPOSITORY RECON (MANDATORY) ======================================================================</p> <p>Before proposing anything:</p> <p>1) Identify and summarize:    - Top-level packages/modules and responsibilities    - Core data structures and interfaces    - Key invariants and assumptions encoded in the system    - CLI entrypoints and current flag surface    - Configuration flow    - Existing extension points (map to design guidelines Section 4.2)    - Areas of tight coupling or fragility    - Current module boundaries vs. target module map (guidelines 4.2)</p> <p>2) Clearly separate:    - Confirmed facts (from inspection \u2014 cite file:line for every claim)    - Inferred behavior (explicitly labeled as inference)    - Open uncertainties</p> <p>3) Identify architectural constraints that must not be violated.</p> <p>No invented abstractions. No imagined extension points. Everything must be grounded in code inspection with source references.</p> <p>ANTI-HALLUCINATION RULE: For every behavioral claim about existing code, provide a file:line citation. If you cannot cite it, mark it as \"UNVERIFIED\" and do not rely on it in subsequent phases.</p> <p>====================================================================== PHASE 1 \u2014 HIGH-LEVEL OBJECTIVES AND MODEL SCOPING ======================================================================</p> <p>Define:</p> <ul> <li>3-7 crisp objectives</li> <li>Explicit non-goals</li> <li>Compatibility constraints</li> <li>Performance constraints</li> <li>Backward compatibility guarantees</li> <li>Operational/CLI stability expectations</li> </ul> <p>Be precise.</p> <p>MODEL SCOPING (required \u2014 applies Banks et al. criteria):</p> <p>For this feature expansion, answer:</p> <p>1) What ANALYSIS QUESTIONS does this feature help answer?    (e.g., \"What is the optimal routing policy for heterogeneous    hardware?\" or \"How does autoscaling latency affect tail TTFT?\")</p> <p>2) What must be MODELED to answer those questions?</p> <p>3) What can be SIMPLIFIED without affecting the analysis?    (e.g., \"model scaling latency as fixed delay, not warmup curve\")</p> <p>4) What can be OMITTED entirely?    (e.g., \"network partitions between instances \u2014 out of scope\")</p> <p>Present as a table:</p> Component Modeled Simplified Omitted Justification (example) Scaling latency -- Fixed delay, not warmup curve -- Same steady-state throughput; warmup matters only for sub-minute scale-up (example) Network partitions -- -- Yes Not needed for routing policy comparison; add if modeling failure recovery <p>For each \"Simplified\" entry, state what real-system behavior is lost and under what conditions it would matter in the Justification column. This is the fidelity trade-off record \u2014 it prevents \"fidelity for its own sake\" and enables future refinement with clear upgrade paths.</p> <p>====================================================================== PHASE 2 \u2014 CONCEPT MODEL ======================================================================</p> <p>Before diving into architecture, define the system at the level a human would explain it on a whiteboard:</p> <p>1) Building Blocks (3-7 named components)</p> <p>For EACH building block, provide the MODULE CONTRACT:    - Name and one-sentence responsibility    - OBSERVES: what state does this module read? (its inputs)    - CONTROLS: what decisions does this module make? (its outputs)    - OWNS: what mutable state does it exclusively manage?    - INVARIANTS: what must always hold for this module?    - EVENTS: what events does it produce or consume?      Classify each as EXOGENOUS (driven by external input) or      ENDOGENOUS (driven by internal state transitions).    - EXTENSION FRICTION: how many files must change to add one more      variant? (Reference targets from design guidelines Section 4.5)</p> <p>No building block may have more than one core responsibility.</p> <p>2) Interaction Model    - Who calls whom (directional arrows)    - Data flow between blocks (what crosses each boundary)    - Ownership transfer rules (when does data change owners?)</p> <p>3) System Invariants    - What must ALWAYS hold (e.g., \"clock never decreases\")    - What must NEVER happen (e.g., \"no cross-instance state mutation\")    - Causality constraints (ordering guarantees)    - DES-specific: state vs. statistics separation \u2014 which data is      simulation state (evolves the system) vs. derived statistics      (output for analysis)?</p> <p>4) Extension Points    - Where do new behaviors plug in? (behavioral description of      interface contract + responsibility)    - What is the default behavior for each extension point?    - What is the FIRST non-default implementation planned?</p> <p>5) State Ownership Map    - For every piece of mutable state: exactly one owner    - Shared state must be explicitly identified and justified</p> <p>6) Real-System Correspondence    - Map each building block to the real inference system component(s)      it models. Use a table:</p> Building Block llm-d vLLM SGLang Other <ul> <li>BLIS models an extensible distributed inference platform, not any      single system. The table ensures the architecture stays grounded      in real systems while remaining general enough to express      behaviors from multiple targets.</li> </ul> <p>THE CONCEPT MODEL MUST FIT IN UNDER 80 LINES. (Increased from 60 to accommodate module contracts and real-system correspondence. If it exceeds 80, the design is too complex \u2014 simplify before proceeding.)</p> <p>Every PR in Phase 6 must map to adding or modifying a specific building block from this model. If a PR cannot be described as a building block change, redesign the PR or the model.</p> <p>====================================================================== PHASE 3 \u2014 ARCHITECTURAL RISK REGISTER ======================================================================</p> <p>For every non-obvious architectural decision in the concept model:</p> Decision Assumption Validation Method Cost if Wrong Gate <ul> <li>DECISION: The choice being made</li> <li>ASSUMPTION: What must be true for this to work</li> <li>VALIDATION: How to test cheaply (mock study, prototype, analysis, spike)</li> <li>COST IF WRONG: What breaks \u2014 count the affected PRs</li> <li>GATE: When validation must complete (before which PR)</li> </ul> <p>Example row: | Shared-clock event loop | O(N) scan per event is fast for N&lt;=16 |   Benchmark N=16, 10K events | PR 3 rework | Before PR 3 merge |</p> <p>MANDATORY VALIDATION RULE: If cost-of-being-wrong &gt;= 3 PRs of rework, validation is MANDATORY. The plan must include a spike/mock study PR or pre-PR validation step.</p> <p>For each validation gate, specify: - Exact success criteria (not \"looks good\" \u2014 measurable outcomes) - Abort plan (what changes if validation fails)</p> <p>====================================================================== PHASE 4 \u2014 PROPOSED ARCHITECTURAL EVOLUTION ======================================================================</p> <p>Only after the concept model and risk register:</p> <ul> <li>Describe how the architecture evolves FROM current TO concept model.</li> <li>Map each structural change to a concept model building block.</li> <li>Identify refactors that are strictly enabling (no behavior change).</li> <li>Explicitly describe what remains unchanged.</li> <li>For each new extension point: what is the default implementation and   when does the first non-default implementation arrive?</li> </ul> <p>Highlight risks and invariants.</p> <p>No premature generalization. No extension point without a concrete non-default implementation planned.</p> <p>ABSTRACTION LEVEL CHECK: This section describes the evolution BEHAVIORALLY. Do NOT include Go code here. Describe what each module will do and what contract it will satisfy, not how it will be implemented. Interface signatures appear only in the output sections (Section G, defined in the Output Format section below) and only for already-frozen interfaces.</p> <p>FIDELITY TRADE-OFFS: For each architectural simplification, state: - What real-system behavior is being approximated - What analysis questions the approximation still answers correctly - Under what conditions the approximation breaks down - What the upgrade path looks like (which future PR refines this)</p> <p>====================================================================== PHASE 5 \u2014 CROSS-CUTTING INFRASTRUCTURE ======================================================================</p> <p>Plan ONCE for the entire PR series. Each item must be assigned to a specific PR (defined in Phase 6) or handled as a standalone preparatory PR. Phases 5 and 6 are co-developed: sketch the PR series first, then assign cross-cutting items, then finalize both.</p> <p>1) Shared Test Infrastructure    - First: identify existing shared test packages in the codebase.      Build on them rather than duplicating or replacing them.    - New test helper packages, shared fixtures, golden dataset types    - Which PR creates them? Which PRs consume them?    - How do golden datasets evolve as the system grows?    - INVARIANT TESTS: which system invariants must have companion tests?      (Golden tests alone are insufficient \u2014 see design guidelines 6.3)</p> <p>2) Documentation Maintenance    - CLAUDE.md update triggers: new packages, new files, changed file      organization, completed plan milestones, new CLI flags    - Who updates CLAUDE.md? (The PR that causes the change.)    - README update triggers and ownership    - Design guidelines compliance: does this feature expansion require      updating the target module map in the design guidelines?</p> <p>3) CI Pipeline Changes    - New test packages to add to CI    - New linter rules or build steps    - Performance regression benchmarks</p> <p>4) Dependency Management    - New external dependencies (justify each one)    - Version pinning strategy</p> <p>5) Interface Freeze Schedule    - Which PR freezes which interface?    - What must be validated before freezing?    - After freeze: parallel development of templates/implementations      can proceed independently</p> <p>No item may be left as \"address when needed.\" This applies to cross-cutting infrastructure (test helpers, CI, docs), not to feature packages which are detailed in Phase 6.</p> <p>====================================================================== PHASE 6 \u2014 ORDERED PR SERIES (PR0 ... PRN) ======================================================================</p> <p>Design an incremental, independently reviewable and mergeable PR sequence.</p> <p>For EACH PR, provide TWO TIERS:</p> <p>--- TIER 1: Human Review Summary (target 15 lines, max 25) ---</p> <ul> <li>Title</li> <li>Building Block Change: Which concept model block is added/modified?</li> <li>Extension Type (from design guidelines Section 5.1):<ul> <li>policy template: new algorithm behind existing interface</li> <li>subsystem module: new interface + new events</li> <li>backend swap: alternative implementation, requires interface extraction</li> <li>tier composition: decorator/wrapper over existing module</li> </ul> </li> <li>Motivation: Why does this PR exist? (1-2 sentences)</li> <li>Scope: In / Out (bullet points)</li> <li>Behavioral Guarantees: What MUST hold after this PR merges?   (Use named contracts: BC-1, BC-2, etc.)</li> <li>Risks: Top 1-2 risks and how they're mitigated</li> <li>Cross-Cutting: Which shared infra does this PR create or consume?</li> <li>Validation Gate: Does this PR depend on a risk register validation?</li> </ul> <p>--- TIER 2: Implementation Guide (for micro-planning) ---</p> <ul> <li>Architectural Impact (what changes structurally)</li> <li>API Surface Changes (new types, interfaces, methods \u2014 described   BEHAVIORALLY, not as Go code. E.g., \"New single-method interface   for latency estimation, replacing hardcoded Step() logic\")</li> <li>CLI Changes (new flags, changed behavior)</li> <li>Test Categories (unit, integration, regression, golden, invariant)</li> <li>Documentation Updates (CLAUDE.md, README, design guidelines if needed)</li> <li>Extension Friction: how many files to add one more variant of the   new type/interface? Compare against reference targets (guidelines 4.5)</li> <li>Parallel Development: after this PR merges, what can proceed   independently? (e.g., \"multiple routing policies can be developed   in parallel after the interface freeze in this PR\")</li> <li>Why this PR is independently reviewable</li> <li>Why it introduces no dead code</li> </ul> <p>Constraints:</p> <ul> <li>Each PR must deliver one cohesive building block change.</li> <li>Each PR must be exercisable immediately after merge.   \"Exercisable\" means: via CLI, OR via tests that demonstrate the   new behavior. Internal refactors exercised by passing existing tests   are valid. Scaffolding exercised only by future PRs is NOT valid.</li> <li>No speculative scaffolding.</li> <li>No unused interfaces.</li> <li>No flags that aren't exercised.</li> <li>Each PR must identify its extension type and follow the corresponding   recipe from the design guidelines (Section 5.2-5.5).</li> </ul> <p>====================================================================== PHASE 7 \u2014 DEPENDENCY DAG &amp; PARALLELISM ======================================================================</p> <p>Provide:</p> <ul> <li>A PR dependency graph (partial order).</li> <li>Parallelizable workstreams.</li> <li>Merge sequencing guidance.</li> <li>Validation gate placement (from risk register).</li> <li>Interface freeze points (from Phase 5, item 5: Interface Freeze   Schedule) \u2014 mark which PRs unlock parallel development of multiple   implementations.</li> <li>Integration risk notes.</li> </ul> <p>Maximize safe parallelism.</p> <p>====================================================================== PHASE 8 \u2014 DESIGN BUG PREVENTION ======================================================================</p> <p>Include:</p> <ul> <li>Invariants that must never be broken (reference concept model).</li> <li>Regression surfaces (which existing tests must keep passing).</li> <li>Cross-PR state migration risks (data format changes across PRs).</li> <li>Backward compatibility enforcement.</li> </ul> <p>Common architectural failure modes and how this plan prevents them:</p> <p>General:   - Scaffolding creep (dead code introduced \"for later\").     Prevention: every struct field, method, and flag must be exercised     by the end of the PR that introduces it.   - Documentation drift (CLAUDE.md diverges from reality).     Prevention: the PR that causes the change updates CLAUDE.md in the     same commit. No deferred documentation.   - Test infrastructure duplication (helpers copied across packages).     Prevention: shared test packages created in an early PR, consumed     by all subsequent PRs (specified in Phase 5, item 1).   - Golden dataset staleness (regression baselines not updated).     Prevention: every PR that changes output format includes a golden     dataset regeneration step with a verification command.   - Interface over-specification (freezing APIs too early).     Prevention: interfaces are frozen only after at least one     non-default implementation is designed (even if not yet built).     The freeze PR must demonstrate the interface accommodates two     implementations.</p> <p>DES-specific (from design guidelines Section 6):   - The Type Catalog trap: macro plan includes Go struct definitions     that diverge from implementation. Prevention: describe modules     behaviorally, not as Go types.   - Fidelity for its own sake: modeling components that don't affect     any analysis question. Prevention: every component must trace to     a modeling decision in Phase 1.   - Golden tests without invariant tests: characterization tests that     encode bugs as expected values. Prevention: every subsystem with     golden tests must have companion invariant tests.   - Mixing exogenous and endogenous: tight coupling between workload     generation and simulation logic that prevents replay experiments.     Prevention: exogenous inputs must be separable from endogenous     simulation logic.   - Interface leaking implementation: interfaces that encode one     backend's data model instead of the abstract behavioral contract.     Prevention: interfaces must accommodate at least two backends     (even if only one is implemented initially).</p> <p>Module architecture (from design guidelines Section 6.2):   - Shotgun surgery: multiple construction sites for the same type.     Prevention: canonical constructors for types constructed in &gt;1 place.   - Destructive reads: methods that both query and clear state.     Prevention: separate Get() and Consume() methods.   - Monolith methods: single methods containing logic for multiple     modules. Prevention: each module's logic callable through its     own interface.   - Config mixing concerns: single config struct combining unrelated     parameters. Prevention: group configuration by module.</p> <p>====================================================================== OUTPUT FORMAT (STRICT) ======================================================================</p> <p>A) Executive Summary (under 15 lines \u2014 synthesize the elevator pitch:    what is being built, why, how many PRs, key milestones) B) Repository Recon Summary C) High-Level Objectives + Non-Goals + Model Scoping Table D) Concept Model (under 80 lines \u2014 building blocks with module    contracts, interactions, invariants, extension points, real-system    correspondence) E) Architectural Risk Register F) Architectural Evolution (current -&gt; target, mapped to concept model,    described BEHAVIORALLY \u2014 no Go code) G) Frozen Interface Reference (ONLY for interfaces whose freeze PR    has already merged \u2014 Go signatures with per-PR annotations.    Include both interfaces frozen BY this plan's PRs and pre-existing    frozen interfaces that this plan depends on. Omit entirely if no    interfaces are frozen yet.) H) Cross-Cutting Infrastructure Plan I) PR Plan (PR0...PRN, Tier 1 + Tier 2 per PR) J) Dependency DAG K) Design Bug Prevention Checklist</p> <p>CONTEXT BUDGET RULE: Sections A, C, and D are the human-review core and must be concise. I-Tier-1 summaries should target 15 lines each (max 25). All other sections are reference material consulted on demand. The plan should be structured so a human can review the core sections (A + C + D + all I-Tier-1 summaries) without needing to read the rest.</p> <p>ABSTRACTION LEVEL CHECK (final gate): Before submitting, verify: - Section F contains ZERO lines of Go code - Section G contains ONLY frozen interface signatures (merged code) - Sections A-E and H-K contain ZERO Go code - All pre-freeze interfaces are described behaviorally - All module contracts use the template from Phase 2, not Go structs</p> <p>======================================================================</p> <p>Quality bar:</p> <ul> <li>Grounded in real code with file:line citations.</li> <li>No hallucinated modules or behaviors.</li> <li>No dead code.</li> <li>No bloated PRs.</li> <li>Must withstand expert review.</li> <li>Must be realistic and implementable.</li> <li>Concept model must be simple enough to explain verbally in 2 minutes.</li> <li>Consistent with design guidelines (docs/templates/design-guidelines.md).</li> <li>Every building block has a module contract.</li> <li>Every PR has an extension type.</li> <li>Every modeling decision traces to an analysis question.</li> </ul> <p>====================================================================== LIVING DOCUMENT PROTOCOL ======================================================================</p> <p>This plan will evolve. When updating:</p> <p>1) Add a dated revision note at the top explaining what changed and why. 2) If a risk register validation fails, document the finding and the    resulting plan changes explicitly. 3) Never silently change a PR's behavioral guarantees \u2014 if contracts    change, note the old contract, new contract, and reason. 4) Track completed PRs by marking their status in the PR plan section. 5) After each PR merges, check: does the concept model still accurately    describe the system? If not, update it. A stale concept model is    worse than no concept model.</p> <p>======================================================================</p> <p>Think deeply before answering. Inspect before designing. Validate before committing. Scope before modeling. Describe behavior, not implementation.</p>"},{"location":"templates/micro-plan/","title":"Micro Plan","text":"<p>You are operating inside a real repository with full code access.</p> <p>You are tasked with producing a PR-SPECIFIC IMPLEMENTATION PLAN that combines: 1. Design rigor (behavioral contracts, architecture validation) 2. Executable task breakdown (TDD, bite-sized steps, verifications)</p> <p>The source of work may be: - A section in an approved Macro Plan (e.g., \"Phase 2, PR 4\") - One or more GitHub issues (e.g., \"#183, #189, #195\") - A design document (e.g., \"docs/plans/2026-02-18-hardening-design.md\") - A feature request or bug report description</p> <p>This plan has TWO AUDIENCES: 1) A human reviewer who validates behavioral correctness 2) Automated agents (via executing-plans skill) who execute the tasks</p> <p>The plan must be comprehensive enough that agents can implement WITHOUT additional codebase exploration.</p> <p>====================================================================== DOCUMENT HEADER (REQUIRED) ======================================================================</p> <p>Every plan MUST start with this exact header format:</p> <pre><code># [PR Title] Implementation Plan\n\n&gt; **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.\n\n**Goal:** [One sentence a non-contributor could understand \u2014 what capability does this PR add? Avoid type names, package paths, or implementation jargon.]\n\n**The problem today:** [2-3 sentences explaining what's missing or broken without this PR. What can't users or the system do? Why does it matter?]\n\n**What this PR adds:** [Numbered list of 2-4 concrete capabilities, each explained in plain language with a brief example. E.g., \"Decision traces \u2014 a log of every routing decision: 'request_42 was sent to instance_2 because it had the highest score of 0.87'\"]\n\n**Why this matters:** [1-2 sentences connecting this PR to the broader project vision. How does this enable downstream work?]\n\n**Architecture:** [2-3 sentences about the technical approach \u2014 packages, key types, integration points. Implementation jargon is OK here since the motivation is already established above.]\n\n**Source:** [Link to the source of work. Examples:\n  - Macro plan: \"Phase 2, PR 4 in docs/plans/macro-plan.md\"\n  - Issues: \"GitHub issues #183, #189, #195, #196, #197, #198, #199, #200\"\n  - Design doc: \"docs/plans/2026-02-18-hardening-design.md\"\n  - Feature request: \"GitHub issue #42\"]\n\n**Closes:** [Issue numbers this PR will close on merge, using GitHub closing keywords.\n  Omit if the source is a macro plan section with no linked issues.\n  Examples:\n  - \"Fixes #183, fixes #189, fixes #195\"\n  - \"Closes #42\"\n  - \"N/A \u2014 source is macro plan, no linked issues\"]\n\n**Behavioral Contracts:** See Part 1, Section B below\n\n---\n</code></pre> <p>The header has TWO audiences reading in order: 1. A human reviewer who needs to understand WHY before HOW (Goal \u2192 Problem \u2192 What \u2192 Why) 2. An implementing agent who needs the technical approach (Architecture)</p> <p>====================================================================== PHASE 0 \u2014 COMPONENT CONTEXT ======================================================================</p> <p>Identify this PR's place in the system architecture:</p> <p>1) Which building block is being added or modified? 2) What are the adjacent blocks it interacts with? 3) What invariants does this PR touch? 4) What state ownership changes (if any)? 5) Construction Site Audit: For every struct this PR adds fields to,    grep for ALL places that struct is constructed (struct literals,    factory functions). List each site with file:line. If there are    multiple construction sites, the plan MUST either:    a) Add a canonical constructor and refactor all sites, OR    b) Update every site explicitly (list each in a task)</p> <p>Then inspect ONLY the relevant parts of the repository.</p> <p>List confirmed facts (with file:line citations). Flag anything from the source document (macro plan, design doc, or issue description) that doesn't match current code as a DEVIATION \u2014 these must be resolved before implementation begins.</p> <p>====================================================================== OUTPUT FORMAT (STRICT) ======================================================================</p> <p>--- PART 1: Design Validation (Human Review, target &lt;120 lines) ---</p> <p>A) Executive Summary (5-10 lines)    - What this PR builds (plain language, not type/package names)    - Where it fits in the system (what comes before it, what depends on it)    - Adjacent blocks it interacts with    - Any DEVIATION flags from Phase 0</p> <p>B) Behavioral Contracts (Phase 1)    - 3-15 named contracts (BC-1, BC-2, ...)    - Format: GIVEN / WHEN / THEN / MECHANISM    - Grouped: positive contracts, negative contracts, error handling</p> <p>C) Component Interaction (Phase 2)    - Component diagram (text)    - API contracts    - State changes and ownership</p> <p>D) Deviation Log (Phase 3)    - Compare micro plan vs source document    - Table: | Source Says | Micro Does | Reason |</p> <p>E) Review Guide (Phase 7-B)    - The tricky part    - What to scrutinize    - What's safe to skim    - Known debt</p> <p>--- PART 2: Executable Implementation (Agent Execution) ---</p> <p>F) Implementation Overview (Phase 4 summary)    - Files to create/modify (one-line each)    - Key decisions    - Confirmation: no dead code, all paths exercisable</p> <p>G) Task Breakdown (Phase 4 detailed)    - 6-12 tasks in TDD format (see Phase 4 template below)    - Continuous execution (no pause points between tasks)    - Each task: test \u2192 fail \u2192 implement \u2192 pass \u2192 lint \u2192 commit</p> <p>H) Test Strategy (Phase 6)    - Map contracts to tasks/tests    - Golden dataset update strategy    - Shared test infrastructure usage</p> <p>I) Risk Analysis (Phase 7-A)    - Risks with likelihood/impact/mitigation</p> <p>--- PART 3: Quality Assurance ---</p> <p>J) Sanity Checklist (Phase 8)    - Pre-implementation verification    - All items from Phase 8 template</p> <p>--- APPENDIX: File-Level Implementation Details ---</p> <p>K) Detailed specifications    - Complete function signatures with doc comments    - Struct definitions    - Event execution logic    - Metric aggregation rules    - RNG subsystem usage    - Any behavioral subtleties (file:line citations)</p> <p>====================================================================== PHASE 1 \u2014 BEHAVIORAL CONTRACTS (Human-Reviewable) ======================================================================</p> <p>This defines what this PR guarantees. Use named contracts (BC-1, BC-2, ...) that can be referenced in tests, reviews, and future PRs.</p> <p>For each contract:</p> <p>BC-N:    - GIVEN    - WHEN    - THEN    - MECHANISM:  (optional but recommended) <p>Group contracts into:</p> <p>1) Behavioral Contracts (what MUST happen)    - Normal operation    - Edge cases    - Backward compatibility</p> <p>2) Negative Contracts (what MUST NOT happen)    - Invariant violations this PR could cause    - Cross-boundary state leaks    - Performance regressions</p> <p>3) Error Handling Contracts    - What happens on invalid input    - What happens on resource exhaustion    - Panic vs error return vs log-and-continue (be explicit)</p> <p>TARGET: 3-15 contracts per PR. Pure refactoring PRs with no new behavior may have as few as 3. More than 15 means the PR may be too large.</p> <p>No vague wording. \"Should\" is banned \u2014 use \"MUST\" or \"MUST NOT.\"</p> <p>THEN CLAUSE QUALITY GATE: Every THEN clause must describe OBSERVABLE BEHAVIOR, not internal structure. The THEN clause directly becomes the test assertion \u2014 a structural THEN produces a structural test.</p> <p>Check each THEN clause against this filter: - Does it contain a concrete type name? \u2192 Rewrite to describe behavior   BAD:  \"THEN it returns a ConstantPriority\"   GOOD: \"THEN it returns a policy that computes 0.0 for any request\" - Does it reference an internal field? \u2192 Rewrite to describe output   BAD:  \"THEN the router's scoreCache has 3 entries\"   GOOD: \"THEN the next routing decision uses cached affinity\" - Does it reproduce a formula? \u2192 Rewrite to describe ordering/outcome   BAD:  \"THEN score equals 0.6cacheHit + 0.4*(1-load)\"   GOOD: \"THEN instances with higher cache hit rates rank higher\" - Does it survive a refactor? \u2192 If renaming a struct or changing an   internal algorithm would invalidate this THEN, it is structural</p> <p>====================================================================== PHASE 2 \u2014 COMPONENT INTERACTION (Human-Reviewable) ======================================================================</p> <p>Describe this PR's building block and how it connects to the system. This is the \"box-and-arrow\" view, NOT the file-level view.</p> <p>1) Component Diagram (text-based)    - This PR's component and its responsibility    - Adjacent components (existing or new)    - Data flow direction between them    - What crosses each boundary (types, not implementations)</p> <p>2) API Contracts    - New interfaces or types (signature + one-line semantics)    - Method preconditions and postconditions    - Failure modes and how callers handle them</p> <p>3) State Changes    - New mutable state and its owner    - State lifecycle (created when, destroyed when, accessed by whom)</p> <p>4) Extension Friction Assessment    - For the main new type/field this PR adds, count: how many files      must change to add ONE more field of the same kind?    - If &gt;3 files, document whether this is acceptable or whether a      structural improvement should happen first/concurrently    - This is not a blocker \u2014 it's awareness for the reviewer</p> <p>TARGET: under 40 lines. Infrastructure PRs that introduce multiple interacting types may go up to 60 lines with justification. Beyond 60 lines, the PR scope is likely too broad.</p> <p>====================================================================== PHASE 3 \u2014 DEVIATION LOG ======================================================================</p> <p>Compare this micro plan against the source document (macro plan section, design doc, or issue description).</p> <p>For each difference:</p> Source Says Micro Plan Does Reason <p>Categories of deviation: - SIMPLIFICATION: Source specified more than needed at this stage - CORRECTION: Source was wrong about existing code or behavior - DEFERRAL: Feature moved to a later PR (explain why) - ADDITION: Something the source missed - SCOPE_CHANGE: Issue description expanded or narrowed during investigation</p> <p>If there are zero deviations, state \"No deviations from source document.\"</p> <p>====================================================================== PHASE 4 \u2014 EXECUTABLE TASK BREAKDOWN ======================================================================</p> <p>Break implementation into 6-12 tasks following TDD principles. Each task is completable in one focused session (~30-45 minutes).</p> <p>Execution is continuous \u2014 all tasks run sequentially without pausing for human input. Execution only stops on test failure, lint failure, or build error. Group tasks into logical sections (e.g., core types, integration, edge cases) for readability, but these are NOT pause points.</p> <p>TASK TEMPLATE:</p>"},{"location":"templates/micro-plan/#task-n-componentfeature-name","title":"Task N: [Component/Feature Name]","text":"<p>Contracts Implemented: BC-X, BC-Y (reference Phase 1)</p> <p>Files: - Create: <code>exact/path/to/file.go</code> - Modify: <code>exact/path/to/existing.go:123-145</code> (line range if known) - Test: <code>exact/path/to/test_file.go</code></p> <p>Step 1: Write failing test for [specific contract]</p> <p>Context: [1-2 sentences explaining what we're testing and why]</p> <pre><code>// Complete test code here\n// Include setup, execution, assertions\nfunc TestComponent_Scenario_Behavior(t *testing.T) {\n    // GIVEN [precondition from contract]\n\n    // WHEN [action from contract]\n\n    // THEN [expected outcome from contract]\n    assert.Equal(t, expected, actual)\n}\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>go test ./path/to/package/... -run TestComponent_Scenario -v</code> Expected: FAIL with \"[expected error message]\"</p> <p>Step 3: Implement minimal code to satisfy contract</p> <p>Context: [1-2 sentences about the implementation approach]</p> <p>In <code>path/to/file.go</code>: <pre><code>// Complete implementation code\n// Include type definitions, method signatures, logic\ntype Component struct {\n    field1 Type1\n    field2 Type2\n}\n\nfunc (c *Component) Method(param Type) (ReturnType, error) {\n    // implementation\n}\n</code></pre></p> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>go test ./path/to/package/... -run TestComponent_Scenario -v</code> Expected: PASS</p> <p>Step 5: Run lint check</p> <p>Run: <code>golangci-lint run ./path/to/package/...</code> Expected: No new issues (pre-existing issues OK, don't fix them)</p> <p>Step 6: Commit with contract reference</p> <pre><code>git add path/to/file.go path/to/test_file.go\ngit commit -m \"feat(package): implement Component.Method (BC-X, BC-Y)\n\n- Add Component type with Method\n- Implement contract BC-X: [brief description]\n- Implement contract BC-Y: [brief description]\n\nCo-Authored-By: Claude &lt;noreply@anthropic.com&gt;\"\n</code></pre> <p>REPEAT TASK TEMPLATE for each task (6-12 total).</p> <p>IMPORTANT TASK DESIGN RULES:</p> <ol> <li> <p>Each task implements 1-3 related contracts - don't split single    contracts across tasks, don't pack unrelated contracts together</p> </li> <li> <p>Complete code in every step - no \"add validation\" or \"implement logic\"    without showing the exact code</p> </li> <li> <p>Exact commands with expected output - agent should know if verification    succeeded or failed</p> </li> <li> <p>Reference shared test infrastructure - use existing helpers from    shared packages (e.g., sim/internal/testutil), don't duplicate</p> </li> <li> <p>Golden dataset updates - if task changes output format or metrics,    include step to update testdata/goldendataset.json with regeneration command</p> </li> <li> <p>Dependency ordering - tasks must be ordered so each can build on    previous completed work</p> </li> <li> <p>No dead code - every struct field, every method, every parameter must    be used by the end of the task or a subsequent task in this PR</p> </li> <li> <p>Commit messages - use conventional commits format with contract references    (feat/fix/refactor/test/docs)</p> </li> <li> <p>Behavioral assertions only - every assertion in a test must    verify OBSERVABLE BEHAVIOR, not internal structure. Apply the    refactor survival test: \"Would this test still pass if the    implementation were completely rewritten but the behavior preserved?\"</p> </li> </ol> <p>PROHIBITED assertion patterns (structural \u2014 these break on refactor):    - Type assertions: <code>policy.(*ConcreteType)</code> \u2014 test behavior instead    - Internal field access: <code>obj.internalField</code> \u2014 test through public API    - Exact formula reproduction: <code>assert.Equal(score, 0.6*cache + 0.4*load)</code>      \u2014 test the ranking/ordering outcome instead    - Implementation count: <code>assert.Equal(len(obj.items), 3)</code> \u2014 test      what the items produce, not how many there are</p> <p>REQUIRED assertion patterns (behavioral \u2014 these survive refactor):    - Observable output: <code>assert.Equal(policy.Compute(req, clock), 0.0)</code>    - Behavioral outcome: <code>assert.Equal(decision.TargetInstance, 1)</code>    - Invariant verification: <code>assert.Equal(completed+queued+running+dropped, injected)</code>    - Ordering/ranking: <code>assert.True(scoreA &gt; scoreB)</code> when contract says      A should rank higher than B</p> <ol> <li> <p>THEN clauses must be behavioral - if a behavioral contract's     THEN clause contains a concrete type name, internal field name, or     implementation detail, rewrite the THEN clause BEFORE writing the     test. The THEN clause drives the assertion; a structural THEN     produces a structural test.</p> <p>BAD:  \"THEN it returns a *ConstantPriority\" GOOD: \"THEN it returns a policy that computes 0.0 for any request\"</p> <p>BAD:  \"THEN the router's scoreCache has 3 entries\" GOOD: \"THEN the next routing decision uses cached scores (latency &lt; uncached)\"</p> <p>BAD:  \"THEN the score equals 0.6cacheHit + 0.4(1-load)\" GOOD: \"THEN instances with higher cache hit rates score higher than        instances with lower cache hit rates, all else being equal\"</p> </li> </ol> <p>====================================================================== PHASE 5 \u2014 REMOVED (Merged into Phase 4 Task Verification) ======================================================================</p> <p>Exercisability is proven by the task-level verification steps. No separate section needed.</p> <p>====================================================================== PHASE 6 \u2014 TEST STRATEGY ======================================================================</p> <p>Map contracts to tasks and tests:</p> Contract Task Test Type Test Name / Description BC-1 Task 1 Unit TestFoo_GivenX_ThenY BC-2 Task 1 Unit TestFoo_GivenZ_ThenW BC-3 Task 2 Golden TestCluster_SingleInstance_MatchesGolden ... ... ... ... <p>Test types: - Unit: specific function/method behavior - Integration: cross-component or CLI-level - Golden: regression against known-good output (testdata/goldendataset.json) - Invariant: system law that must hold regardless of output values (see req 6) - Failure: error paths, panics, edge cases - Benchmark: performance-sensitive paths (optional)</p> <p>Additional requirements:</p> <ol> <li> <p>Shared test infrastructure: Use existing helpers from shared test    packages (e.g., sim/internal/testutil). If new helpers are needed, add    them to the shared package in an early task \u2014 not duplicated locally.</p> </li> <li> <p>Golden dataset updates: If this PR changes output format or adds new    metrics, document:</p> </li> <li>Which task updates the golden dataset</li> <li>Exact regeneration command</li> <li> <p>How to verify the update is correct (compare key metrics)</p> </li> <li> <p>Lint requirements: <code>golangci-lint run ./...</code> must pass with zero new    issues. Pre-existing issues are acceptable; do not fix unrelated lint    warnings (scope creep).</p> </li> <li> <p>Test naming convention: Use BDD-style names that describe the scenario:    <code>TestType_Scenario_Behavior</code> (e.g., <code>TestTokenBucket_CapacityExceeded_RejectsRequest</code>)</p> </li> <li> <p>Test isolation: Each test must be independently runnable (no order    dependencies). Use table-driven tests for multiple scenarios of the same behavior.</p> </li> <li> <p>Invariant tests alongside golden tests (MANDATORY): Golden dataset    tests are characterization tests \u2014 they capture what the code does, not    what the code should do. If the code has a bug when the golden dataset    is generated, the test encodes the bug as the expected value. This has    happened: issue #183 found that the codellama golden dataset expected 499    completions because one request was silently dropped \u2014 a bug that the    golden test perpetuated instead of catching.</p> </li> </ol> <p>Rule: Every golden dataset test MUST be paired with at least one    invariant test that verifies a system law derived from the specification    (not from running the code). Invariant tests answer \"is the code correct?\"    while golden tests answer \"did the code change?\"</p> <p>Key invariants for this simulator (derived from CLAUDE.md):    - Request conservation: completed + still_queued + still_running + dropped_unservable = injected    - KV block conservation: allocated_blocks + free_blocks = total_blocks    - Clock monotonicity: simulation clock never decreases    - Causality: arrival_time \u2264 enqueue_time \u2264 schedule_time \u2264 completion_time    - Determinism: same seed produces byte-identical output across runs</p> <p>When adding a golden test, ask: \"If this golden value were wrong, would    any other test catch it?\" If the answer is no, add an invariant test.    If this PR touches request lifecycle, KV cache, or metrics, at least one    invariant test MUST be added or extended.</p> <p>====================================================================== PHASE 7 \u2014 RISK ANALYSIS &amp; REVIEW GUIDE ======================================================================</p> <p>PART A: Risks</p> <p>For each risk: - Risk description - Likelihood (low/medium/high) - Impact (low/medium/high) - Mitigation (specific test or design choice) - Which task mitigates the risk</p> <p>PART B: Review Guide (for the human reviewer)</p> <p>In 5-10 lines, tell the reviewer:</p> <p>1) THE TRICKY PART: What's the most subtle or error-prone aspect? 2) WHAT TO SCRUTINIZE: Which contract(s) are hardest to verify? 3) WHAT'S SAFE TO SKIM: Which parts are mechanical/boilerplate? 4) KNOWN DEBT: Any pre-existing issues encountered but not fixed?</p> <p>This section exists because human attention is scarce. Direct it to where it matters most.</p> <p>====================================================================== PHASE 8 \u2014 DESIGN SANITY CHECKLIST ======================================================================</p> <p>Before implementation, verify:</p> <p>Plan-specific checks: - [ ] No unnecessary abstractions. - [ ] No feature creep beyond PR scope. - [ ] No unexercised flags or interfaces. - [ ] No partial implementations. - [ ] No breaking changes without explicit contract updates. - [ ] No hidden global state impact. - [ ] All new code will pass golangci-lint. - [ ] Shared test helpers used from existing shared test package (not duplicated locally). - [ ] CLAUDE.md updated if: new files/packages added, file organization       changed, plan milestone completed, new CLI flags added. - [ ] No stale references left in CLAUDE.md. - [ ] Documentation DRY: If this PR modifies a canonical source (docs/standards/rules.md, docs/standards/invariants.md, docs/standards/principles.md, docs/extension-recipes.md), all working copies in the source-of-truth map are updated. If a new file is added, it appears in the CLAUDE.md File Organization tree. - [ ] Deviation log reviewed \u2014 no unresolved deviations. - [ ] Each task produces working, testable code (no scaffolding). - [ ] Task dependencies are correctly ordered. - [ ] All contracts are mapped to specific tasks. - [ ] Golden dataset regeneration documented (if needed). - [ ] Construction site audit completed (Phase 0, item 5) \u2014 all struct       construction sites listed and covered by tasks. - [ ] If this PR is part of a macro plan, the macro plan status is updated.</p> <p>Antipattern rules (full details in docs/standards/rules.md): - [ ] R1: No silent <code>continue</code>/<code>return</code> dropping data - [ ] R2: Map keys sorted before float accumulation or ordered output - [ ] R3: Every new CLI flag validated (zero, negative, NaN, Inf) - [ ] R4: All struct construction sites audited for new fields - [ ] R5: Resource allocation loops handle mid-loop failure with rollback - [ ] R6: No <code>logrus.Fatalf</code> or <code>os.Exit</code> in <code>sim/</code> packages - [ ] R7: Invariant tests alongside any golden tests - [ ] R8: No exported mutable maps - [ ] R9: <code>*float64</code> for YAML fields where zero is valid - [ ] R10: YAML strict parsing (<code>KnownFields(true)</code>) - [ ] R11: Division by runtime-derived denominators guarded - [ ] R12: Golden dataset regenerated if output changed - [ ] R13: New interfaces work for 2+ implementations - [ ] R14: No method spans multiple module responsibilities - [ ] R15: Stale PR references resolved - [ ] R16: Config params grouped by module - [ ] R17: Routing scorer signals documented for freshness tier - [ ] R18: CLI flag values not silently overwritten by defaults.yaml - [ ] R19: Unbounded retry/requeue loops have circuit breakers - [ ] R20: Detectors and analyzers handle degenerate inputs (empty, skewed, zero)</p> <p>====================================================================== APPENDIX \u2014 FILE-LEVEL IMPLEMENTATION DETAILS ======================================================================</p> <p>This section has NO LENGTH LIMIT. It should contain everything needed to implement the PR without further codebase exploration.</p> <p>For each file to be created or modified, provide:</p> <p>File: <code>exact/path/to/file.go</code></p> <p>Purpose: [1-2 sentences]</p> <p>Complete Implementation:</p> <pre><code>// Package documentation\n\npackage name\n\nimport (\n    // all imports\n)\n\n// Complete type definitions with doc comments\n// Complete function implementations\n// Complete test code\n// Include all struct fields, all methods, all parameters\n\n// Behavioral notes:\n// - [Any subtlety, e.g., \"horizon boundary: requests at exactly\n//    horizon time are NOT completed\"]\n// - [Citation: existing behavior to preserve, with file:line]\n</code></pre> <p>Key Implementation Notes: - RNG usage: [Which subsystem from PartitionedRNG? e.g., \"SubsystemRouter\"] - Metrics: [What metrics are collected? Where aggregated?] - Event ordering: [Priority? Timestamp? Secondary tie-breaking?] - State mutation: [What gets modified? Who owns it?] - Error handling: [Panic, return error, log-and-continue?]</p> <p>Include this level of detail for EVERY file touched by this PR.</p> <p>====================================================================== EXECUTION HANDOFF ======================================================================</p> <p>After creating the plan, the workflow continues with:</p> <p>Option 1: Subagent-Driven Development (in current session) - Invoke superpowers:subagent-driven-development - Fresh subagent per task - Code review between tasks - Fast iteration</p> <p>Option 2: Worktree with executing-plans (recommended for complex PRs) - Create isolated worktree (superpowers:using-git-worktrees) - Continue in same session (.worktrees/) or open new session (sibling directory) - Invoke superpowers:executing-plans with this plan - Continuous execution (stops only on failure) - Invoke commit-commands:commit-push-pr when complete</p> <p>====================================================================== QUALITY BAR ======================================================================</p> <p>This plan must: - Survive expert review (behavioral contracts are sound) - Survive systems-level scrutiny (architecture is correct) - Eliminate dead code (all code exercisable immediately) - Reduce implementation bugs (TDD, explicit verifications) - Stay strictly within source document scope (deviations justified) - Pass golangci-lint with zero new issues - Enable automated execution (complete code, exact commands) - Map every contract to a task (traceability)</p> <p>====================================================================== LINTING REQUIREMENTS ======================================================================</p> <p>This project uses golangci-lint for static analysis. Version is pinned in CI (see .github/workflows/ci.yml).</p> <p>Local verification (run before submitting PR): <pre><code>golangci-lint run ./...\n</code></pre></p> <p>Rules: 1. All NEW code must pass lint with zero issues. 2. Do not fix pre-existing lint issues in unrelated code (scope creep). 3. If a lint rule seems wrong, document why and discuss before disabling.</p> <p>======================================================================</p> <p>Think carefully. Inspect deeply. Design defensively. Break into executable tasks. Verify every step. Direct the reviewer's attention wisely.</p>"}]}