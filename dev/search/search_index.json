{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"BLIS \u2014 Blackbox Inference Simulator","text":"<p>A discrete-event simulator for LLM inference serving systems. BLIS models multi-instance clusters with configurable admission control, request routing, KV-cache dynamics (including tiered GPU+CPU offloading), scheduling policies, and token generation \u2014 all driven by pluggable latency models (data-driven coefficients, analytical roofline, or custom backends).</p> <p>The simulator is CPU-only, deterministic, and designed for capacity planning, policy optimization research, and performance prediction across model/GPU/TP configurations without requiring real GPUs.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>git clone https://github.com/inference-sim/inference-sim.git\ncd inference-sim\ngo build -o blis main.go\n./blis run --model meta-llama/llama-3.1-8b-instruct\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Discrete-event simulation for prefill, decode, and request scheduling</li> <li>Deterministic execution \u2014 same seed produces byte-identical output across runs</li> <li>KV-cache modeling with prefix caching and tiered GPU+CPU offload</li> <li>Chunked prefill and preemption-aware batch formation</li> <li>Pluggable latency models \u2014 blackbox (data-driven coefficients) and roofline (analytical FLOPs/bandwidth), with an extensible interface for custom backends</li> <li>Multi-instance cluster simulation with shared-clock event loop</li> <li>Pluggable routing policies \u2014 round-robin, least-loaded, and composable weighted-scoring with prefix-affinity, queue-depth, and KV-utilization scorers</li> <li>Admission control, priority policies, and instance schedulers \u2014 each a pluggable policy axis</li> <li>Canonical workload specification \u2014 multi-client YAML DSL with Poisson/Gamma/Weibull/constant arrival processes, 5 distribution types, SLO classes (critical/standard/sheddable/batch/background), prefix groups, cohort dynamics, multimodal, reasoning multi-turn, and composable specs via <code>blis compose</code></li> <li>Rich metrics pipeline \u2014 per-request, per-instance, and cluster-level metrics including TTFT/ITL/E2E distributions, KV cache diagnostics, anomaly detection (priority inversions, HOL blocking), SLO attainment, Jain fairness index, and multi-objective fitness evaluation</li> <li>Decision tracing and counterfactual analysis with top-k regret computation</li> <li>Hypothesis experimentation framework for rigorous, reproducible experiments</li> </ul>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>Request Arrival \u2192 Admission \u2192 Routing \u2192 WaitQueue \u2192 Batch Formation \u2192 Step Execution \u2192 Completion\n                                            \u2193              \u2193\n                                      KV Allocation   Latency Estimation\n</code></pre> <p>Admission and Routing apply in cluster mode (multi-instance). Single-instance mode skips directly to WaitQueue.</p>"},{"location":"#documentation-guide","title":"Documentation Guide","text":"Section What You'll Find Getting Started What is BLIS, installation, quick start, capacity planning tutorial Concepts System architecture, core engine, glossary, roofline estimation User Guide Task-oriented guides: routing, admission, scheduling, latency models, KV cache, workloads, cluster, metrics, experimentation Reference Configuration reference, supported models, workload spec schema Contributing Extension recipes, PR workflow, standards, templates"},{"location":"#reading-order-for-newcomers","title":"Reading Order for Newcomers","text":"<ol> <li>What is BLIS? \u2014 understand the problem BLIS solves</li> <li>Quick Start \u2014 run your first simulation</li> <li>Tutorial: Capacity Planning \u2014 end-to-end walkthrough</li> <li>Glossary \u2014 learn BLIS-specific terminology</li> <li>User Guide \u2014 task-oriented how-to guides</li> </ol>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the Apache License, Version 2.0. See LICENSE for details.</p>"},{"location":"concepts/","title":"Concepts","text":"<p>These pages explain BLIS's architecture and core mechanisms at the level needed to understand the system without reading source code. For task-oriented how-to guides, see the User Guide.</p>"},{"location":"concepts/#concept-pages","title":"Concept Pages","text":"Page Description Glossary Definitions of BLIS-specific terminology Cluster Architecture Multi-instance simulation: admission, routing, scorer composition, snapshot freshness, shared-clock event loop Core Engine Single-instance DES engine: event queue, Step() phases, request lifecycle, batch formation, KV cache, latency models Roofline Estimation Analytical GPU step time estimation without training data"},{"location":"concepts/#diagrams","title":"Diagrams","text":"Diagram Description End-to-end cluster pipeline: request arrival through metrics output Request state machine: states, transitions, and metric recording points DES event loop: min-heap queue, clock advancement, Step() decomposition Weighted scorer composition: per-scorer normalization, weight multiplication, argmax selection"},{"location":"concepts/#reading-order","title":"Reading Order","text":"<p>For newcomers to BLIS:</p> <ol> <li>Start with Glossary to learn BLIS-specific terminology</li> <li>Read Core Engine to understand the DES architecture and single-instance simulation</li> <li>Read Cluster Architecture to understand multi-instance orchestration</li> <li>Consult Configuration Reference when running experiments</li> <li>See Extension Recipes when adding new policies or features</li> </ol>"},{"location":"concepts/architecture/","title":"Cluster Architecture","text":"<p>This page describes how BLIS simulates multi-instance inference serving clusters. For single-instance simulation internals, see Core Engine.</p> <p>Canonical sources: Signal freshness (INV-7) is defined in <code>docs/contributing/standards/invariants.md</code>. If signal freshness descriptions here diverge, <code>invariants.md</code> is authoritative.</p>"},{"location":"concepts/architecture/#overview","title":"Overview","text":"<p>A BLIS cluster consists of N independent inference instances orchestrated by a shared-clock event loop. Each incoming request passes through a three-stage pipeline \u2014 admission, routing, and per-instance processing \u2014 before metrics are aggregated across all instances.</p> <pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  Requests \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  Admission   \u2502\u2500\u2500rejected\u2500\u2500\u25b6 (counted)\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502 admitted\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Routing   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502 target instance selected\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u25bc            \u25bc            \u25bc\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502Instance 0\u2502 \u2502Instance 1\u2502 \u2502Instance 2\u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502            \u2502            \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Metrics   \u2502\u2500\u2500\u25b6 stdout (JSON)\n                    \u2502 Aggregation \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p></p>"},{"location":"concepts/architecture/#shared-clock-event-loop","title":"Shared-Clock Event Loop","text":"<p>The cluster simulator maintains a single global clock shared across all instances. At each iteration, it compares the earliest cluster-level event (request arrival, admission decision, routing decision) against the earliest per-instance event (step completion, queueing), and processes whichever is earlier.</p> <p>Ordering rules: - Cluster events at time T are processed before instance events at time T (cluster-first priority) - When multiple instances have events at the same time, the instance with the lowest index goes first - Within a single instance, events are ordered by timestamp only</p> <p>The simulation terminates when the clock exceeds the configured horizon or no events remain.</p>"},{"location":"concepts/architecture/#admission-pipeline","title":"Admission Pipeline","text":"<p>Admission is the first gate in the online routing pipeline. Every incoming request is evaluated by the admission policy before being passed to routing.</p>"},{"location":"concepts/architecture/#built-in-admission-policies","title":"Built-in Admission Policies","text":"Policy Behavior <code>always-admit</code> Accept all requests (default) <code>token-bucket</code> Rate-limiting via a token bucket with configurable capacity and refill rate <code>reject-all</code> Reject all requests (for pathological testing) <p>Token bucket rate-limits by consuming tokens proportional to input length: each request consumes tokens equal to its input token count, tokens refill at a constant rate, and requests are rejected when the bucket has insufficient tokens. Capacity and refill rate are configured via <code>--token-bucket-capacity</code> and <code>--token-bucket-refill-rate</code>.</p> <p>Rejected requests are counted in the output metrics but do not enter the routing pipeline. To add a new admission policy, see Extension Recipes. See Configuration Reference for flag details.</p>"},{"location":"concepts/architecture/#routing-pipeline","title":"Routing Pipeline","text":"<p>Routing selects which instance receives an admitted request. The routing policy sees a <code>RouterState</code> containing routing snapshots for all instances and the current simulation clock.</p>"},{"location":"concepts/architecture/#simple-routing-policies","title":"Simple Routing Policies","text":"Policy Selection Rule <code>round-robin</code> Cyclic instance assignment <code>least-loaded</code> Instance with minimum effective load <code>always-busiest</code> Instance with maximum load (for pathological testing) <p>Effective load is defined as <code>QueueDepth + BatchSize + InFlightRequests</code>, where <code>InFlightRequests</code> counts requests that have been dispatched to an instance but not yet completed. This tracks the full dispatch-to-response lifecycle, matching real HTTP router behavior (llm-d, Envoy).</p>"},{"location":"concepts/architecture/#weighted-scoring-policy","title":"Weighted Scoring Policy","text":"<p>The <code>weighted</code> routing policy composes multiple scorers into a single routing decision. This is the recommended policy for production-like simulations and matches the architecture of real-world inference routers like llm-d's Endpoint Picker.</p> <p>The routing decision follows this pipeline:</p> <ol> <li>Score: Each scorer produces a per-instance score in [0, 1]</li> <li>Clamp: Scores are clamped to [0, 1] (scorers should already produce values in this range)</li> <li>Weight: Each score is multiplied by its configured weight</li> <li>Sum: Weighted scores are summed across scorers for each instance</li> <li>Select: The instance with the highest total score is chosen (argmax)</li> </ol> <p>Default weights: <code>prefix-affinity:3, queue-depth:2, kv-utilization:2</code> (llm-d parity). Note: weights are normalized to sum to 1.0 before scoring, so only weight ratios matter \u2014 <code>prefix-affinity:3,queue-depth:2</code> is identical to <code>prefix-affinity:30,queue-depth:20</code>. To add a new scorer, see Extension Recipes.</p> <p></p>"},{"location":"concepts/architecture/#scorer-composition","title":"Scorer Composition","text":"<p>Scorers are the building blocks of the weighted routing policy. Each scorer evaluates one signal dimension across all instances.</p>"},{"location":"concepts/architecture/#built-in-scorers","title":"Built-in Scorers","text":"Scorer Signal Score Computation Notes <code>prefix-affinity</code> Prefix cache overlap Proportion of request's block hashes found in instance's cache index Stateful: updates cache index after routing via observer <code>queue-depth</code> Instance load Min-max normalization of effective load (lower load = higher score) Stateless <code>kv-utilization</code> Memory pressure <code>1 - KVUtilization</code> (lower utilization = higher score) Stateless <code>load-balance</code> Instance load <code>1 / (1 + EffectiveLoad)</code> (decreasing function of load) Stateless"},{"location":"concepts/architecture/#stateful-vs-stateless-scorers","title":"Stateful vs. Stateless Scorers","text":"<p>Most scorers are stateless \u2014 they compute scores purely from the current routing snapshot. The <code>prefix-affinity</code> scorer is stateful: after a routing decision, an observer callback updates the router-side prefix cache index with the routed request's block hashes. This enables the scorer to track which prefixes are cached at which instance without querying the actual per-instance KV caches.</p>"},{"location":"concepts/architecture/#router-side-prefix-cache-index","title":"Router-Side Prefix Cache Index","text":"<p>The prefix-affinity scorer maintains a lightweight approximate cache of per-instance block hash history. This is separate from the actual per-instance KV cache and serves as a routing-time estimate of cache hit probability.</p> <p>Key properties: - Per-instance LRU with bounded capacity (default: 10,000 blocks) - Hierarchical block hashing: each block's hash chains with the prior block's hash for semantic prefix matching - Updated synchronously after each routing decision (synchronous freshness) - Score = proportion of request's block hashes found in the instance's cache index</p>"},{"location":"concepts/architecture/#signal-freshness","title":"Signal Freshness","text":"<p>Routing decisions depend on instance state signals with different freshness guarantees. Understanding freshness tiers is important for interpreting simulation results under high load.</p>"},{"location":"concepts/architecture/#freshness-tiers","title":"Freshness Tiers","text":"Tier Signals Update Mechanism Staleness Synchronous (router-local) InFlightRequests, prefix cache index Router increments InFlightRequests at dispatch, decrements at completion; prefix cache updated after each routing decision None \u2014 router owns this state Immediate/Periodic (instance-reported) QueueDepth, BatchSize, KVUtilization, FreeKVBlocks, CacheHitRate When <code>--snapshot-refresh-interval=0</code>: Immediate (read from instance at routing time). When <code>&gt;0</code>: all Prometheus-sourced signals share the same Periodic refresh interval, matching real vLLM's single <code>/metrics</code> endpoint (#463). Immediate: current within tick. Periodic: stale up to interval. <p>The <code>--snapshot-refresh-interval</code> flag controls how frequently Prometheus-sourced signals (QueueDepth, BatchSize, KVUtilization) are re-read from instances. Setting it to 0 (default) makes all signals Immediate. Non-zero values introduce realistic staleness matching real vLLM Prometheus scrape intervals.</p>"},{"location":"concepts/architecture/#counterfactual-regret","title":"Counterfactual Regret","text":"<p>When decision tracing is enabled (<code>--trace-level decisions</code>), BLIS computes counterfactual regret for each routing decision. This measures how much better an alternative routing choice could have been.</p> <p>Computation: 1. Score all candidate instances using the routing policy's scoring function 2. Rank candidates by score (descending) 3. Compute <code>regret = best_score - chosen_score</code> (clamped to &gt;= 0) 4. Record the top-k candidates with their scores and instance state</p> <p>Interpretation: - For the <code>weighted</code> policy, regret is typically zero because the chosen instance IS the highest-scored candidate - For <code>round-robin</code>, regret is non-zero because the policy ignores load signals - Higher regret does not necessarily imply worse performance \u2014 round-robin can achieve lower tail latency than least-loaded through perfect distribution uniformity</p> <p>Configure counterfactual analysis via <code>--counterfactual-k</code> (number of candidates to record per decision).</p>"},{"location":"concepts/architecture/#metrics-aggregation","title":"Metrics Aggregation","text":"<p>After simulation completes, per-instance metrics are aggregated into a unified cluster result:</p> Metric Category Aggregation TTFT, E2E Combined across all instances. JSON output: mean, p90, p95, p99. Internal <code>Distribution</code>: also p50, min, max (used by fitness evaluation). ITL: mean, p90, p95, p99 in JSON output. Throughput Total output tokens / simulation time; total requests / simulation time Request counts Sum of completed, queued, running, preempted, dropped across instances Per-SLO-class Separate distributions per SLO class (for multi-tenant analysis) Fairness Jain Fairness Index across tenant throughputs"},{"location":"concepts/architecture/#fitness-evaluation","title":"Fitness Evaluation","text":"<p>When <code>--fitness-weights</code> are configured, BLIS computes a single fitness score from the aggregated metrics. This enables automated policy comparison:</p> <ul> <li>Latency metrics (TTFT, E2E) are normalized via <code>1/(1 + value/1000)</code> where <code>value</code> is in ticks (microseconds) and 1000 ticks = 1ms is the reference point (lower latency = higher score). For example, TTFT of 50,000 ticks (50ms) maps to <code>1/(1+50) = 0.0196</code>.</li> <li>Throughput metrics are normalized via <code>value/(value + reference)</code> where <code>referenceRPS = 100.0</code> and <code>referenceTPS = 10000.0</code> (higher throughput = higher score)</li> <li>Normalized scores are multiplied by their configured weights and summed</li> <li>Higher fitness = better performance</li> </ul> <p>Note: the normalization compresses raw metric differences significantly. A 38% TTFT improvement might map to only an 8% fitness score difference. Always examine raw metrics alongside fitness scores.</p>"},{"location":"concepts/architecture/#instance-isolation","title":"Instance Isolation","text":"<p>Each instance in the cluster is a fully independent single-instance simulator with its own: - Event queue and simulation state - Wait queue and running batch - KV cache (block allocation, prefix caching, LRU eviction) - Latency model - Scheduling and priority policies</p> <p>Instances share the global clock but have no direct communication. All inter-instance coordination happens through the routing layer (via routing snapshots). This matches the architecture of real inference serving clusters where instances are independent processes.</p>"},{"location":"concepts/architecture/#online-routing-pipeline-walkthrough","title":"Online Routing Pipeline Walkthrough","text":"<p>A complete request lifecycle through the cluster pipeline:</p> <ol> <li>Generation: The workload generator creates a request with arrival time, input tokens, and output tokens</li> <li>ClusterArrivalEvent: Scheduled at the request's arrival time</li> <li>AdmissionDecisionEvent: Admission policy evaluates the request</li> <li>If rejected: request counted, pipeline ends</li> <li>If admitted: proceed to routing (with optional <code>--admission-latency</code> delay)</li> <li>RoutingDecisionEvent: Routing policy selects target instance</li> <li>InFlightRequests for target instance incremented</li> <li>Request injected into target instance's wait queue (with optional <code>--routing-latency</code> delay)</li> <li>QueuedEvent: Fired by target instance when request enters its queue</li> <li>If no StepEvent exists, one is scheduled (work-conserving)</li> <li>Per-instance processing: Request follows the single-instance lifecycle (see Core Engine)</li> <li>Completion/Drop: InFlightRequests decremented when request completes or is dropped as unservable</li> <li>Metrics: Request metrics (TTFT, E2E, ITL) recorded at instance level, aggregated at cluster level</li> </ol>"},{"location":"concepts/core-engine/","title":"Core Engine","text":"<p>This page describes BLIS's single-instance discrete event simulation engine. For multi-instance cluster orchestration, see Cluster Architecture.</p> <p>Canonical sources: System invariants (INV-1 through INV-8) are defined in <code>docs/contributing/standards/invariants.md</code>. If invariant descriptions here diverge, <code>invariants.md</code> is authoritative.</p>"},{"location":"concepts/core-engine/#overview","title":"Overview","text":"<p>Each BLIS instance is a self-contained discrete event simulator. The engine maintains an event queue (min-heap), processes events in timestamp order, and advances a simulation clock. The core loop is:</p> <pre><code>while events remain and clock &lt; horizon:\n    event = pop earliest event from queue\n    advance clock to event.timestamp\n    execute event (may produce new events)\n</code></pre> <p>The engine models the full vLLM inference pipeline: request arrival, queueing, batch formation, step execution, KV cache management, and latency estimation \u2014 all without real GPU hardware.</p>"},{"location":"concepts/core-engine/#event-queue","title":"Event Queue","text":"<p>The event queue is a min-heap ordered by event timestamp. Events represent state transitions in the simulation:</p> Event Type Trigger Effect <code>ArrivalEvent</code> Request enters system Computes queueing delay (alpha overhead), schedules <code>QueuedEvent</code> <code>QueuedEvent</code> Request enters wait queue Adds request to wait queue; if no <code>StepEvent</code> exists, schedules one (work-conserving) <code>StepEvent</code> Batch ready for execution Runs the 4-phase Step() cycle (see below) <code>ScheduledEvent</code> Request moves to running batch Timeline marker for tracing (scheduling delay recorded in <code>scheduleBatch</code>) <code>PreemptionEvent</code> KV cache eviction Timeline marker for tracing (preemption count recorded in <code>scheduleBatch</code>) <code>RequestLeftEvent</code> Request completes Timeline marker for tracing (E2E metrics recorded in <code>processCompletions</code>) <p>Clock monotonicity (INV-3): The simulation clock never decreases. Events are processed in strictly non-decreasing timestamp order.</p> <p>Work-conserving (INV-8): After every step completion, if the wait queue is non-empty, a <code>StepEvent</code> must exist in the event queue. The simulator never idles while work is waiting.</p> <p></p>"},{"location":"concepts/core-engine/#step-phases","title":"Step Phases","text":"<p>The <code>Step()</code> function is a 4-line orchestrator that delegates to four phases:</p>"},{"location":"concepts/core-engine/#phase-1-schedule-batch-schedulebatch","title":"Phase 1: Schedule Batch (<code>scheduleBatch</code>)","text":"<ol> <li>Assign priority scores to all queued requests via the priority policy</li> <li>Reorder the wait queue via the scheduling policy (e.g., FCFS, SJF, priority-based)</li> <li>Invoke batch formation to select which requests enter the running batch</li> <li>Schedule <code>PreemptionEvent</code> for any evicted requests</li> <li>Schedule <code>ScheduledEvent</code> for any newly admitted requests</li> </ol>"},{"location":"concepts/core-engine/#phase-2-execute-batch-step-executebatchstep","title":"Phase 2: Execute Batch Step (<code>executeBatchStep</code>)","text":"<ol> <li>Compute step time via the latency model based on batch composition</li> <li>For each request in the batch:</li> <li>If in prefill phase: advance the progress index through input tokens (respecting chunked prefill limits)</li> <li>If in decode phase: advance the progress index by one output token</li> <li>Record TTFT at the prefill-to-decode boundary</li> <li>Return the computed step time to Phase 4 for scheduling the next <code>StepEvent</code> (the clock itself advances only at the event-loop level when the next event is popped)</li> </ol>"},{"location":"concepts/core-engine/#phase-3-process-completions-processcompletions","title":"Phase 3: Process Completions (<code>processCompletions</code>)","text":"<p>Identify completed requests (all output tokens generated), release their KV blocks, record E2E metrics, and schedule <code>RequestLeftEvent</code>. Note: TTFT is already recorded in Phase 2 at the prefill-to-decode boundary, so Phase 3 only handles E2E and completion bookkeeping. This separation (TTFT in Phase 2, E2E in Phase 3) is the \"two-pass\" design that ensures TTFT is recorded before E2E.</p>"},{"location":"concepts/core-engine/#phase-4-schedule-next-step-schedulenextstep","title":"Phase 4: Schedule Next Step (<code>scheduleNextStep</code>)","text":"<ol> <li>If the running batch still has requests, schedule the next <code>StepEvent</code> at <code>now + stepTime</code></li> <li>If the running batch is empty but the wait queue has requests, schedule the next <code>StepEvent</code> at <code>now + stepTime</code> (work-conserving)</li> <li>If both are empty, do nothing (next event will be a future arrival)</li> </ol>"},{"location":"concepts/core-engine/#request-lifecycle","title":"Request Lifecycle","text":"<p>Requests follow a linear state machine with one exception (preemption):</p> <pre><code>                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                      \u2502         Preemption               \u2502\n                      \u2502    (re-enqueue at front)         \u2502\n                      \u25bc                                  \u2502\n  Arrival \u2500\u2500\u25b6 Queued \u2500\u2500\u25b6 Running \u2500\u2500\u25b6 Completed\n               \u2502                        \u2502\n               \u2502                        \u25bc\n               \u2502                   RequestLeft\n               \u2502\n  Arrival \u2500\u2500\u25b6 DroppedUnservable\n             (input too large for KV cache)\n</code></pre> <p></p>"},{"location":"concepts/core-engine/#states","title":"States","text":"State Description Queued In wait queue, not yet in running batch. Ordered by scheduling policy. Running In running batch, actively being processed. Progressing through prefill or decode. Completed All output tokens generated. KV blocks released. Metrics recorded."},{"location":"concepts/core-engine/#key-timestamps","title":"Key Timestamps","text":"<p>These are the conceptual timestamps in a request's lifecycle. Some are stored as struct fields (e.g., <code>ArrivalTime</code>, <code>FirstTokenTime</code>), while others are computed at metric recording time.</p> Timestamp When Recorded Used For Arrival time Request creation E2E and scheduling delay baseline Enqueue time After alpha queueing delay Conceptual start of wait queue residence Schedule time Batch formation selects request Scheduling delay = time in wait queue First token time End of prefill phase TTFT = FirstTokenTime (stored on Request) Completion time All tokens generated E2E = FirstTokenTime + sum(ITLs) <p>Causality invariant (INV-5): <code>arrival_time &lt;= enqueue_time &lt;= schedule_time &lt;= completion_time</code></p>"},{"location":"concepts/core-engine/#preemption","title":"Preemption","text":"<p>When KV cache pressure forces eviction, running requests are preempted: 1. Request is removed from the running batch (tail-first eviction) 2. Its KV blocks are freed 3. It is re-enqueued at the front of the wait queue (not the back) 4. A <code>PreemptionEvent</code> is recorded for tracing</p> <p>Preempted requests reset to the beginning of prefill (ProgressIndex = 0) and their KV blocks are freed. However, the freed blocks' prefix hashes are preserved in the KV cache's free list \u2014 when the request is re-scheduled, prefix caching may find these blocks (if not yet evicted by LRU), reducing recomputation. This matches vLLM's \"recompute\" preemption mode.</p>"},{"location":"concepts/core-engine/#dropped-requests","title":"Dropped Requests","text":"<p>Requests whose input tokens require more KV blocks than the total cache capacity are dropped at enqueue time with a <code>DroppedUnservable</code> counter increment. This prevents livelock where the simulator would endlessly preempt and re-enqueue a request that can never fit.</p>"},{"location":"concepts/core-engine/#batch-formation","title":"Batch Formation","text":"<p>BLIS models vLLM's continuous batching algorithm through the <code>VLLMBatchFormation</code> implementation.</p>"},{"location":"concepts/core-engine/#two-phase-algorithm","title":"Two-Phase Algorithm","text":"<p>Phase 1: Continuing Requests Process requests already in the running batch: - Apply chunked prefill limits (<code>--long-prefill-token-threshold</code>) - Allocate token budget for decode tokens - If KV allocation fails for a continuing request, preempt requests from the batch tail to free blocks</p> <p>Phase 2: New Requests Dequeue requests from the wait queue: - Compute cached prefix blocks (prefix caching reduces allocation needs) - Allocate KV blocks for uncached prefix tokens being processed this step (bounded by chunked prefill threshold and remaining token budget) - Stop dequeuing when: max batch size reached (<code>--max-num-running-reqs</code>), allocation fails (cache full), token budget exhausted, or a preemption occurred during Phase 1</p>"},{"location":"concepts/core-engine/#constraints","title":"Constraints","text":"Constraint Flag Effect Max batch size <code>--max-num-running-reqs</code> Limits number of concurrent requests in the running batch Token budget <code>--max-num-scheduled-tokens</code> Limits total new tokens across all running requests per step Chunked prefill <code>--long-prefill-token-threshold</code> Splits long prefills across multiple steps"},{"location":"concepts/core-engine/#preemption-strategy","title":"Preemption Strategy","text":"<p>When KV allocation fails for a continuing request: 1. Evict requests from the batch tail (reverse order of admission) 2. Free their KV blocks 3. Re-enqueue evicted requests at the front of the wait queue 4. Retry allocation for the original request 5. Circuit breaker: Stop if allocation still fails after exhausting the batch, or if the request itself is unservable</p>"},{"location":"concepts/core-engine/#kv-cache-management","title":"KV Cache Management","text":"<p>The KV cache simulates GPU memory organized as fixed-size blocks. Each block holds <code>--block-size-in-tokens</code> tokens (default: 16).</p>"},{"location":"concepts/core-engine/#single-tier-cache","title":"Single-Tier Cache","text":"<p>The default KV cache operates entirely in GPU memory:</p> <ul> <li>Block allocation: Requests are allocated blocks proportional to their token count</li> <li>Prefix caching: Hierarchical block hashing enables prefix sharing across requests. Each block's hash chains with the prior block's hash, creating semantic prefix signatures. When a new request shares a prefix with an existing request, the cached blocks are reused rather than reallocated.</li> <li>LRU eviction: Free blocks are managed via a doubly-linked list with LRU ordering</li> <li>Reference counting: Shared blocks (prefix caching) are reference-counted and exempt from eviction while any request references them</li> <li>Transactional allocation: Multi-block allocations are rolled back on failure (no partial allocation)</li> </ul> <p>Conservation invariant (INV-4): <code>allocated_blocks + free_blocks = total_blocks</code> at all times.</p>"},{"location":"concepts/core-engine/#tiered-cache-gpu-cpu","title":"Tiered Cache (GPU + CPU)","text":"<p>When <code>--kv-cpu-blocks</code> is set to a positive value, BLIS enables a two-tier cache:</p> <ul> <li>GPU tier: Full KV cache with prefix caching and LRU eviction</li> <li>CPU tier: Simple capacity store for offloaded blocks</li> <li>Offload trigger: When GPU utilization exceeds <code>--kv-offload-threshold</code> (default: 0.9), blocks are offloaded to CPU</li> <li>Reload: On GPU allocation failure, blocks are reloaded from CPU with a transfer latency penalty</li> <li>Transfer latency: Per reloaded block: <code>base_latency + ceil(block_size_tokens / bandwidth)</code>. Accumulated across all reloaded blocks. Non-blocking (added to step time).</li> <li>Thrashing detection: Blocks offloaded and reloaded within 1000 ticks (1ms) increment a thrashing counter</li> </ul>"},{"location":"concepts/core-engine/#latency-models","title":"Latency Models","text":"<p>BLIS predicts GPU step time through one of two latency model backends. The choice is made automatically based on available configuration.</p>"},{"location":"concepts/core-engine/#blackbox-model-default","title":"Blackbox Model (Default)","text":"<p>Uses trained regression coefficients to predict step time:</p> <pre><code>StepTime    = beta0 + beta1 * cache_miss_tokens + beta2 * decode_tokens\nQueueingTime = alpha0 + alpha1 * input_length\nOutputTokenProcessingTime = alpha2\n</code></pre> <ul> <li>Beta coefficients model GPU execution time as a linear function of batch composition</li> <li>Alpha coefficients model non-GPU overhead (tokenization, API serialization, output processing)</li> <li>Coefficients are trained offline via Bayesian optimization against real vLLM measurements</li> <li>Pre-trained coefficients for common model/GPU combinations are shipped in <code>defaults.yaml</code></li> </ul> <p>See Configuration Reference: Coefficient Calibration for the training process.</p>"},{"location":"concepts/core-engine/#roofline-model-analytical","title":"Roofline Model (Analytical)","text":"<p>Uses analytical FLOPs/bandwidth estimation when no trained coefficients are available:</p> <pre><code>Phase Time = max(total_FLOPs / peak_compute, total_bytes / peak_bandwidth)\nStep Time  = Prefill Phase Time + Decode Phase Time\n</code></pre> <ul> <li>Requires HuggingFace <code>config.json</code> (model architecture: layers, heads, hidden dim)</li> <li>Requires <code>hardware_config.json</code> (GPU specs: peak TFLOPS, peak bandwidth, MFU)</li> <li>Accounts for Tensor Parallelism, All-Reduce latency, and per-layer overheads</li> <li>No training data needed \u2014 works for any supported model immediately</li> </ul> <p>See Roofline Estimation for implementation details.</p>"},{"location":"concepts/core-engine/#alpha-overhead","title":"Alpha Overhead","text":"<p>Alpha overhead models non-GPU processing time: - Queueing time (<code>alpha0 + alpha1 * input_length</code>): Delays request enqueue but does not block the server. The simulation clock is not advanced by this overhead. - Output token processing time (<code>alpha2</code>): Added to per-request ITL/TTFT metrics but does not block the next step.</p> <p>This is architecturally correct for vLLM, where CPU post-processing (tokenization, output serialization) runs concurrently with GPU execution.</p>"},{"location":"concepts/core-engine/#scheduling-policies","title":"Scheduling Policies","text":"<p>Scheduling policies control the order in which queued requests are selected for batch formation. They operate per-instance. To add a new scheduling policy, see Extension Recipes.</p> Policy Ordering Rule Use Case <code>fcfs</code> No reordering (arrival order) Default, fair <code>priority-fcfs</code> Priority descending, then arrival ascending SLO-aware scheduling <code>sjf</code> Input token count ascending, then arrival ascending Shortest-job-first for TTFT optimization <code>reverse-priority</code> Priority ascending (starves high-priority) Pathological testing only"},{"location":"concepts/core-engine/#priority-policies","title":"Priority Policies","text":"<p>Priority policies assign a numeric score to each request before scheduling:</p> Policy Score Computation Behavior <code>constant</code> Fixed score for all requests No differentiation (default) <code>slo-based</code> <code>base + age_weight * (clock - arrival)</code> Favors older requests, prevents starvation. Note: despite the name, does not currently use per-request SLO metadata. <code>inverted-slo</code> <code>base - age_weight * age</code> Starves older requests (pathological)"},{"location":"concepts/core-engine/#metrics","title":"Metrics","text":"<p>BLIS records per-request and aggregate metrics throughout the simulation.</p>"},{"location":"concepts/core-engine/#per-request-metrics","title":"Per-Request Metrics","text":"Metric Definition TTFT Time from arrival to first token: includes queueing delay, prefill step times, and output processing overhead (alpha2) E2E <code>FirstTokenTime + sum(ITLs)</code>, where each ITL includes step time + alpha2 ITL Observed time between consecutive decode steps (includes alpha2 per token) Scheduling Delay Time from request arrival to entering the running batch (includes alpha queueing overhead + wait queue residence)"},{"location":"concepts/core-engine/#aggregate-metrics","title":"Aggregate Metrics","text":"Metric Aggregation TTFT, E2E, ITL distributions Mean, p90, p95, p99 in JSON output. Cluster-internal <code>Distribution</code> type also computes p50, min, max for fitness evaluation. Throughput Output tokens per second, requests per second Preemption count Total KV cache evictions KV allocation failures Failed block allocations Dropped unservable Requests too large for cache"},{"location":"concepts/core-engine/#conservation-invariant-inv-1","title":"Conservation Invariant (INV-1)","text":"<p>At simulation end: <code>injected_requests == completed_requests + still_queued + still_running + dropped_unservable</code></p> <p>This is the fundamental accounting invariant that ensures no requests are silently lost.</p>"},{"location":"concepts/core-engine/#determinism","title":"Determinism","text":"<p>BLIS guarantees deterministic output: the same seed produces byte-identical stdout across runs (INV-6).</p> <p>Key mechanisms: - Partitioned RNG: Each subsystem (workload generation, scheduling, etc.) uses an independent random stream derived from the seed, so adding randomness to one subsystem doesn't perturb others - Sorted map iteration: Where map iteration order affects output, keys are sorted first - Stdout/stderr separation: Deterministic results go to stdout; wall-clock timing and diagnostics go to stderr via logrus</p>"},{"location":"concepts/glossary/","title":"Concepts &amp; Glossary","text":"<p>This page defines terminology used throughout BLIS documentation. Terms are listed alphabetically with cross-references to the relevant design pages.</p>"},{"location":"concepts/glossary/#admission-policy","title":"Admission Policy","text":"<p>A cluster-level gate that decides whether an incoming request enters the routing pipeline or is rejected. Built-in policies: <code>always-admit</code> (accept all), <code>token-bucket</code> (rate-limiting), <code>reject-all</code> (testing only). See Cluster Architecture.</p>"},{"location":"concepts/glossary/#alpha-coefficients","title":"Alpha Coefficients","text":"<p>Three regression coefficients <code>[alpha0, alpha1, alpha2]</code> that model non-GPU overhead per request. <code>alpha0 + alpha1 * input_length</code> estimates queueing delay (tokenization, API serialization); <code>alpha2</code> estimates output token processing time. These overheads are added to per-request metrics but do not block the simulation clock. See Core Engine: Latency Models.</p>"},{"location":"concepts/glossary/#batch-formation","title":"Batch Formation","text":"<p>The process of selecting which requests from the wait queue join the running batch for the next step. BLIS implements vLLM-style continuous batching with chunked prefill and preemption. See Core Engine: Batch Formation.</p>"},{"location":"concepts/glossary/#beta-coefficients","title":"Beta Coefficients","text":"<p>Three regression coefficients <code>[beta0, beta1, beta2]</code> that predict GPU step time: <code>beta0 + beta1 * cache_miss_tokens + beta2 * decode_tokens</code>. Trained offline via Bayesian optimization against real vLLM measurements. See Core Engine: Latency Models.</p>"},{"location":"concepts/glossary/#block-kv-block","title":"Block (KV Block)","text":"<p>The unit of KV cache allocation. Each block holds a fixed number of tokens (default: 16). Requests are allocated blocks proportional to their token count. Blocks are reference-counted and can be shared across requests via prefix caching. See Core Engine: KV Cache.</p>"},{"location":"concepts/glossary/#chunked-prefill","title":"Chunked Prefill","text":"<p>A vLLM optimization where long prefill sequences are split into chunks that fit within the per-step token budget (<code>max-num-scheduled-tokens</code>). Controlled by <code>--long-prefill-token-threshold</code>. See Core Engine: Batch Formation.</p>"},{"location":"concepts/glossary/#continuous-batching","title":"Continuous Batching","text":"<p>The serving strategy where new requests can join the running batch between decode steps, rather than waiting for the entire batch to complete. BLIS models this by re-evaluating the batch composition at every step. See Core Engine: Batch Formation.</p>"},{"location":"concepts/glossary/#counterfactual-regret","title":"Counterfactual Regret","text":"<p>A trace-level metric that measures how much better an alternative routing decision could have been. For each routing decision, BLIS scores all candidate instances and computes <code>regret = best_score - chosen_score</code>. Useful for offline analysis of routing policy quality. See Cluster Architecture: Counterfactual Regret.</p>"},{"location":"concepts/glossary/#decode-phase","title":"Decode Phase","text":"<p>The token generation phase where the model produces output tokens one at a time (or in parallel within a step). Each decode token uses the KV cache from all prior tokens. Decode steps are typically memory-bandwidth-bound. Contrast with Prefill Phase.</p>"},{"location":"concepts/glossary/#discrete-event-simulation-des","title":"Discrete Event Simulation (DES)","text":"<p>A simulation paradigm where the system state changes only at discrete event times. BLIS maintains a priority queue of timestamped events and advances the simulation clock by jumping between events, rather than stepping through fixed time intervals. See Core Engine: Event Queue.</p>"},{"location":"concepts/glossary/#e2e-end-to-end-latency","title":"E2E (End-to-End Latency)","text":"<p>Total time from request arrival to final token completion. Computed as <code>TTFT + sum(ITLs)</code>, where each ITL includes step time plus output processing overhead (alpha2). See Core Engine: Metrics.</p>"},{"location":"concepts/glossary/#effective-load","title":"Effective Load","text":"<p>A routing signal computed as <code>QueueDepth + BatchSize + InFlightRequests</code>. Because <code>InFlightRequests</code> tracks the full dispatch-to-completion window, it overlaps with <code>QueueDepth</code> and <code>BatchSize</code> \u2014 this intentional double-counting combines stale Prometheus signals with the synchronous gateway counter. Used by least-loaded routing and load-balance scoring. See Cluster Architecture: Routing Pipeline.</p>"},{"location":"concepts/glossary/#fitness-score","title":"Fitness Score","text":"<p>A single numeric value summarizing multi-objective simulation performance. Computed as a weighted combination of configurable metrics (TTFT percentiles, E2E percentiles, throughput). Latency metrics normalized via <code>1/(1 + value/1000)</code>; throughput metrics via <code>value/(value + reference)</code>. See Configuration Reference.</p>"},{"location":"concepts/glossary/#horizon","title":"Horizon","text":"<p>The simulation time limit in ticks (microseconds). The simulation stops when the clock exceeds the horizon or all requests complete, whichever comes first. See Configuration Reference.</p>"},{"location":"concepts/glossary/#itl-inter-token-latency","title":"ITL (Inter-Token Latency)","text":"<p>The observed time between consecutive decode steps for a single request. ITL varies with batch composition changes between steps. Mean ITL is reported as TPOT (Time Per Output Token).</p>"},{"location":"concepts/glossary/#kv-cache","title":"KV Cache","text":"<p>GPU memory organized as blocks that store key-value tensors computed during attention. BLIS simulates block allocation, prefix sharing, LRU eviction, and optional CPU offloading without actual GPU memory. See Core Engine: KV Cache.</p>"},{"location":"concepts/glossary/#latency-model","title":"Latency Model","text":"<p>The component that predicts GPU execution time for a batch step. Two modes: Blackbox (trained regression coefficients) and Roofline (analytical FLOPs/bandwidth estimation). See Core Engine: Latency Models and Roofline Estimation.</p>"},{"location":"concepts/glossary/#pending-requests","title":"Pending Requests","text":"<p>Requests that have been routed to an instance but not yet enqueued (the queueing event hasn't fired). Tracked per-instance to prevent routing pile-on at high arrival rates. Decremented when the <code>QueuedEvent</code> fires. See Cluster Architecture: Routing Pipeline.</p>"},{"location":"concepts/glossary/#policy-bundle","title":"Policy Bundle","text":"<p>A YAML configuration file (<code>--policy-config</code>) that specifies admission, routing, priority, and scheduling policies in one place. CLI flags override bundle values when explicitly set. See Configuration Reference.</p>"},{"location":"concepts/glossary/#preemption","title":"Preemption","text":"<p>KV cache eviction under memory pressure. When the batch formation algorithm cannot allocate blocks for a continuing request, it evicts requests from the batch tail, frees their blocks, and re-enqueues them at the front of the wait queue. See Core Engine: Batch Formation.</p>"},{"location":"concepts/glossary/#prefill-phase","title":"Prefill Phase","text":"<p>The initial processing phase where the model computes attention over all input tokens. Prefill is compute-bound for large inputs. After prefill completes, the request transitions to decode. TTFT is recorded at this boundary.</p>"},{"location":"concepts/glossary/#prefix-caching","title":"Prefix Caching","text":"<p>Reuse of KV blocks across requests that share a common input prefix. BLIS uses hierarchical block hashing: each block's hash chains with the prior block's hash, enabling semantic prefix matching. Shared blocks are reference-counted and exempt from eviction while in use. See Core Engine: KV Cache.</p>"},{"location":"concepts/glossary/#prefix-affinity-scoring","title":"Prefix-Affinity Scoring","text":"<p>A routing scorer that directs requests to instances likely to have their prefix cached. Uses a lightweight router-side cache index (not the actual per-instance KV cache) to estimate cache hit probability per instance. Score range [0, 1]. See Cluster Architecture: Scorer Composition.</p>"},{"location":"concepts/glossary/#priority-policy","title":"Priority Policy","text":"<p>A per-instance policy that assigns a numeric priority score to each request before batch formation. Used by priority-aware schedulers to reorder the wait queue. Built-in policies: <code>constant</code>, <code>slo-based</code>, <code>inverted-slo</code> (testing only). Note: despite its name, <code>slo-based</code> currently uses only request age (favoring older requests), not per-request SLO metadata. See Core Engine: Scheduling.</p>"},{"location":"concepts/glossary/#roofline-model","title":"Roofline Model","text":"<p>An analytical latency estimation technique that predicts step time as <code>max(FLOPs / peak_compute, bytes / peak_bandwidth)</code>. Requires only the model's HuggingFace <code>config.json</code> and GPU hardware specs. No training data needed. See Roofline Estimation.</p>"},{"location":"concepts/glossary/#routing-policy","title":"Routing Policy","text":"<p>A cluster-level policy that selects which instance receives an admitted request. Simple policies (round-robin, least-loaded) use fixed rules. The weighted scoring policy composes multiple scorers with configurable weights. See Cluster Architecture: Routing Pipeline.</p>"},{"location":"concepts/glossary/#routing-snapshot","title":"Routing Snapshot","text":"<p>A point-in-time view of instance state used for routing decisions. Contains queue depth, batch size, KV utilization, cache hit rate, and pending request count. Signals have different freshness tiers depending on how they're collected. See Cluster Architecture: Signal Freshness.</p>"},{"location":"concepts/glossary/#scorer","title":"Scorer","text":"<p>A component in the weighted scoring pipeline that produces a per-instance score in [0, 1] for a specific signal dimension. Built-in scorers: <code>prefix-affinity</code>, <code>queue-depth</code>, <code>kv-utilization</code>, <code>load-balance</code>. Scores are multiplied by weights and summed. See Cluster Architecture: Scorer Composition.</p>"},{"location":"concepts/glossary/#seed","title":"Seed","text":"<p>The random seed for deterministic simulation. Same seed produces byte-identical stdout across runs (INV-6). BLIS uses partitioned RNG to isolate randomness across subsystems. See Configuration Reference.</p>"},{"location":"concepts/glossary/#step","title":"Step","text":"<p>A single iteration of the inference engine. Each step processes one batch: prefill tokens for new/continuing requests and decode tokens for generating output. Step time is predicted by the latency model. See Core Engine: Step Phases.</p>"},{"location":"concepts/glossary/#tick","title":"Tick","text":"<p>The fundamental time unit in BLIS, representing one microsecond. All timestamps, durations, and latencies are measured in ticks. The simulation clock advances in ticks.</p>"},{"location":"concepts/glossary/#tiered-kv-cache","title":"Tiered KV Cache","text":"<p>An extension of the KV cache with GPU and CPU tiers. When GPU utilization exceeds a threshold, blocks are offloaded to CPU memory. On cache miss, blocks can be reloaded from CPU with a transfer latency penalty. See Core Engine: KV Cache.</p>"},{"location":"concepts/glossary/#ttft-time-to-first-token","title":"TTFT (Time To First Token)","text":"<p>Time from request arrival to completion of the prefill phase (first output token ready). Includes queueing delay, prefill step times, and output processing overhead (alpha2). A key latency SLO metric for interactive applications. See Core Engine: Metrics.</p>"},{"location":"concepts/glossary/#workload-specification","title":"Workload Specification","text":"<p>A YAML file (<code>--workload-spec</code>) defining multi-client workloads with per-client arrival distributions, token length distributions, prefix groups, and SLO classes. Supports <code>poisson</code>, <code>gamma</code>, <code>weibull</code>, and <code>constant</code> arrival processes and <code>gaussian</code>, <code>exponential</code>, <code>pareto_lognormal</code>, <code>constant</code>, and <code>empirical</code> token distributions. See Configuration Reference.</p>"},{"location":"concepts/glossary/#work-conserving","title":"Work-Conserving","text":"<p>The property that the simulator never idles while requests are waiting. After every step completion, if the wait queue is non-empty, a new <code>StepEvent</code> is scheduled immediately (INV-8). See Core Engine: Event Queue.</p>"},{"location":"concepts/roofline/","title":"Roofline Step Time Estimation Logic","text":"<p>This document describes the analytical approach used to estimate the GPU latency for a single inference step using a roofline model. This requires no training, and works off-the-shelf for any Huggingface LLM whose <code>config.json</code> is saved under <code>model_configs/</code>, except Mixture-of-Expert (MoE) models currently.</p>"},{"location":"concepts/roofline/#1-why-roofline","title":"1. Why Roofline?","text":"<p>The Blackbox optimization approach outlined in Configuration: Coefficient Calibration requires training over ground-truth workload metrics obtained by running live vLLM instances over GPUs. In practice, collecting this ground-truth data and training the GPU latency model for each LLM can take several hours. For a faster approximation of GPU latencies without actually ever running vLLM, we recommend the roofline approach. This technique only requires the Huggingface LLM config (<code>config.json</code>) and hardware specifications (e.g. GPU Peak TFLOPS/Peak BW from NVIDIA datasheets) in <code>hardware_config.json</code>. It allows BLIS to generalize across diverse LLMs, TP values and workloads, while preserving TTFT/ITL/E2E accuracy.</p>"},{"location":"concepts/roofline/#2-core-methodology-the-roofline-model","title":"2. Core Methodology: The Roofline Model","text":"<p>The simulator predicts execution time by identifying whether a phase (prefill/decode) is Compute-Bound or Memory-Bound. </p> <p>For each phase:</p> <p>$$\\text{Phase Time} = \\max\\left( \\frac{\\text{Total FLOPS}}{\\text{Peak Performance}}, \\frac{\\text{Total Bytes Transferred}}{\\text{Memory Bandwidth}} \\right)$$</p> <p>As a result, overall step time is given by:</p> <p>$$\\text{Step Time} = \\text{Prefill Phase Time} + \\text{Decode Phase Time}$$</p>"},{"location":"concepts/roofline/#3-calculation-phases","title":"3. Calculation Phases","text":""},{"location":"concepts/roofline/#a-flops-calculation-calculatetransformerflops","title":"A. FLOPs Calculation (<code>calculateTransformerFlops</code>)","text":"<p>We track two types of operations: * GEMM Ops: Matrix multiplications for QKV projections, Attention Output, and MLP (SwiGLU) layers. Includes $QK^T$ and $AV$ operations. * Vector Ops: Non-tensor core operations like Softmax, RoPE (rotary embeddings), and normalization.</p>"},{"location":"concepts/roofline/#b-memory-access-calculatememoryaccessbytes","title":"B. Memory Access (<code>calculateMemoryAccessBytes</code>)","text":"<p>We calculate the data movement between HBM (High Bandwidth Memory) and the processor: * Weights: Static model parameters loaded once per layer. * KV Cache Growth: Writing new Key/Value tokens to memory. * KV Cache Access: Reading the history (past tokens) for attention.</p>"},{"location":"concepts/roofline/#3-execution-pipeline","title":"3. Execution Pipeline","text":"<p>The final step time is the sum of independent phases and overheads:</p> <ol> <li>Prefill Phase: Calculated for the initial prompt processing chunk.</li> <li>Decode Phase: Calculated for generating new tokens (usually memory-bound).</li> <li>Communication Overhead: If using Tensor Parallelism ($TP &gt; 1$), adds All-Reduce latency per layer.</li> <li>Hardware Overheads: Static kernel launch times and layer-by-layer overhead constants.</li> </ol>"},{"location":"concepts/roofline/#4-key-performance-variables","title":"4. Key Performance Variables","text":"<ul> <li>MFU (Model Flops Utilization): Scaled TFLOPs efficiency factors for Prefill vs. Decode.</li> <li>TP Factor: Divides compute and memory load across multiple GPUs.</li> <li>Bandwidth Efficiency: Real-world effective bandwidth versus theoretical peak bandwidth.</li> </ul>"},{"location":"concepts/roofline/#5-onboarding-a-new-llmgpu","title":"5. Onboarding a new LLM/GPU","text":""},{"location":"concepts/roofline/#automatic-recommended-roofline-flag","title":"Automatic (recommended): <code>--roofline</code> flag","text":"<p>The simplest way to run roofline mode is with the <code>--roofline</code> flag, which auto-resolves the model config:</p> <pre><code>./blis run --model meta-llama/llama-3.1-8b-instruct --roofline --hardware H100 --tp 1\n</code></pre> <p>The flag automatically: 1. Checks <code>model_configs/</code> for an existing <code>config.json</code> (previously fetched) 2. Fetches from HuggingFace on miss and writes into <code>model_configs/</code> (supports <code>HF_TOKEN</code> for gated models)</p> <p>For models not in <code>defaults.yaml</code>, add an <code>hf_repo</code> entry mapping the BLIS model name to the case-sensitive HuggingFace repo path.</p>"},{"location":"concepts/roofline/#manual-explicit-config-paths","title":"Manual: explicit config paths","text":"<p>Alternatively, download the <code>config.json</code> manually:</p> <ul> <li>Download the <code>config.json</code> for the LLM of your choice into <code>model_configs/</code>. This is an example config.json for <code>meta-llama/Llama-3.3-70B-Instruct</code>. The recommended file structure is <code>model_configs/llama-3.1-8b-instruct/config.json</code>.</li> </ul>"},{"location":"concepts/roofline/#adding-a-new-gpu","title":"Adding a new GPU","text":"<ul> <li>Refer to NVIDIA datasheets for GPU specs (for example, datasheet for H100) and add an entry to <code>hardware_config.json</code>:</li> </ul> <pre><code>{\n    \"&lt;GPU_name&gt;\": {\n        \"TFlopsPeak\":        989.5,\n        \"BwPeakTBs\":         3.35,\n        \"BwEffConstant\":     0.72,\n        \"TOverheadMicros\":   500.0,\n        \"perLayerOverhead\":  20.0,\n        \"mfuPrefill\":        0.65,\n        \"mfuDecode\":         0.12,\n        \"allReduceLatency\":  20.0,\n        \"MemoryGiB\":         80.0\n    }\n}\n</code></pre> Field Description <code>TFlopsPeak</code> Peak BF16 TFLOPS from GPU datasheet <code>BwPeakTBs</code> Peak HBM bandwidth in TB/s from GPU datasheet <code>BwEffConstant</code> Fraction of peak BW achieved in practice (0-1) <code>TOverheadMicros</code> Per-step overhead in microseconds <code>perLayerOverhead</code> CPU scheduling overhead per transformer layer in microseconds <code>mfuPrefill</code> Static MFU for prefill (used when MFU database is unavailable) <code>mfuDecode</code> Static MFU for decode (used when MFU database is unavailable) <code>allReduceLatency</code> All-reduce latency in microseconds (multi-GPU TP) <code>MemoryGiB</code> GPU memory capacity in GiB (reserved for future KV capacity auto-calculation) <p>Note: The Peak TFLOPS and BW for a given GPU family might vary by GPU connectivity (e.g. SXM vs PCIe). We recommend a separate entry for each GPU connectivity type - e.g. A100-SXM, A100-PCIe etc in <code>hardware_config.json</code>.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Welcome to the BLIS contributor guide. This section covers engineering standards, development workflows, and extension recipes for building on BLIS.</p>"},{"location":"contributing/#quick-start","title":"Quick Start","text":"<pre><code># Build\ngo build -o blis main.go\n\n# Test\ngo test ./...\n\n# Lint (install once: go install github.com/golangci/golangci-lint/v2/cmd/golangci-lint@v2.9.0)\ngolangci-lint run ./...\n</code></pre> <p>All three must pass before submitting a PR. CI runs on every PR (see <code>.github/workflows/ci.yml</code>).</p>"},{"location":"contributing/#your-first-contribution","title":"Your First Contribution","text":"<p>See CONTRIBUTING.md for a step-by-step walkthrough that adds a trivial admission policy \u2014 the lightest extension type (~3 files).</p>"},{"location":"contributing/#development-workflows","title":"Development Workflows","text":"Workflow When to Use PR Workflow Every PR: worktree \u2192 plan \u2192 review \u2192 implement \u2192 audit \u2192 commit Design Process New features that introduce module boundaries Macro Planning Multi-PR features requiring decomposition Hypothesis Experiments Rigorous experiments to validate simulator behavior Convergence Protocol Review gate used by all workflows above"},{"location":"contributing/#extension-recipes","title":"Extension Recipes","text":"<p>Extension Recipes \u2014 Step-by-step guides for adding policies, scorers, KV tiers, trace records, and per-request metrics.</p>"},{"location":"contributing/#standards","title":"Standards","text":"Document Covers Antipattern Rules (R1-R20) 20 rules, each tracing to a real bug System Invariants (INV-1-8) Properties that must always hold Engineering Principles Separation of concerns, interface design, BDD/TDD Experiment Standards Hypothesis families, rigor requirements"},{"location":"contributing/#templates","title":"Templates","text":"Template Purpose Agent Prompt Design Guidelines DES foundations, module architecture, extension framework N/A (reference material) Macro Plan Multi-PR feature decomposition <code>macro-plan-prompt.md</code> Micro Plan Single-PR implementation with TDD tasks <code>micro-plan-prompt.md</code> Hypothesis Experiment FINDINGS.md structure N/A (template is audience-neutral) <p>Templates describe the output format (what sections to include and why). Agent prompts contain LLM-specific instructions for generating those artifacts. Claude Code skills reference the prompt files automatically.</p>"},{"location":"contributing/convergence/","title":"Universal Convergence Protocol","text":"<p>Status: Active (v1.0 \u2014 extracted 2026-02-26 from hypothesis.md)</p> <p>This document defines the convergence protocol used by all review gates across BLIS workflows: - PR workflow (docs/contributing/pr-workflow.md): Plan Review (10 perspectives), Code Review (10 perspectives) - Hypothesis workflow (docs/contributing/hypothesis.md): Design Review (5 perspectives), Code Review (5 perspectives), FINDINGS Review (10 perspectives) - Design process (docs/contributing/design-process.md): Design Doc Review (8 perspectives) - Macro-plan process (docs/contributing/macro-planning.md): Macro Plan Review (8 perspectives)</p> <p>Executable implementation: The <code>convergence-review</code> skill automates this protocol \u2014 dispatching perspectives, tallying findings independently, and enforcing the re-run gate. Invoke with <code>/convergence-review &lt;gate-type&gt; [artifact-path] [--model opus|sonnet|haiku]</code> (default: <code>haiku</code>).</p>"},{"location":"contributing/convergence/#the-protocol","title":"The Protocol","text":"<ol> <li>Run all N perspectives in parallel (one round)</li> <li>Collect all findings; each classified as CRITICAL / IMPORTANT / SUGGESTION</li> <li>If zero CRITICAL and zero IMPORTANT across all N reviewers: Converged \u2014 proceed to next step</li> <li>If any CRITICAL or IMPORTANT from any reviewer: Fix all issues, return to step 1 (re-run entire round)</li> <li>Repeat until convergence</li> </ol>"},{"location":"contributing/convergence/#rules","title":"Rules","text":"<ul> <li>Max 10 rounds per gate. Each gate has its own independent round counter. If a gate fails to converge within 10 rounds, suspend the work: document the remaining issues as future work.</li> <li>No minimum round count. Convergence in Round 1 is valid if no reviewer flags any CRITICAL or IMPORTANT item.</li> <li>Hard gate \u2014 NO EXCEPTIONS. You MUST re-run after fixes. You may NOT skip the re-run, propose alternative steps, or rationalize that fixes were \"trivial enough.\" The re-run is the only evidence of convergence. This is non-negotiable.</li> <li>SUGGESTION-level items (documentation nits, cosmetic fixes, off-by-one line citations) do not block convergence.</li> </ul>"},{"location":"contributing/convergence/#severity-levels","title":"Severity Levels","text":"<p>Each reviewer must classify every finding:</p> <ul> <li>CRITICAL: Must fix before proceeding. Examples: missing control experiment (RCV-4), status classification contradicted by data, silent data loss in analyzer, cross-document contradiction.</li> <li>IMPORTANT: Should fix before proceeding. The key test: would proceeding with this unfixed item mislead a reader or produce incorrect conclusions? Examples: sub-threshold effect size in one seed, stale text contradicting current results, undocumented confound.</li> <li>SUGGESTION: Does not affect correctness or reader understanding. Examples: off-by-one line citation (\u00b12 lines), cosmetic terminology, style consistency.</li> </ul> <p>When in doubt between IMPORTANT and SUGGESTION: If fixing the item would change any conclusion, metric, or user guidance, it is IMPORTANT. If it would only improve readability without changing any conclusion, it is SUGGESTION. If multiple reviewers classify the same item at different severities, the highest severity applies.</p>"},{"location":"contributing/convergence/#agent-failure-handling","title":"Agent Failure Handling","text":"<ul> <li>Timeout: 5 minutes per reviewer agent. If an agent exceeds this, check its output file and restart if stalled.</li> <li>Failure: If a reviewer agent fails or hangs, fall back to performing that review directly (read the artifact yourself with that reviewer's checklist). Do not skip a reviewer perspective.</li> <li>External contributors: Submit your artifacts via PR. Maintainers will run the review protocol on your behalf.</li> </ul>"},{"location":"contributing/convergence/#expected-convergence-rates","title":"Expected Convergence Rates","text":"<p>Gates with more perspectives (FINDINGS Review: 10) will naturally converge more slowly than gates with fewer (Design Review: 5). This is correct behavior \u2014 more eyes = higher quality bar. Typical expectations: - Hypothesis Design Review (5 perspectives): 1-2 rounds (empirical, from PR #310-#433) - Hypothesis Code Review (5 perspectives): 1-3 rounds (empirical) - Design Doc Review (8 perspectives): 1-2 rounds (estimated \u2014 no empirical data yet) - Macro Plan Review (8 perspectives): 1-2 rounds (estimated) - PR Plan/Code Review (10 perspectives): 1-3 rounds (empirical, from PR #381-#433) - FINDINGS Review (10 perspectives): 1-5 rounds (empirical)</p>"},{"location":"contributing/convergence/#references","title":"References","text":"<ul> <li>PR workflow: docs/contributing/pr-workflow.md</li> <li>Hypothesis workflow: docs/contributing/hypothesis.md</li> <li>Design process: docs/contributing/design-process.md</li> <li>Macro-plan process: docs/contributing/macro-planning.md</li> </ul>"},{"location":"contributing/design-process/","title":"Design Process","text":"<p>Status: Active (v1.0 \u2014 updated 2026-02-26)</p> <p>This document describes the process for writing a BLIS design document. For the design document template itself, see docs/contributing/templates/design-guidelines.md.</p>"},{"location":"contributing/design-process/#when-a-design-doc-is-needed","title":"When a Design Doc is Needed","text":"<ul> <li>New subsystem modules (new interface + integration)</li> <li>Backend swaps (alternative implementations requiring interface extraction)</li> <li>Architecture changes affecting module boundaries</li> </ul> <p>Not needed for: Bug fixes, new policy templates behind existing interfaces, documentation changes.</p>"},{"location":"contributing/design-process/#steps","title":"Steps","text":"<ol> <li>Identify the extension type \u2014 policy template, subsystem module, backend swap, or tier composition (see design guidelines Section 5)</li> <li>Choose the design doc species \u2014 decision record, specification, problem analysis, or system overview (Section 3.2)</li> <li>Complete the DES checklist (Section 2.6) \u2014 model scoping, event design, state/statistics, V&amp;V, randomness</li> <li>Write the design doc per the template's required sections (Section 3.3): motivation, scope, modeling decisions, invariants, decisions with trade-offs, extension points, validation strategy</li> <li>Apply the staleness test (Section 3.1) \u2014 would this content mislead if the implementation changes?</li> <li>Convergence review \u2014 Run all 8 perspectives in parallel (or sequentially), applying the convergence protocol. Fix CRITICAL and IMPORTANT findings, re-run until convergence.</li> <li>Human review \u2014 approve before macro/micro planning begins</li> </ol>"},{"location":"contributing/design-process/#design-review-perspectives-8","title":"Design Review Perspectives (8)","text":"<p>For each perspective, check every item. Classify findings as CRITICAL / IMPORTANT / SUGGESTION per the convergence protocol. Section references below refer to design-guidelines.md unless otherwise noted.</p> <p>Perspective 1 \u2014 Motivation &amp; Scoping: - Are the analysis questions clear and specific? - Is the modeling decisions table complete (modeled / simplified / omitted)? - Does every \"simplified\" entry state what real-system behavior is lost? - Has each component been evaluated against the six model scoping criteria (design-guidelines Section 2.1)?</p> <p>Perspective 2 \u2014 DES Foundations: - Is the DES design review checklist (Section 2.6) completed with all 10 questions answered? - Are new events classified as exogenous or endogenous? - Do new events specify priority constants for tie-breaking? - Is state/statistics separation maintained (Section 2.3)? - Are new randomness sources declared with PartitionedRNG subsystem names?</p> <p>Perspective 3 \u2014 Module Contract Completeness: - Does every new or modified module have all 6 contract aspects (observes, controls, owns, invariants, events, extension friction)? - Are invariants named (INV-N) and cross-referenced with existing invariants? - Is the extension friction count specified and within reference targets (Section 4.5)?</p> <p>Perspective 4 \u2014 Extension Framework Fit: - Is the extension type correctly identified (policy template / subsystem module / backend swap / tier composition)? - Is the correct recipe from Section 5 followed? - Is the no-op default specified (existing behavior unchanged when extension not configured)? - Is parallel development path described?</p> <p>Perspective 5 \u2014 Prohibited Content: - Any Go struct definitions with field lists? (Prohibited \u2014 Section 3.4) - Any method implementations? (Prohibited) - Any file paths with line numbers? (Prohibited) - Any interface signatures in Go syntax for pre-freeze interfaces? (Prohibited)</p> <p>Perspective 6 \u2014 Trade-off Quality: - Does every non-obvious decision have alternatives listed with rationale? - For each decision: what breaks if it's wrong? - Is there a Decision Status column (Proposed / Implemented / Superseded)?</p> <p>Perspective 7 \u2014 Validation Strategy: - How will correctness be verified? (Which invariants?) - How will fidelity be validated? (Against what real-system data?) - Are both verification and validation addressed (not just one)?</p> <p>Perspective 8 \u2014 Staleness Resistance: - Apply the staleness test (Section 3.1) to every section - Would any content mislead if the implementation changes during micro-planning? - Is content described behaviorally (what crosses a boundary and why) rather than structurally (how the boundary is implemented)?</p>"},{"location":"contributing/design-process/#quality-gates","title":"Quality Gates","text":"<ul> <li> Extension type identified and correct recipe followed</li> <li> DES checklist from Section 2.6 completed</li> <li> No prohibited content (Section 3.4): no Go structs, no method implementations, no file:line references</li> <li> Every non-obvious decision has alternatives listed with rationale</li> <li> Validation strategy specified (which invariants? against what real-system data?)</li> </ul> <p>Automation</p> <p><code>/convergence-review design &lt;design-doc-path&gt;</code> dispatches all 8 perspectives and enforces convergence automatically. See Skills &amp; Plugins.</p>"},{"location":"contributing/design-process/#references","title":"References","text":"<ul> <li>Template: docs/contributing/templates/design-guidelines.md</li> <li>Convergence protocol: docs/contributing/convergence.md</li> <li>Standards: docs/contributing/standards/rules.md, docs/contributing/standards/invariants.md</li> </ul>"},{"location":"contributing/extension-recipes/","title":"BLIS Extension Recipes","text":"<p>Step-by-step guides for extending BLIS. Each recipe lists the exact files to touch, the order, and examples to follow.</p>"},{"location":"contributing/extension-recipes/#adding-new-policy-templates","title":"Adding New Policy Templates","text":"<p>To add a new policy template (e.g., a new routing algorithm):</p> <ol> <li>Implement the interface in the corresponding file:</li> <li><code>AdmissionPolicy</code> \u2192 <code>sim/admission.go</code> (cluster-level: receives <code>*RouterState</code> with snapshots + clock)</li> <li><code>RoutingPolicy</code> \u2192 <code>sim/routing.go</code> (cluster-level: receives <code>*RouterState</code> with snapshots + clock)</li> <li><code>PriorityPolicy</code> \u2192 <code>sim/priority.go</code> (instance-level: receives <code>req</code> + <code>clock</code> only)</li> <li><code>InstanceScheduler</code> \u2192 <code>sim/scheduler.go</code> (instance-level: receives <code>requests</code> + <code>clock</code> only)</li> <li> <p>Note: <code>RouterState</code> is a bridge type in <code>sim/</code> to avoid import cycles \u2014 see <code>sim/router_state.go</code></p> </li> <li> <p>Register in two places (both required):</p> </li> <li>Add policy name to valid names map in <code>sim/bundle.go</code> (e.g., <code>validRoutingPolicies</code>) and corresponding <code>IsValid*</code> function</li> <li>Add <code>case</code> to factory function in the same policy file (e.g., <code>NewRoutingPolicy</code> in <code>sim/routing.go</code>)</li> <li> <p>CLI error messages auto-derive from <code>ValidAdmissionPolicyNames()</code> etc. \u2014 no manual update needed</p> </li> <li> <p>Add tests following BDD naming: <code>TestMyPolicy_Scenario_Behavior</code></p> </li> <li>Test observable behavior, not internal structure</li> <li>Include empty-snapshots panic test for routing policies (defensive programming convention)</li> <li> <p>Use <code>&amp;RouterState{Snapshots: snapshots, Clock: clock}</code> in test setup</p> </li> <li> <p>Update documentation: CLAUDE.md file organization, README policy lists</p> </li> </ol> <p>Important: For load-based routing, use <code>snap.EffectiveLoad()</code> \u2014 never compute <code>QueueDepth + BatchSize + InFlightRequests</code> inline. This ensures all routing policies use the same formula.</p> <p>Examples: - See <code>RejectAll</code> in <code>sim/admission.go</code> for a simple admission template (constant return) - See <code>newPrefixAffinityScorer</code> in <code>sim/routing_prefix_scorer.go</code> for a stateful scorer with observer-based state updates (the prefix-affinity scorer uses a router-side <code>PrefixCacheIndex</code> to track per-instance block hash history)</p>"},{"location":"contributing/extension-recipes/#adding-new-scorers-weighted-routing","title":"Adding New Scorers (Weighted Routing)","text":"<p>To add a new scoring dimension for the <code>weighted</code> routing policy (e.g., predicted-latency):</p> <ol> <li>Implement the scorer function in <code>sim/routing_scorers.go</code> (stateless) or a new file (stateful) \u2014 a <code>scorerFunc</code> that takes <code>(*Request, []RoutingSnapshot)</code> and returns <code>map[string]float64</code> with scores in [0,1] per instance. Stateful scorers also return an <code>observerFunc</code> called after each routing decision.</li> <li>Register the scorer in <code>sim/routing_scorers.go</code>: add to <code>validScorerNames</code> map + <code>newScorerWithObserver</code> factory switch</li> <li>Add behavioral tests \u2014 monotonicity, boundary values, INV-1/INV-2 conformance</li> <li>Extension friction: 2 touch points (implementation + registration in <code>newScorerWithObserver</code>). Stateful scorers (like prefix-affinity) may use a separate file (e.g., <code>sim/routing_prefix_scorer.go</code>) but the registration point is the same <code>newScorerWithObserver</code> switch in <code>sim/routing_scorers.go</code>.</li> <li>Stateful scorers return an <code>observerFunc</code> alongside the <code>scorerFunc</code> from <code>newScorerWithObserver</code>. The <code>observerFunc</code> signature is <code>func(req *Request, targetInstance string)</code> and is called after each routing decision to update scorer state. The scorer and observer share state via closure.</li> </ol> <p>Examples: - See <code>scoreLoadBalance</code> in <code>sim/routing_scorers.go</code> for a simple stateless scorer - See <code>scoreQueueDepth</code> for a scorer with edge case handling (uniform load) - See <code>newPrefixAffinityScorer</code> in <code>sim/routing_prefix_scorer.go</code> for a stateful scorer with observer and router-side cache</p>"},{"location":"contributing/extension-recipes/#extending-kv-cache-tiers","title":"Extending KV Cache Tiers","text":"<p>To add a new KV tier (e.g., NVMe offloading for 3-tier GPU+CPU+NVMe):</p> <ol> <li>Implement the <code>KVStore</code> interface in <code>sim/kv/</code> (11 methods: allocate, get cached, release, capacity queries, metrics, <code>SetClock</code>, <code>ConsumePendingTransferLatency</code>)</li> <li>Compose existing tiers \u2014 e.g., wrap <code>TieredKVCache</code> (GPU+CPU) with NVMe logic, following the same delegation pattern</li> <li>Update <code>NewKVStore</code> factory in <code>sim/kv/register.go</code> to instantiate your tier based on <code>KVCacheConfig</code> fields (add new fields to <code>KVCacheConfig</code> in <code>sim/config.go</code>)</li> <li>Add CLI flags in <code>cmd/root.go</code> for new parameters (e.g., <code>--kv-nvme-blocks</code>) and wire them into the <code>KVCacheConfig</code> sub-config</li> <li>Aggregate metrics \u2014 combine hit/miss/thrashing counters from all tiers; see <code>TieredKVCache.CacheHitRate()</code> for the 2-tier pattern</li> <li>Add behavioral tests in <code>sim/kv/*_test.go</code></li> <li>Preserve rollback semantics \u2014 <code>KVCacheState.AllocateKVBlocks</code> is transactional: on mid-loop failure, <code>rollbackAllocation()</code> undoes all mutations (UsedBlockCnt, CacheMisses, CacheHits, RefCount, InUse, free list, HashToBlock, RequestMap). If your tier adds mutations beyond what delegation to <code>gpu.AllocateKVBlocks()</code> handles, you must roll those back too. See <code>cachedBlockMutation</code> and <code>newBlockMutation</code> types in <code>sim/kv/cache.go</code>.</li> <li><code>GetCachedBlocks</code> is a pure query \u2014 it returns cached block IDs without side effects. <code>CacheHits</code> are counted by <code>AllocateKVBlocks</code> when cached blocks are committed to an allocation (and rolled back on failure). This was fixed in the Phase 3 hardening PR; the previous implementation incremented CacheHits in GetCachedBlocks, causing double-counting in tiered mode.</li> </ol> <p>Examples: - See <code>TieredKVCache</code> in <code>sim/kv/tiered.go</code> for 2-tier GPU+CPU composition - See <code>KVCacheState</code> in <code>sim/kv/cache.go</code> for single-tier baseline (also implements <code>KVStore</code>) - See <code>docs/plans/archive/pr12-architectural-predesign.md</code> for the design decisions behind the tiered architecture</p>"},{"location":"contributing/extension-recipes/#adding-new-trace-record-types","title":"Adding New Trace Record Types","text":"<p>To add a new trace record type (e.g., <code>ScaleRecord</code> for autoscaling events):</p> <ol> <li>Define the record struct in <code>sim/trace/record.go</code> (pure data, no <code>sim/</code> dependency)</li> <li>Add a slice field to <code>SimulationTrace</code> in <code>sim/trace/trace.go</code> (e.g., <code>Scales []ScaleRecord</code>)</li> <li>Add a recording method to <code>SimulationTrace</code> (e.g., <code>RecordScale(ScaleRecord)</code>)</li> <li>Hook recording into the cluster event pipeline in <code>sim/cluster/cluster_event.go</code> (guard with <code>if cs.trace != nil</code> for zero-overhead default)</li> <li>Update <code>Summarize()</code> in <code>sim/trace/summary.go</code> to aggregate the new record type</li> <li>Add behavioral tests in <code>sim/trace/*_test.go</code></li> </ol> <p>Examples: - See <code>AdmissionRecord</code> in <code>sim/trace/record.go</code> for a simple record - See <code>RoutingRecord</code> with <code>CandidateScore</code> for a record with nested counterfactual data - See <code>computeCounterfactual()</code> in <code>sim/cluster/counterfactual.go</code> for derived computation that lives in <code>sim/cluster/</code> (not <code>sim/trace/</code>) because it needs <code>sim.RoutingSnapshot</code></p>"},{"location":"contributing/extension-recipes/#adding-new-latency-model-backends","title":"Adding New Latency Model Backends","text":"<p>To add a new latency estimation backend (e.g., SGLang RadixAttention, TensorRT-LLM, neural surrogate):</p> <ol> <li>Implement the <code>LatencyModel</code> interface in <code>sim/latency/latency.go</code> (or a new file in <code>sim/latency/</code> for complex models) \u2014 5 methods:</li> <li><code>StepTime(batch []*Request) int64</code> \u2014 estimate batch step duration from request states</li> <li><code>QueueingTime(req *Request) int64</code> \u2014 estimate arrival-to-queue delay</li> <li><code>OutputTokenProcessingTime() int64</code> \u2014 per-token post-processing overhead</li> <li><code>SchedulingProcessingTime() int64</code> \u2014 scheduling overhead per request</li> <li><code>PreemptionProcessingTime() int64</code> \u2014 preemption overhead per eviction</li> <li>Register in <code>NewLatencyModel</code> factory in <code>sim/latency/latency.go</code>: add a branch based on <code>ModelHardwareConfig</code> fields (e.g., a new string field or boolean in <code>sim/config.go</code>). The factory signature is <code>NewLatencyModel(LatencyCoeffs, ModelHardwareConfig)</code>.</li> <li>Add behavioral tests in <code>sim/latency/</code> \u2014 monotonicity (more tokens \u2192 longer step time), positive output, boundary cases (empty batch)</li> <li>Extension friction: 2 touch points (implementation + factory branch)</li> </ol> <p>Examples: - See <code>BlackboxLatencyModel</code> in <code>sim/latency/latency.go</code> for a simple stateless model (alpha/beta regression) - See <code>RooflineLatencyModel</code> in <code>sim/latency/latency.go</code> for a model that uses hardware config (FLOPs/bandwidth)</p>"},{"location":"contributing/extension-recipes/#adding-new-batch-formation-strategies","title":"Adding New Batch Formation Strategies","text":"<p>To add a new batch formation strategy (e.g., disaggregated prefill/decode, speculative decoding, continuous batching without preemption):</p> <ol> <li>Implement the <code>BatchFormation</code> interface in <code>sim/batch_formation.go</code> (or a new file for complex strategies) \u2014 1 method:</li> <li><code>FormBatch(ctx BatchContext) BatchResult</code> \u2014 compose the running batch for the next step</li> <li>The implementation receives <code>BatchContext</code> with: RunningBatch, WaitQ, KVCache, token budget, batch size limit, chunked prefill threshold, simulation time, step count, and ComputedTokens map</li> <li>The implementation MUST update <code>ctx.ComputedTokens[req.ID]</code> for each request that receives new tokens (Phase 2 of <code>Step()</code> reads this map to advance <code>ProgressIndex</code>)</li> <li>The implementation may mutate <code>WaitQ</code> (dequeue/prepend) and <code>KVCache</code> (allocate/release) during batch formation</li> <li>The implementation MUST NOT schedule events or record metrics \u2014 return decisions in <code>BatchResult</code>, the Simulator applies them</li> <li>Register in <code>NewBatchFormation</code> factory in <code>sim/batch_formation.go</code>: add a selection branch. The factory signature is <code>NewBatchFormation(LatencyModel)</code> \u2014 a future PR will add a strategy selection parameter (e.g., a string field in <code>PolicyConfig</code> or <code>BatchConfig</code>)</li> <li>Add behavioral tests \u2014 token budget enforcement, batch size limits, KV conservation, preemption behavior (if applicable), FCFS ordering</li> <li>Extension friction: 2 touch points (implementation + factory registration)</li> </ol> <p>Note: Currently only <code>VLLMBatchFormation</code> exists. Adding a second strategy will also require: (a) a <code>BatchFormation string</code> field in <code>PolicyConfig</code> or <code>BatchConfig</code> (in <code>sim/config.go</code>), (b) a CLI flag in <code>cmd/root.go</code>, (c) validation in <code>sim/bundle.go</code>, (d) selection logic in <code>NewBatchFormation</code>.</p> <p>Examples: - See <code>VLLMBatchFormation</code> in <code>sim/batch_formation.go</code> for the vLLM FCFS + chunked-prefill + preemption strategy - See <code>preemptForTokens</code> for the KV allocation + eviction loop pattern</p>"},{"location":"contributing/extension-recipes/#adding-new-per-request-metric-fields","title":"Adding New Per-Request Metric Fields","text":"<p>To add a new field to per-request JSON output (appears in <code>--results-path</code> output):</p> <ol> <li>Add field to <code>Request</code> in <code>sim/request.go</code> (runtime state, zero-value safe). When constructing <code>Request</code> structs, use <code>RequestState</code> typed constants (<code>StateQueued</code>, <code>StateRunning</code>, <code>StateCompleted</code>) \u2014 never bare strings.</li> <li>Add field to <code>RequestMetrics</code> in <code>sim/metrics_utils.go</code> (JSON output struct, use <code>omitempty</code> for backward compatibility)</li> <li>Update <code>NewRequestMetrics()</code> constructor in <code>sim/metrics_utils.go</code> to propagate the new field from <code>Request</code> to <code>RequestMetrics</code></li> <li>Set the field at the appropriate event (e.g., <code>RoutingDecisionEvent</code> for cluster-level, or completion for computed metrics)</li> <li>Add behavioral tests covering multi-instance, single-instance, and standalone boundaries</li> </ol> <p>Examples: - See <code>HandledBy</code> (#181) \u2014 set by <code>RoutingDecisionEvent</code>, zero-value when used outside cluster pipeline (suppressed from JSON via <code>omitempty</code>) - See <code>SLOClass</code>/<code>TenantID</code> (PR10) \u2014 set during workload generation, propagated at injection</p>"},{"location":"contributing/hypothesis/","title":"Hypothesis Experiment Process","text":"<p>Status: Active (v2.1 \u2014 updated 2026-02-27)</p> <p>This document describes the end-to-end process for running a hypothesis-driven experiment in BLIS. For experiment standards (rigor, classification, analysis), see docs/contributing/standards/experiments.md. For the FINDINGS.md template, see docs/contributing/templates/hypothesis.md. For experiment status and coverage gaps, see hypotheses/README.md.</p> <p>For external contributors: Submit your experiment artifacts via PR. Maintainers will run the review protocols on your behalf. You can also conduct reviews manually using the perspective checklists documented in each gate.</p>"},{"location":"contributing/hypothesis/#overview","title":"Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 0: Create Worktree          \u2502 (Isolated workspace)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 1: Select &amp; Classify        \u2502 (Family, VV&amp;UQ, type)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 2: Design + Design Review   \u2502 (5 perspectives \u2192 convergence)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 3: Human Approval           \u2502 (Approve design before implementation)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 4: Implement                \u2502 (run.sh + analyze.py using harness)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 5: Code Review              \u2502 (5 perspectives \u2192 convergence)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 6: Run Experiments          \u2502 (Execute across seeds)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 7: Analyze &amp; Document       \u2502 (FINDINGS.md)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 8: FINDINGS Review          \u2502 (10 perspectives \u2192 convergence)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2191 (iterate rounds until convergence)\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 9: Self-Audit               \u2502 (6 dimensions, no agent)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 10: Verify + Commit + PR    \u2502 (verification gate, commit-push-pr)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key insights: 1. Three review gates at different lifecycle stages \u2014 each catches issues the others cannot 2. Human gate before implementation (Step 3) \u2014 prevents wasted experiment runs 3. Universal convergence protocol \u2014 same rules for all three gates 4. Self-audit \u2014 catches substance issues that pattern-matching agents miss</p>"},{"location":"contributing/hypothesis/#quick-reference","title":"Quick Reference","text":"Step Action 0. Worktree Create isolated workspace: <code>git worktree add .worktrees/h-&lt;name&gt; -b h-&lt;name&gt;</code> 1. Classify Choose family, VV&amp;UQ category, type from experiments.md 2. Design ED-1\u2013ED-6 compliance, then 5-perspective Design Review 3. Human gate Present design for approval \u2014 pause until approved 4. Implement <code>run.sh</code> + <code>analyze.py</code> using <code>hypotheses/lib/</code> harness 5. Code Review 5-perspective code review \u2192 convergence 6. Run Execute <code>./run.sh</code> across required seeds 7. Document Write FINDINGS.md using template 8. FINDINGS Review 10-perspective review \u2192 convergence (iterate rounds) 9. Self-audit 6 dimensions of deliberate critical thinking 10. Commit + PR Verification gate (if code fixes), then commit, push, create PR"},{"location":"contributing/hypothesis/#step-by-step-process","title":"Step-by-Step Process","text":""},{"location":"contributing/hypothesis/#step-0-create-isolated-worktree","title":"Step 0: Create Isolated Worktree","text":"<p>Context: Main repo (inference-sim)</p> <p>Create an isolated workspace BEFORE any work begins:</p> <pre><code>git worktree add .worktrees/h-&lt;hypothesis-name&gt; -b h-&lt;hypothesis-name&gt;\ncd .worktrees/h-&lt;hypothesis-name&gt;\n</code></pre> <p>This creates <code>.worktrees/h-&lt;name&gt;/</code> with a new branch. All subsequent steps happen in the worktree.</p> <p>Automation</p> <p><code>/superpowers:using-git-worktrees h-&lt;name&gt;</code> creates the worktree and switches your shell into it. See Skills &amp; Plugins.</p>"},{"location":"contributing/hypothesis/#step-1-select-and-classify-hypothesis","title":"Step 1: Select and Classify Hypothesis","text":"<p>Context: Worktree</p> <ol> <li>Select hypothesis \u2014 from <code>docs/plans/research.md</code>, coverage gaps in hypotheses/README.md, or a new observation</li> <li>Classify:</li> <li>(a) Which family? (See experiments.md for the 6 families and sentence patterns)</li> <li>(b) Verification, Validation, or UQ? (Determines evidence requirements)</li> <li>(c) Deterministic or statistical? If statistical, which subtype (dominance, monotonicity, equivalence, Pareto)?</li> </ol> <p>The family determines design rules; the VV&amp;UQ category determines evidence requirements.</p> <p>Tip: Pose the hypothesis WITHOUT reading the code first. Code-grounded hypotheses test implementation, not behavior. See Generating Hypotheses below.</p>"},{"location":"contributing/hypothesis/#step-2-design-experiment-design-review","title":"Step 2: Design Experiment + Design Review","text":"<p>Context: Worktree</p> <p>Design the experiment following ED-1 through ED-6 (see experiments.md): - ED-1: Controlled comparison (vary exactly one dimension) - ED-2: Rate awareness (run at target rate AND where effect should vanish) - ED-3: Precondition verification (in script, not just prose) - ED-4: Workload seed independence - ED-5: Reproducibility (everything from <code>run.sh</code> alone) - ED-6: Config diff against referenced experiments</p> <p>Then run the 5-perspective Design Review using the universal convergence protocol. Review from each perspective below (in parallel or sequentially), then apply the convergence protocol.</p> <p>Automation</p> <p><code>/convergence-review h-design</code> dispatches all 5 perspectives and enforces convergence. See Skills &amp; Plugins.</p>"},{"location":"contributing/hypothesis/#design-review-perspectives","title":"Design Review Perspectives","text":"<p>Perspective 1 \u2014 Hypothesis Quality: - Is the hypothesis behavioral, testable, and diagnostic? - Does it follow the family-specific sentence pattern from experiments.md? - Is the diagnostic clause present (\"If this fails, it would indicate...\")? - Is it correctly classified (family, VV&amp;UQ category, type)?</p> <p>Perspective 2 \u2014 Experiment Design Rigor (ED-1\u2013ED-6): - Is exactly one dimension varied? (ED-1) - Is there a rate where the effect should vanish, to confirm mechanism dependence? (ED-2) - Are preconditions verified in the script? (ED-3) - Is seed handling correct? (ED-4) - Is the experiment reproducible from <code>run.sh</code> alone? (ED-5) - If reusing calibration from a prior experiment, is the config diff documented? (ED-6)</p> <p>Perspective 3 \u2014 Parameter Calibration: - Are parameters computed analytically from known coefficients (alpha/beta), not guessed? - Are capacity estimates matched to the actual workload mode (CLI defaults vs workload-spec YAML)? - Is the operating point correct for the intended effect? (e.g., near saturation for queueing effects, sub-saturation for baseline)</p> <p>Perspective 4 \u2014 Control Completeness: - Does every proposed mechanism have a planned control experiment? (RCV-4) - Does each control isolate exactly one variable? - Is the baseline configuration clearly defined?</p> <p>Perspective 5 \u2014 DES and Domain Fit: - Will the experiment create the conditions needed for the hypothesis to be testable? - Are there DES-specific subtleties (event ordering, clock granularity, alpha overhead) that could confound results? - Is the experiment duration sufficient? Is the warmup period adequate?</p> <p>Cross-gate regression: If a later gate (Code Review, FINDINGS Review) discovers a design-level flaw (e.g., confounding variable, wrong operating point), the workflow loops back to Step 2 for re-design, re-convergence, and re-approval.</p>"},{"location":"contributing/hypothesis/#step-3-human-approval-of-experiment-design","title":"Step 3: Human Approval of Experiment Design","text":"<p>Context: Worktree (after Design Review convergence)</p> <p>Present the experiment design for human approval. The human reviews: - Hypothesis classification (family, VV&amp;UQ, type) - Experiment design (ED-1\u2013ED-6 compliance) - Parameter choices (computed from coefficients, not guessed) - Planned controls (one per proposed mechanism) - Expected outcomes and diagnostic implications</p> <p>This is a hard gate. Wait for explicit approval before proceeding. Do not assume silence is consent \u2014 the pause is the point.</p> <p>In parallel mode: Each hypothesis gets its own independent human approval. The team lead presents each design; the human approves each independently.</p>"},{"location":"contributing/hypothesis/#step-4-implement-experiment-code","title":"Step 4: Implement Experiment Code","text":"<p>Context: Worktree (after human approval)</p> <p>Create <code>hypotheses/&lt;name&gt;/run.sh</code> and <code>hypotheses/&lt;name&gt;/analyze.py</code>.</p> <p>Harness requirements (mandatory): - <code>run.sh</code> MUST source <code>hypotheses/lib/harness.sh</code> and use <code>blis_run</code> for every simulation call - Every <code>blis_run</code> call MUST have an appropriate timeout tier (<code>TIMEOUT_QUICK</code>/<code>TIMEOUT_STANDARD</code>/<code>TIMEOUT_EXTENDED</code>) - If using <code>--total-kv-blocks</code>, call <code>preflight_kv_check</code> with max expected input tokens - <code>analyze.py</code> MUST import <code>analyze_helpers</code> and use <code>parse_blis_output</code> (handles timeouts gracefully)</p> <p>Reference comment: If reusing calibration from a prior experiment, include <code># Reference: hypotheses/&lt;name&gt;/run.sh</code> with the file path.</p>"},{"location":"contributing/hypothesis/#step-5-code-review-5-perspectives","title":"Step 5: Code Review (5 perspectives)","text":"<p>Context: Worktree (after implementation, BEFORE running experiments)</p> <p>Every <code>run.sh</code> and <code>analyze.py</code> must be code-reviewed BEFORE running experiments. This is non-negotiable. Three of four major bugs in PR #310 would have been caught by code review before a single experiment ran.</p> <p>Cross-gate regression: If this gate discovers a design-level flaw (e.g., confounding variable, wrong operating point), loop back to Step 2 for re-design, re-convergence, and re-approval. The experiment-wide limit of 2 cross-gate regressions applies (see Step 8 for the circuit breaker).</p> <p>Run the 5-perspective Code Review using the universal convergence protocol. Review from each perspective below (in parallel or sequentially), then apply the convergence protocol.</p> <p>Automation</p> <p><code>/convergence-review h-code hypotheses/&lt;name&gt;/</code> dispatches all 5 perspectives and enforces convergence. See Skills &amp; Plugins.</p>"},{"location":"contributing/hypothesis/#code-review-perspectives","title":"Code Review Perspectives","text":"<p>Perspective 1 \u2014 Parser\u2013Output Format Agreement: - For every regex or field extraction in <code>analyze.py</code>, verify the pattern matches the actual output format in the simulator code - <code>cmd/root.go</code> \u2014 what text does the CLI print? (e.g., <code>\"Preemption Rate: %.4f\"</code>) - <code>sim/metrics_utils.go</code> \u2014 what JSON fields exist? - Match every regex in <code>analyze.py</code> against the format string in the producer code - Silent defaults: Verify that when a regex matches nothing, <code>analyze.py</code> emits a warning to stderr rather than silently defaulting to 0</p> <p>Perspective 2 \u2014 CLI Flag Correctness: - For every flag in <code>run.sh</code>, verify the flag name and value match <code>cmd/root.go</code> defaults and help text - Check for typos that strict parsing would reject - Cross-reference every CLI flag against <code>cmd/root.go</code> flag definitions</p> <p>Perspective 3 \u2014 YAML Field Validation: - Verify workload YAML field names against <code>sim/workload/spec.go</code> struct tags - <code>KnownFields(true)</code> will reject typos at runtime, but catching them at review saves a failed experiment run</p> <p>Perspective 4 \u2014 Config Diff (ED-6): - If the experiment reuses calibration from a prior experiment, diff every CLI flag and YAML field between the two experiments - Explicitly list differences - Verify the <code># Reference:</code> comment in <code>run.sh</code> points to the correct file</p> <p>Perspective 5 \u2014 Seed and Determinism: - Verify <code>--seed</code> is passed correctly and workload YAML <code>seed:</code> field doesn't conflict - Verify seeds vary across runs as intended (ED-4) - Check that <code>run.sh</code> builds the binary and is fully self-contained (ED-5)</p> <p>Evidence: what code review catches</p> Bug Round discovered Would code review have caught it? YAML <code>input_dist</code> vs <code>input_distribution</code> (H5) Round 1 run failure Yes \u2014 cross-ref against <code>spec.go</code> struct tags Analyzer regex <code>Preemptions?: (\\d+)</code> vs actual <code>Preemption Rate: 0.1750</code> (H10) Round 4 Yes \u2014 cross-ref against <code>cmd/root.go</code> format string H10 routing policy mismatch with H8 Round 2 Yes \u2014 ED-6 config diff H5 bucket cap=500 &lt; mean_input=512 Round 2 Possibly \u2014 first-principles check on parameters"},{"location":"contributing/hypothesis/#step-6-run-experiments","title":"Step 6: Run Experiments","text":"<p>Context: Worktree (after Code Review convergence)</p> <p>Execute experiments across required seeds: - Deterministic experiments: Single seed sufficient (determinism is the point) - Statistical experiments: Minimum 3 seeds (42, 123, 456) for each configuration - Verify reproducibility: running <code>./run.sh</code> twice produces identical output (ED-5)</p>"},{"location":"contributing/hypothesis/#step-7-analyze-and-document-findingsmd","title":"Step 7: Analyze and Document FINDINGS.md","text":"<p>Context: Worktree (after experiments complete)</p> <ol> <li>Analyze \u2014 produce comparison tables, compute effect sizes</li> <li>Verify root cause \u2014 trace every causal claim through code (RCV-1, RCV-2, RCV-3)</li> <li>Document FINDINGS.md \u2014 use the template. All sections must be present and non-empty.</li> <li>Update <code>hypotheses/README.md</code> \u2014 add a row to the \"Validated Hypotheses\" table and update \"Coverage by Family\" if needed</li> </ol>"},{"location":"contributing/hypothesis/#step-8-findings-review-10-perspectives","title":"Step 8: FINDINGS Review (10 perspectives)","text":"<p>Context: Worktree (after FINDINGS.md documented)</p> <p>Run the 10-perspective FINDINGS Review using the universal convergence protocol. Review from each perspective below (in parallel or sequentially), then apply the convergence protocol.</p> <p>Automation</p> <p><code>/convergence-review h-findings hypotheses/&lt;name&gt;/FINDINGS.md</code> dispatches all 10 perspectives and enforces convergence. See Skills &amp; Plugins.</p> <p>Cross-gate regression: If this gate discovers a design-level flaw (e.g., confounding variable not identified in design), loop back to Step 2 for re-design, re-convergence, and re-approval. Maximum 2 cross-gate regressions per experiment (across all gates combined) \u2014 if the design still has fundamental issues after 2 regressions, suspend the experiment and escalate for a re-scoping decision.</p>"},{"location":"contributing/hypothesis/#findings-review-perspectives","title":"FINDINGS Review Perspectives","text":"<p>Reviewer 1 \u2014 Code Verifier: - READ the actual source files cited in the FINDINGS.md. Verify every <code>file:line</code> citation against current code. - Does the code at the cited location actually produce the claimed behavior? - Are there off-by-one errors in line citations? (Acceptable: \u00b12 lines. Flag: &gt;2 lines off.) - Does the mechanism explanation match what the code does, not just what it's named?</p> <p>Reviewer 2 \u2014 Experiment Designer: - Are there confounding variables? Is exactly one dimension varied? (ED-1) - Was the experiment run at a rate where the effect should vanish, to confirm mechanism dependence? (ED-2) - Are experiment preconditions verified in the script (e.g., queue depth &gt; batch size for SJF tests)? (ED-3) - Is workload seed handling correct? Does <code>--seed</code> on the CLI properly vary across runs? (ED-4) - Is the experiment reproducible from <code>run.sh</code> alone \u2014 binary built, seeds documented, no manual steps? (ED-5) - Is the config diff against referenced experiments documented? (ED-6) - Are there missing control experiments or confound matrix cells? - Are parameters properly calibrated? (e.g., bucket cap vs mean input) - Cross-reference every CLI flag in <code>run.sh</code> against <code>cmd/root.go</code> flag definitions. - Cross-reference every YAML field name against <code>sim/workload/spec.go</code> struct tags.</p> <p>Reviewer 3 \u2014 Statistical Rigor: - Are \"surprises\" computed from first principles? (RCV-2) - Is the sample size adequate (seeds, operating points)? - Are claims properly scoped (not over-generalized from narrow evidence)? - Is the evidence quality table complete and honest? - Do per-seed effect sizes meet the legacy thresholds (&gt;20% for dominance, &lt;5% for equivalence)? - Is the status classification consistent with the data? (e.g., \"Confirmed\" requires &gt;20% in ALL seeds.)</p> <p>Reviewer 4 \u2014 Control Experiment Auditor: - Does every proposed mechanism (RCV-3) have a corresponding control experiment (RCV-4)? - Were control experiments actually EXECUTED, not just proposed? Look for conditional language (\"one would test\", \"could be confirmed by\") vs past tense with data (\"the control showed 0.0% difference\"). Verify that control results appear in the Results section with actual numbers, not just in Root Cause Analysis as narrative. - Does each control isolate exactly one variable? Diff the CLI flags between treatment and control runs in <code>run.sh</code> \u2014 the control should differ by exactly one flag or parameter. - Do the control results confirm or refute the proposed mechanism? - Do the control experiment results in the Evidence Quality table accurately reflect the current round (not stale text from a prior round)? - Does the mechanism explain the direction using experimental evidence, not just code-reading claims?</p> <p>Reviewer 5 \u2014 Standards Compliance: - Are ALL FINDINGS.md sections present and non-empty? (per <code>docs/contributing/templates/hypothesis.md</code>) - Is the hypothesis correctly classified (family, VV&amp;UQ category, type)? - Does the Devil's Advocate section (RCV-5) argue both directions convincingly? - Are scope and limitations (RCV-6) complete \u2014 operating point, dependencies, what was NOT tested, generalizability, UQ? - Does the standards audit correctly check findings against <code>docs/contributing/standards/rules.md</code> and <code>docs/contributing/standards/invariants.md</code>? - Are any new rules or invariants warranted by the findings?</p> <p>Reviewer 6 \u2014 Substance and Logic: - Are there logical errors in the conclusions? - Are there mathematical mistakes in effect size calculations or statistical claims? - Does the evidence actually support the claims? (Not just \"the numbers are close enough\") - Are alternative explanations adequately considered?</p> <p>Reviewer 7 \u2014 DES Mechanism Expert: - Are there event-ordering subtleties that could explain the results differently? - Are assumptions about DES timing correct (alpha overhead, step quantization, clock granularity)? - Could the result be an artifact of the simulation architecture rather than the modeled system behavior?</p> <p>Reviewer 8 \u2014 Reproducibility and Robustness: - Can <code>run.sh</code> reproduce the results from scratch on a clean checkout? - Are results fragile to small parameter variations? (Would \u00b110% on key parameters change the conclusion?) - Are all intermediate files generated by the script, not checked in as stale artifacts?</p> <p>Reviewer 9 \u2014 Cross-Experiment Consistency: - Do the findings contradict any prior experiment results? If so, is the contradiction acknowledged and explained? - Are references to prior experiments accurate? (Check specific claims against referenced FINDINGS.md files) - Are there stale references to prior rounds that should have been updated?</p> <p>Reviewer 10 \u2014 User Guidance and Actionability: - Are the \"Implications for Users\" practical and specific enough to act on? - Are proposed issues (bugs, enhancements, follow-up hypotheses) well-scoped? - Would a BLIS user reading this FINDINGS.md understand what to do differently?</p> <p>The overlap between reviewers is intentional \u2014 different perspectives checking the same FINDINGS.md catch different issues. Evidence from PR #385: Reviewer 1 (code) and Reviewer 3 (rigor) both caught H19's stale evidence quality row; Reviewer 2 (design) caught H16's sub-threshold seed; Reviewer 4 (control) caught H21's unexecuted control experiments.</p>"},{"location":"contributing/hypothesis/#step-9-self-audit-6-dimensions","title":"Step 9: Self-Audit (6 dimensions)","text":"<p>Context: Worktree (after FINDINGS Review convergence)</p> <p>Review each dimension yourself using critical thinking \u2014 do not delegate to automated tools. This step requires the kind of deliberate reflection that only happens when you pause and think, not when you dispatch a tool.</p> <p>Why this step exists: In PR9, 3 real bugs were found by critical thinking after 4 automated passes found 0 issues. Automated review perspectives check structure; this step checks substance.</p> <p>Self-audit dimensions \u2014 think through each one:</p> <ol> <li>Logic bugs in analyzer code: Trace through <code>analyze.py</code> mentally. Are there edge cases where regex parsing silently defaults to 0? Could integer vs float conversion produce wrong results?</li> <li>Results determinism/reproducibility: Would running <code>./run.sh</code> again produce identical output? Are there any non-deterministic dependencies (timestamps, system load, file ordering)?</li> <li>FINDINGS.md internal consistency: Does the Status match the data in the Results section? Does the Devil's Advocate section actually argue against the conclusion? Are all per-seed values consistent with the summary?</li> <li>Cross-experiment contradictions: Do these findings contradict any known BLIS behavior documented in prior experiments or MEMORY.md? If so, is the contradiction explained?</li> <li>User guidance practicality: Would a BLIS user know what to do with these findings? Are the implications actionable?</li> <li>Issue filing completeness: For each actionable finding, is there a clear issue to file? Are there findings that need issues but don't have them planned?</li> </ol> <p>Fix all issues found. Then proceed to Step 10.</p>"},{"location":"contributing/hypothesis/#step-10-verification-gate-commit-pr","title":"Step 10: Verification Gate + Commit + PR","text":"<p>Context: Worktree (after self-audit)</p> <p>If the experiment discovered code fixes (e.g., #386 KV livelock, #387 conservation test update), run the verification gate before committing:</p> <pre><code>go build ./...          # Build passes\ngo test ./... -count=1  # All tests pass\ngolangci-lint run ./... # Zero lint issues\n</code></pre> <p>Commit and PR:</p> <pre><code>git add hypotheses/&lt;name&gt;/\ngit commit -m \"experiment(&lt;name&gt;): &lt;hypothesis sentence&gt; \u2014 &lt;status&gt;\"\ngit push -u origin &lt;branch-name&gt;\ngh pr create --title \"experiment: &lt;name&gt;\" --body \"&lt;hypothesis, findings, closing keywords&gt;\"\n</code></pre> <p>The PR description should include: - Hypothesis sentence and status - Key findings (1-3 bullet points) - <code>Fixes #NNN</code> for any issues this experiment addresses</p> <p>Automation</p> <p><code>/commit-commands:commit-push-pr</code> handles staging, committing, pushing, and PR creation in one command.</p> <p>Post-PR issue filing: See Two-Track Issue Filing below.</p>"},{"location":"contributing/hypothesis/#universal-convergence-protocol","title":"Universal Convergence Protocol","text":"<p>Canonical source: <code>docs/contributing/convergence.md</code>. If this section diverges, convergence.md is authoritative.</p> <p>All three review gates (Design Review, Code Review, FINDINGS Review) use the same convergence protocol: run all N perspectives in parallel, fix any CRITICAL/IMPORTANT findings, re-run until zero CRITICAL and zero IMPORTANT in a round. Max 10 rounds per gate. See <code>docs/contributing/convergence.md</code> for the full protocol, severity definitions, agent failure handling, and expected convergence rates.</p> <p>Automation</p> <p>The <code>convergence-review</code> skill automates this protocol: <code>/convergence-review &lt;gate-type&gt; [artifact-path]</code>. See Skills &amp; Plugins.</p>"},{"location":"contributing/hypothesis/#parallel-execution-mode","title":"Parallel Execution Mode","text":"<p>The #385/#390 pattern: run N hypothesis experiments simultaneously with a team lead coordinating.</p>"},{"location":"contributing/hypothesis/#setup","title":"Setup","text":"<ol> <li>Team lead creates worktree: <code>.worktrees/h-&lt;batch-name&gt;/</code></li> <li>Team lead builds binary once: <code>go build -o blis main.go</code> (agents reference this shared binary)</li> <li>Team lead creates team with N hypothesis agents, each assigned to <code>hypotheses/&lt;name&gt;/</code></li> <li>Each agent runs the full pipeline independently (Steps 1-9)</li> </ol>"},{"location":"contributing/hypothesis/#coordination-rules","title":"Coordination Rules","text":"<ul> <li>Each agent creates files ONLY in its own <code>hypotheses/&lt;name&gt;/</code> directory \u2014 no file conflicts</li> <li>README.md updates are deferred to the team lead's consolidation step (not done by individual agents)</li> <li>Team lead MUST independently run convergence review for each experiment. Do NOT delegate convergence assessment to the same agent that ran the experiment. Evidence from #390: agents self-reported \"Round 1 convergence\" but actual independent review found 3 CRITICAL + 18 IMPORTANT issues.</li> <li>Step 3 is a synchronization point. All agents pause at Step 3 until the human has reviewed and approved each design independently. The team lead should batch-present all designs for human review to minimize idle time.</li> <li>Solo mode is the degenerate case (team size = 1)</li> </ul>"},{"location":"contributing/hypothesis/#consolidation","title":"Consolidation","text":"<p>After all agents complete: 1. Team lead reviews all proposed issues for deduplication 2. Team lead updates <code>hypotheses/README.md</code> with all new experiments 3. Cross-experiment consistency check across all N experiments 4. Single PR via <code>commit-push-pr</code> skill</p>"},{"location":"contributing/hypothesis/#two-track-issue-filing","title":"Two-Track Issue Filing","text":""},{"location":"contributing/hypothesis/#immediate-track-file-as-soon-as-discovered","title":"Immediate Track (file as soon as discovered)","text":"<p>Only for bugs that affect experiment validity: - Simulator panics or crashes during experiment - Conservation violations (INV-1) discovered during analysis - Livelock or infinite loops (R19) preventing experiment completion - Data loss or silent failures affecting measured metrics</p> <p>Example: #386 (KV livelock) was filed immediately because it caused infinite preempt-requeue cycles.</p>"},{"location":"contributing/hypothesis/#post-convergence-track-file-after-convergence-pr-creation","title":"Post-Convergence Track (file AFTER convergence + PR creation)","text":"<p>For all findings-derived issues: - Bugs found but not affecting current experiment validity - Enhancements, design limitations, new hypotheses - Standards updates (new rules, invariants) - Promotion to Go test suite</p> <p>Why issues come last: Findings can change across rounds (H10 went from \"untested\" to \"confirmed\" between Rounds 3-4). Filing issues before convergence risks creating wrong issues. File once, file right.</p> <p>See Issue Taxonomy for the complete filing guide.</p>"},{"location":"contributing/hypothesis/#quality-gates","title":"Quality Gates","text":""},{"location":"contributing/hypothesis/#pre-execution-gates-check-before-running-experiments-step-5","title":"Pre-Execution Gates (check BEFORE running experiments \u2014 Step 5)","text":"<ul> <li> <code>run.sh</code> sources <code>hypotheses/lib/harness.sh</code> and uses <code>blis_run</code> for every simulation call</li> <li> Every <code>blis_run</code> call has an appropriate timeout tier (<code>TIMEOUT_QUICK</code>/<code>TIMEOUT_STANDARD</code>/<code>TIMEOUT_EXTENDED</code>)</li> <li> KV safety pre-flight: if experiment uses <code>--total-kv-blocks</code>, call <code>preflight_kv_check</code> with max expected input tokens</li> <li> <code>analyze.py</code> imports <code>analyze_helpers</code> and uses <code>parse_blis_output</code> (handles timeouts gracefully)</li> <li> <code>run.sh</code> flags verified against <code>cmd/root.go</code> help text</li> <li> <code>analyze.py</code> regexes verified against actual output format strings in <code>cmd/root.go</code> and <code>sim/metrics_utils.go</code></li> <li> Workload YAML field names verified against <code>sim/workload/spec.go</code> struct tags</li> <li> Config diff against referenced experiments documented (ED-6)</li> <li> Code Review (5 perspectives) converged</li> </ul>"},{"location":"contributing/hypothesis/#per-round-gates-check-after-each-findings-review-round-step-8","title":"Per-Round Gates (check after each FINDINGS Review round \u2014 Step 8)","text":"<ul> <li> Every causal claim cites <code>file:line</code> (RCV-1)</li> <li> Every \"surprise\" has a first-principles calculation (RCV-2)</li> <li> Root cause explains mechanism AND direction (RCV-3)</li> <li> All reviewer assessments completed and all CRITICAL/IMPORTANT items addressed</li> </ul>"},{"location":"contributing/hypothesis/#final-gates-check-before-pr-step-10","title":"Final Gates (check before PR \u2014 Step 10)","text":"<ul> <li> Hypothesis classified (deterministic or statistical + subtype)</li> <li> Experiment design follows ED-1 through ED-6</li> <li> If reusing prior calibration data, config diff documented (ED-6)</li> <li> Results reproducible via <code>./run.sh</code></li> <li> FINDINGS Review converged: zero CRITICAL and zero IMPORTANT items across all 10 reviewers in the current round</li> <li> Self-audit completed (6 dimensions)</li> <li> All review feedback addressed or explicitly acknowledged as open</li> <li> Findings classified per the findings table (including resolution type)</li> <li> Standards audit completed</li> <li> Promotion assessment completed (see Promotion of Confirmed Hypotheses)</li> <li> <code>hypotheses/README.md</code> updated with new experiment row(s) and coverage changes</li> <li> If code fixes involved: <code>go build</code>, <code>go test</code>, <code>golangci-lint</code> all pass</li> </ul>"},{"location":"contributing/hypothesis/#post-pr-gates-check-after-pr-creation-step-10","title":"Post-PR Gates (check after PR creation \u2014 Step 10)","text":"<ul> <li> Issues filed per Issue Taxonomy \u2014 one per actionable finding</li> <li> Each issue has correct label (<code>bug</code>, <code>enhancement</code>, <code>hypothesis</code>, <code>design</code>, or <code>standards</code>)</li> <li> Each issue references the PR number</li> <li> No issues filed for \"documented here\" findings with no action needed</li> </ul>"},{"location":"contributing/hypothesis/#when-to-run-experiments","title":"When to Run Experiments","text":"<ul> <li>Validating that a new feature works as designed (post-PR confirmation)</li> <li>Testing intuitive claims about system behavior (from <code>docs/plans/research.md</code>)</li> <li>Investigating unexpected behavior observed during development</li> <li>Exploring design tradeoffs between configurations</li> <li>Filling coverage gaps identified in the family coverage table</li> </ul>"},{"location":"contributing/hypothesis/#generating-hypotheses","title":"Generating Hypotheses","text":"<p>Hypotheses can come from internal sources (your own experiments and development) or external sources (user questions, literature, analytical models). This section provides structured guidance for generating good hypotheses. See also experiments.md for family-specific sentence patterns.</p>"},{"location":"contributing/hypothesis/#sources-of-hypotheses","title":"Sources of hypotheses","text":"Source How it works Example User intuition \"I think X should be better than Y because of Z\" \"SJF should reduce TTFT for mixed workloads because short jobs finish first\" Coverage gaps Check the family coverage table for untested families Workload/arrival family has 0 experiments \u2192 \"Gamma sampler should match theoretical CV\" Experiment findings Surprises and open questions from completed experiments spawn follow-up hypotheses H10's maybeOffload finding \u2192 \"test at GPU=1500 for preemption-path offload\" Bug reports \"This behavior seems wrong\" \u2192 formalize as a testable claim H12: preemption panic \u2192 \"conservation should hold even under preemption pressure\" Analytical models Divergence between theory and simulation \u2192 \"does the DES match M/M/k under matching assumptions?\" \"Under Poisson arrivals, queue length should match M/M/k within 5%\" Literature / external Published results about inference serving systems \"Prefix caching should reduce TTFT proportional to prefix length (as in vLLM literature)\" Design docs Claims made in design documents that have never been validated \"The composable scorer framework should produce Pareto-optimal configurations\" Strategy Evolution Each strategy iteration produces a hypothesis bundle \u2014 a main hypothesis plus ablation, control, and robustness arms \"SLO-tiered priority will reduce critical TTFT P99 by &gt;30%\" + ablation arms for each component"},{"location":"contributing/hypothesis/#what-makes-a-good-hypothesis","title":"What makes a good hypothesis","text":"<p>A good hypothesis is behavioral (about observable system behavior), testable (with a clear experiment), and diagnostic (failure points to something worth investigating).</p> Criterion Good Bad Behavioral \"Burst smoothing should reduce tail latency\" \"The token bucket decrements currentTokens correctly\" Testable \"TTFT should decrease monotonically as prefix_length increases\" \"The system should be fast\" Diagnostic \"If this fails, it indicates the cache eviction path has a bug\" \"If this fails, something is wrong\" Conceptual \"Tiered storage should reduce preemptions\" \"tiered.go:224 should delete the hash\" Intuitive \"More instances should roughly halve latency under saturation\" \"The event queue should process 2x events\""},{"location":"contributing/hypothesis/#anti-patterns-in-hypothesis-generation","title":"Anti-patterns in hypothesis generation","text":"Anti-pattern Problem Fix Code-grounded hypothesis Tests implementation, not behavior. Prevents discovery of design gaps. Pose the hypothesis WITHOUT reading the code first. Unfalsifiable hypothesis \"The system should work correctly\" \u2014 no way to fail Specify a concrete metric and direction: \"TTFT P99 should be lower for A than B\" Hypothesis that tests the obvious \"More resources should improve performance\" \u2014 trivially true Add a diagnostic clause: \"...and the improvement should be proportional to the resource increase (not sub-linear due to contention)\" Hypothesis with no failure action Confirmation and refutation both lead to \"ok, noted\" Every hypothesis should specify: \"If this fails, investigate X\" Over-scoped hypothesis \"The entire system should be correct under all configurations\" Decompose by family: scheduler invariant + structural model + robustness are separate experiments"},{"location":"contributing/hypothesis/#how-to-propose-a-new-hypothesis","title":"How to propose a new hypothesis","text":"<ol> <li>Check coverage: Read the family coverage table. Prioritize families with low coverage.</li> <li>Choose a family: Which domain does your claim target? (See experiments.md for the 6 families.)</li> <li>Write the sentence: Use the family-specific pattern from experiments.md.</li> <li>Add the diagnostic clause: \"If this fails, it would indicate...\"</li> <li>Check for redundancy: Search existing hypotheses in <code>docs/plans/research.md</code> and on GitHub: issues labeled <code>hypothesis</code>.</li> <li>File as a GitHub issue: Use the Hypothesis Proposal issue template on GitHub (click \"New Issue\" \u2192 \"Hypothesis Proposal\"). This template has fields for family, VV&amp;UQ category, diagnostic value, and experiment design.</li> </ol> <p>External contributors should file a GitHub issue using the Hypothesis Proposal template. Maintainers will triage, prioritize, and run the review protocol.</p>"},{"location":"contributing/hypothesis/#issue-taxonomy-after-convergence","title":"Issue Taxonomy (after convergence)","text":"<p>After convergence and PR creation, walk the findings classification table in FINDINGS.md and file one GitHub issue per actionable finding. Not every hypothesis produces issues \u2014 a clean confirmation (like H13) may produce none.</p> <p>Issue types and labels:</p> Issue Type Label When to file Title format Example Bug <code>--label bug</code> Code defect discovered during experiment <code>bug: &lt;component&gt; \u2014 &lt;defect&gt;</code> <code>bug: sim/simulator.go \u2014 preempt() panics on empty RunningBatch</code> (H12) Enhancement <code>--label enhancement</code> New feature, rule, or documentation improvement needed <code>enhancement: &lt;area&gt; \u2014 &lt;improvement&gt;</code> <code>enhancement: CLI \u2014 document token-bucket per-input-token cost model</code> (H5) New hypothesis <code>--label hypothesis</code> Follow-up experiment spawned by current findings <code>hypothesis: &lt;claim to test&gt;</code> <code>hypothesis: test tiered KV at GPU=1500 blocks to trigger preemption-path offload</code> (H10) Design limitation <code>--label design</code> System works as coded but has undocumented behavioral limitation <code>design: &lt;limitation&gt;</code> <code>design: no burst-smoothing sweet spot under Gamma CV&gt;3</code> (H5) Standards update <code>--label standards</code> New rule or invariant discovered that should be added <code>standards: &lt;rule/invariant&gt;</code> <code>standards: R17 signal freshness \u2014 routing signals have tiered staleness</code> (H3) Promotion <code>--label promotion</code> Confirmed hypothesis finding promoted to Go test suite <code>enhancement: promote &lt;hypothesis&gt; &lt;finding&gt; to Go test suite</code> <code>enhancement: promote H-Overload conservation under 10x to Go test suite</code> (#337) <p>Mapping from resolution type to expected issues:</p> Resolution Expected issues Clean confirmation Usually none. Optionally: promotion to Go test suite, standards update confirming existing rules. Confirmation with wrong mechanism Enhancement: update documentation with correct mechanism. Confirmation with bug discovery Bug: one per code defect. Enhancement: if detector/tooling needs improvement. Partial confirmation with surprise New hypothesis: follow-up experiments to investigate surprise. Refuted \u2014 system design flaw Design: architectural limitation. Enhancement: proposed fix. Refuted \u2014 mechanism not plausible Design: document the limitation. Enhancement: update CLI help or user docs if misleading. Refuted \u2014 wrong mental model Usually none. Optionally: enhancement if CLI help text is misleading. Inconclusive \u2014 parameter-dependent New hypothesis: test at different parameters. Converged to open question New hypothesis: specific experiment or tooling to resolve. <p>Issue body template:</p> <pre><code>## Context\nDiscovered in hypothesis experiment &lt;name&gt; (PR #NNN).\n\n## Finding\n&lt;One-paragraph description from FINDINGS.md&gt;\n\n## Evidence\n&lt;Key data point or code reference&gt;\n\n## Proposed action\n&lt;What should be done \u2014 fix, new experiment, documentation update&gt;\n</code></pre> <p>What NOT to file: - Issues for findings that are \"documented here\" with no action needed - Duplicate issues for findings already covered by existing open issues - Issues for scope limitations that are acknowledged in FINDINGS.md (these are future work, not bugs)</p>"},{"location":"contributing/hypothesis/#promotion-of-confirmed-hypotheses","title":"Promotion of Confirmed Hypotheses","text":"<p>After convergence, assess whether any confirmed findings should be promoted from bash-script experiments to the Go test suite and/or formal invariants. Hypothesis experiments run as bash scripts are NOT in CI \u2014 a regression would not be caught by <code>go test ./...</code>.</p>"},{"location":"contributing/hypothesis/#when-to-promote","title":"When to promote","text":"Condition Promote to Why Confirmed deterministic hypothesis Go test (regression protection in CI) Deterministic properties are exact \u2014 they can be encoded as pass/fail tests. Deterministic invariant aspect of a statistical hypothesis Go test for the invariant aspect Statistical hypotheses often contain deterministic sub-claims (e.g., conservation holds across all configs). New invariant discovered <code>docs/contributing/standards/invariants.md</code> entry Codify as a formal system property with verification strategy. New rule discovered <code>docs/contributing/standards/rules.md</code> entry Codify as an antipattern check for PR reviews."},{"location":"contributing/hypothesis/#what-a-promoted-test-looks-like","title":"What a promoted test looks like","text":"<pre><code>// TestClusterConservation_AcrossPolicyCombinations tests INV-1 at cluster level.\n// Promoted from hypothesis H12 (hypotheses/h12-conservation/).\nfunc TestClusterConservation_AcrossPolicyCombinations(t *testing.T) {\n    configs := []struct{ routing, scheduler, admission string }{\n        {\"round-robin\", \"fcfs\", \"always-admit\"},\n        {\"least-loaded\", \"fcfs\", \"always-admit\"},\n        {\"weighted\", \"priority-fcfs\", \"token-bucket\"},\n        // ... all 10 H12 configurations\n    }\n    for _, cfg := range configs {\n        t.Run(cfg.routing+\"/\"+cfg.scheduler+\"/\"+cfg.admission, func(t *testing.T) {\n            // Run cluster simulation\n            // Assert: injected == completed + still_queued + still_running\n        })\n    }\n}\n</code></pre> <p>The bash experiment remains as the full reproducible artifact with analysis. The Go test is the CI-integrated regression guard.</p>"},{"location":"contributing/hypothesis/#why-iterate-until-convergence-not-fixed-rounds","title":"Why Iterate Until Convergence (Not Fixed Rounds)?","text":"<p>Evidence from PR #310 (H5, H10, H13):</p> Round What happened What was caught 1 Initial experiments Wrong root causes for H5 and H10 2 Code + external review Corrected math (H5), identified mechanism (H10), designed confound matrix 3 Confound matrix + calibrated bucket H5 burst-smoothing mechanism refuted, H10 analyzer bug masked preemptions 4 Corrected analyzer H10 confirmed \u2014 preemptions DO occur, cache hits INCREASE <p>H13 converged in Round 1 (deterministic = pass/fail). H5 converged in Round 3. H10 required Round 4 due to an analyzer bug. Fixed round counts would have either stopped too early (missing the H10 bug) or forced unnecessary work (H13 didn't need Round 2).</p>"},{"location":"contributing/hypothesis/#why-internal-agents-beat-external-llms","title":"Why internal agents beat external LLMs","text":"Capability External (<code>/review-plan</code>) Internal (Task agent) Read source files No Yes \u2014 verifies every citation Cross-ref regexes against format strings No Yes \u2014 catches analyzer bugs Check YAML fields against struct tags No Yes \u2014 catches typos Run <code>grep</code> to verify claims No Yes \u2014 can search for executed vs proposed controls API reliability Fragile (auth, timeouts, rate limits) Reliable (same process)"},{"location":"contributing/hypothesis/#references","title":"References","text":"<ul> <li>Standards: docs/contributing/standards/experiments.md</li> <li>Template: docs/contributing/templates/hypothesis.md</li> <li>Hypothesis catalog: docs/plans/research.md</li> <li>Validated experiments: hypotheses/README.md</li> <li>PR workflow (structural inspiration): docs/contributing/pr-workflow.md</li> </ul>"},{"location":"contributing/hypothesis/#appendix-workflow-evolution","title":"Appendix: Workflow Evolution","text":"<p>v1.0 (PR #310): Three external LLM reviews per round, no design gate, no code review gate, ad-hoc git commands. v2.0 (2026-02-23, #392): Three review gates (Design 5, Code 5, FINDINGS 10) with universal convergence protocol, human approval gate, self-audit, verification gate, parallel execution, two-track issue filing, explicit worktree/commit skill integration. Structural alignment with PR workflow v3.0. v2.1 (2026-02-27, #464): Human-first rewrite. Manual steps primary; skills in admonition callouts. Prerequisites table removed (skills referenced inline per step). \"For Claude\" directives rewritten as universal process guidance.</p>"},{"location":"contributing/macro-planning/","title":"Macro Plan Process","text":"<p>Status: Active (v1.0 \u2014 updated 2026-02-26)</p> <p>This document describes the process for creating a macro-level implementation plan (multi-PR feature). For the macro plan template, see docs/contributing/templates/macro-plan.md.</p>"},{"location":"contributing/macro-planning/#when-a-macro-plan-is-needed","title":"When a Macro Plan is Needed","text":"<ul> <li>Features spanning 2+ PRs</li> <li>Work requiring a dependency DAG between PRs</li> <li>Features touching multiple module boundaries</li> </ul> <p>Not needed for: Single-PR features, bug fixes, documentation changes.</p>"},{"location":"contributing/macro-planning/#steps","title":"Steps","text":"<ol> <li>Design doc(s) as input \u2014 read the relevant design doc(s) and/or GitHub issues</li> <li>Decompose into PRs \u2014 each PR should be independently mergeable and testable</li> <li>Define the dependency DAG \u2014 which PRs can be parallelized? Which must be sequential?</li> <li>Define module contracts per PR boundary \u2014 what does each PR guarantee to the next?</li> <li>Identify frozen interfaces \u2014 which interfaces are stable (can be developed against in parallel)?</li> <li>Identify flexible internals \u2014 which implementation details may change during micro-planning?</li> <li>Convergence review \u2014 Run all 8 perspectives in parallel (or sequentially), applying the convergence protocol. Fix CRITICAL and IMPORTANT findings, re-run until convergence.</li> <li>Human review \u2014 approve before micro-planning begins for any PR in the plan</li> </ol>"},{"location":"contributing/macro-planning/#macro-plan-review-perspectives-8","title":"Macro Plan Review Perspectives (8)","text":"<p>For each perspective, check every item. Classify findings as CRITICAL / IMPORTANT / SUGGESTION per the convergence protocol. Section references below refer to design-guidelines.md and macro-plan template unless otherwise noted.</p> <p>Perspective 1 \u2014 Objective Clarity: - Are 3-7 crisp objectives defined? - Are non-goals explicitly listed? - Is the model scoping table present (modeled / simplified / omitted / justification)? - Are analysis questions specific enough to drive component selection?</p> <p>Perspective 2 \u2014 Concept Model Quality: - Is the concept model under 80 lines? - Does every building block have all 6 module contract aspects (observes, controls, owns, invariants, events, extension friction)? - Is real-system correspondence documented (llm-d / vLLM / SGLang mapping table)? - Is the state ownership map complete (exactly one owner per mutable state)?</p> <p>Perspective 3 \u2014 PR Decomposition: - Is every PR independently mergeable and testable? - Does the dependency DAG have no cycles? - Can module contracts be tested with mocks (parallel development enabled)? - Does each PR identify its extension type (policy template / subsystem module / backend swap / tier composition)?</p> <p>Perspective 4 \u2014 Abstraction Level: - Zero Go code in Sections A-F and H-K (only Section G may have frozen interface signatures)? - Are all pre-freeze interfaces described behaviorally, not as Go code? - Is every code snippet a FACT about merged code, not an ASPIRATION about unwritten code? - Are module contracts using the template from Phase 2, not Go structs?</p> <p>Perspective 5 \u2014 Risk Register: - Does every non-obvious architectural decision have a risk entry? - For decisions with cost-of-being-wrong &gt;= 3 PRs, is validation MANDATORY with a specific gate? - Does each validation gate have exact success criteria (not \"looks good\")? - Are abort plans specified (what changes if validation fails)?</p> <p>Perspective 6 \u2014 Cross-Cutting Infrastructure: - Are test infrastructure, documentation, and CI changes each assigned to a specific PR? - Is the interface freeze schedule documented (which PR freezes which interface)? - Is CLAUDE.md update ownership clear (the PR that causes the change updates it)? - Are no items left as \"address when needed\"?</p> <p>Perspective 7 \u2014 Extension Friction: - For each new module boundary, is the touch-point count for adding one more variant specified? - Are touch-point counts within reference targets from design guidelines Section 4.5? - If friction exceeds targets, is this acknowledged and justified?</p> <p>Perspective 8 \u2014 Design Bug Prevention: - Is scaffolding creep prevented (every struct/method/flag exercised by end of introducing PR)? - Is documentation drift prevented (CLAUDE.md updated in the same PR that causes the change)? - Is test infrastructure duplication prevented (shared packages created early)? - Is golden dataset staleness prevented (regeneration steps included)? - Are DES-specific anti-patterns addressed (type catalog, fidelity for its own sake, golden without invariant)?</p>"},{"location":"contributing/macro-planning/#quality-gates","title":"Quality Gates","text":"<ul> <li> Every PR in the plan is independently mergeable (no PR requires another PR's uncommitted code)</li> <li> Dependency DAG has no cycles</li> <li> Module contracts are testable with mocks (parallel development enabled)</li> <li> No Go struct definitions or method implementations (those belong in micro plans)</li> <li> Extension friction assessed for each new module boundary</li> </ul> <p>Automation</p> <p><code>/convergence-review macro-plan &lt;plan-path&gt;</code> dispatches all 8 perspectives and enforces convergence automatically. See Skills &amp; Plugins.</p>"},{"location":"contributing/macro-planning/#references","title":"References","text":"<ul> <li>Template: docs/contributing/templates/macro-plan.md</li> <li>Design guidelines: docs/contributing/templates/design-guidelines.md</li> <li>Convergence protocol: docs/contributing/convergence.md</li> <li>Standards: docs/contributing/standards/rules.md</li> </ul>"},{"location":"contributing/pr-workflow/","title":"PR Development Workflow","text":"<p>Status: Active (v4.0 \u2014 updated 2026-02-27)</p> <p>This document describes the complete workflow for implementing a PR from any source: a macro plan section, GitHub issues, a design document, or a feature request. The same steps apply whether you use Claude Code or standard git tools.</p>"},{"location":"contributing/pr-workflow/#overview","title":"Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 1: Create Isolated Workspace\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 2: Write Implementation Plan\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 2.5: Review the Plan        \u2502 (10 perspectives \u2192 convergence)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 3: Human Review             \u2502 (Approve plan)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 4: Implement the Plan       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 4.5: Review the Code        \u2502 (10 perspectives \u2192 convergence)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 4.75: Pre-Commit Self-Audit \u2502 (Critical thinking \u2014 no agent)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Step 5: Commit, Push, Create PR  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key insights: 1. Worktree isolation from start (Step 1) \u2014 Create worktree BEFORE any work. Main worktree never touched. Enables parallel work on multiple PRs. 2. Three-stage quality assurance:    - Plan Review (Step 2.5) \u2014 two-stage: holistic pre-pass, then 10 targeted perspectives. Catches design issues before implementation.    - Code Review (Step 4.5) \u2014 two-stage: holistic pre-pass, then 10 targeted perspectives. Catches implementation issues before PR creation.    - Self-Audit (Step 4.75) \u2014 deliberate critical thinking across 10 dimensions. Catches substance bugs that pattern-matching agents miss.</p>"},{"location":"contributing/pr-workflow/#step-by-step-process","title":"Step-by-Step Process","text":""},{"location":"contributing/pr-workflow/#step-1-create-an-isolated-workspace","title":"Step 1: Create an Isolated Workspace","text":"<p>Create a git worktree BEFORE any work begins:</p> <pre><code>git worktree add .worktrees/pr&lt;N&gt;-&lt;feature-name&gt; -b pr&lt;N&gt;-&lt;feature-name&gt;\ncd .worktrees/pr&lt;N&gt;-&lt;feature-name&gt;\n</code></pre> <p>Why first? This ensures: - Main worktree stays clean (no uncommitted plans or code) - Plan document committed on feature branch (not main) - Complete isolation for entire PR lifecycle (planning + implementation) - Ability to work on multiple PRs in parallel</p> <p>All remaining steps happen in the worktree.</p> <p>Automation</p> <p><code>/superpowers:using-git-worktrees pr&lt;N&gt;-&lt;feature-name&gt;</code> creates the worktree and switches your shell into it. Optional pre-cleanup: <code>/commit-commands:clean_gone</code> removes stale branches. See Skills &amp; Plugins.</p>"},{"location":"contributing/pr-workflow/#step-2-write-an-implementation-plan","title":"Step 2: Write an Implementation Plan","text":"<p>Write an implementation plan following the micro-plan template. The plan must include:</p> <ul> <li>Behavioral contracts (GIVEN/WHEN/THEN) defining what this PR guarantees</li> <li>TDD task breakdown (6\u201312 tasks, each: test \u2192 fail \u2192 implement \u2192 pass \u2192 lint \u2192 commit)</li> <li>Test strategy mapping contracts to specific tests</li> </ul> <p>Save the plan to <code>docs/plans/&lt;feature-name&gt;-plan.md</code>.</p> <p>The source of work can be a macro plan section, a design document, one or more GitHub issues, or a feature request.</p> <p>Automation</p> <p><code>/superpowers:writing-plans for &lt;work-item&gt; in @docs/plans/&lt;name&gt;-plan.md using @docs/contributing/templates/micro-plan-prompt.md and @&lt;source-document&gt;</code> generates the plan automatically. The skill reads the source document and the template, inspects the codebase, and produces behavioral contracts with executable tasks.</p>"},{"location":"contributing/pr-workflow/#step-25-review-the-plan","title":"Step 2.5: Review the Plan","text":"<p>Review the plan from 10 targeted perspectives, applying the convergence protocol: run all perspectives in parallel as one round; if zero CRITICAL and zero IMPORTANT across all reviewers, the round converged; otherwise fix and re-run the entire round. Max 10 rounds per gate. Hard gate \u2014 no exceptions. See convergence.md for full rules.</p> <p>Two-stage review:</p> <ol> <li>Holistic pre-pass: Do a single deep review to catch cross-cutting issues before the formal convergence protocol.</li> <li>Formal convergence: Run all 10 perspectives below in parallel.</li> </ol> <p>Automation</p> <p>Stage 1: <code>/pr-review-toolkit:review-pr</code>. Stage 2: <code>/convergence-review pr-plan docs/plans/&lt;name&gt;-plan.md</code>. See Skills &amp; Plugins.</p> <p>Why two stages? The holistic sweep catches emergent cross-cutting issues (the kind a human reviewer would spot). Fixing those first means the convergence review starts from a cleaner baseline \u2014 fewer rounds needed because obvious issues are already addressed.</p> <p>Why rounds with multiple perspectives? Generic \"review everything\" misses issues that targeted perspectives catch. Different lenses find different bugs: cross-doc consistency catches stale references, architecture catches boundary violations, substance catches design bugs. Running them in parallel maximizes coverage per round. The hypothesis process proved this model: 3 parallel reviewers with different foci caught issues that sequential single-reviewer rounds missed.</p>"},{"location":"contributing/pr-workflow/#perspective-1-substance-design","title":"Perspective 1: Substance &amp; Design","text":"<p>Check for: design bugs, mathematical errors, logical inconsistencies, scale mismatches, missing edge cases. Are the behavioral contracts logically sound? Could the design actually achieve what the contracts promise? Check formulas, thresholds, and edge cases from first principles \u2014 not just structural completeness.</p> <p>Catches: Design bugs, mathematical errors, logical inconsistencies, scale mismatches, missing edge cases.</p> <p>Why this perspective exists: In PR9, the fitness normalization formula (<code>1/(1+value)</code>) passed all structural checks but was a design bug (500,000x scale imbalance between throughput and latency). A substance-focused review caught what structure-focused reviews missed.</p>"},{"location":"contributing/pr-workflow/#perspective-2-cross-document-consistency","title":"Perspective 2: Cross-Document Consistency","text":"<p>Check for: scope mismatch between micro plan and source document, stale file paths, deviation log completeness. Does the deviation log account for all differences between what the source says and what the micro plan does? Check for stale references.</p> <p>Catches: Stale references, scope mismatch, missing deviations, wrong file paths.</p>"},{"location":"contributing/pr-workflow/#perspective-3-architecture-boundary-verification","title":"Perspective 3: Architecture Boundary Verification","text":"<p>Check for: import cycle risks, boundary violations (individual instances accessing cluster-level state), types in wrong packages, multiple construction sites for the same type, high touch-point multipliers (adding one field requires &gt;3 files), library code (<code>sim/</code>) calling <code>logrus.Fatalf</code>.</p> <p>Catches: Import cycle risks, boundary violations, missing bridge types, wrong abstraction level, construction site proliferation, high touch-point multipliers, error handling boundary violations.</p>"},{"location":"contributing/pr-workflow/#perspective-4-codebase-readiness","title":"Perspective 4: Codebase Readiness","text":"<p>Check for: stale comments (\"planned for PR N\" where N is completed), pre-existing bugs in files the plan will modify, missing dependencies, unclear insertion points, TODO/FIXME items in the modification zone.</p> <p>Catches: Stale comments, pre-existing bugs, missing dependencies, unclear insertion points.</p>"},{"location":"contributing/pr-workflow/#perspective-5-plan-structural-validation","title":"Perspective 5: Plan Structural Validation","text":"<p>Perform these 4 checks directly (no agent needed):</p> <p>Check 1: Task Dependencies \u2014 For each task, verify it can actually start given what comes before it. Trace the dependency chain: what files does each task create/modify? Does any task require a file or type that hasn't been created yet? Flag tasks that modify the same file and could conflict.</p> <p>Check 2: Template Completeness \u2014 Verify all sections from the micro-plan template are present and non-empty: Header, Part 1 (A\u2013E), Part 2 (F\u2013I), Part 3 (J), Appendix.</p> <p>Check 3: Executive Summary Clarity \u2014 Read the executive summary as if you're a new team member with no context. Is it clear what the PR does and why? Can you understand the scope without reading the rest of the plan?</p> <p>Check 4: Under-specified Tasks \u2014 For each task, verify it has complete code in every step (no \"add validation\" without showing exact code). Verify exact test commands with expected output. Verify exact commit commands. Flag any step that an executing agent would need to figure out on its own.</p> <p>Catches: Broken task ordering, missing template sections, unclear summaries, vague implementation steps that will cause agent confusion.</p>"},{"location":"contributing/pr-workflow/#perspective-6-des-expert","title":"Perspective 6: DES Expert","text":"<p>Check for: event ordering bugs, clock monotonicity violations, stale signal propagation between event types, heap priority errors, event-driven race conditions, work-conserving property violations, incorrect assumptions about DES event processing semantics. Verify that any new events respect the <code>(timestamp, priority, seqID)</code> ordering contract.</p> <p>Catches: Event ordering violations, clock regression, stale-signal bugs, priority inversion in event queues.</p>"},{"location":"contributing/pr-workflow/#perspective-7-vllmsglang-expert","title":"Perspective 7: vLLM/SGLang Expert","text":"<p>Check for: batching semantics that don't match real continuous-batching servers, KV cache eviction policies that differ from vLLM's implementation, chunked prefill behavior mismatches, preemption policy differences, missing scheduling features that real servers have. Flag any assumption about LLM serving that this plan gets wrong.</p> <p>Catches: Batching model inaccuracies, KV cache behavior mismatches, prefill/decode pipeline errors, scheduling assumption violations.</p>"},{"location":"contributing/pr-workflow/#perspective-8-distributed-inference-platform-expert","title":"Perspective 8: Distributed Inference Platform Expert","text":"<p>Check for: multi-instance coordination bugs, routing load imbalance under high request rates, stale snapshot propagation between instances, admission control edge cases at scale, horizontal scaling assumption violations, prefix-affinity routing correctness across instances.</p> <p>Catches: Load imbalance, stale routing state, admission control failures, scaling assumption violations, cross-instance coordination bugs.</p>"},{"location":"contributing/pr-workflow/#perspective-9-performance-scalability","title":"Perspective 9: Performance &amp; Scalability","text":"<p>Check for: algorithmic complexity issues (O(n\u00b2) where O(n) suffices), unnecessary allocations in hot paths, map iteration in O(n) loops that could grow, benchmark-sensitive changes, memory growth patterns, changes that would degrade performance at 1000+ requests or 10+ instances.</p> <p>Catches: Algorithmic complexity regressions, hot-path allocations, memory growth, scalability bottlenecks.</p>"},{"location":"contributing/pr-workflow/#perspective-10-security-robustness","title":"Perspective 10: Security &amp; Robustness","text":"<p>Check for: input validation completeness (all CLI flags, YAML fields, config values), panic paths reachable from user input, resource exhaustion vectors (unbounded loops, unlimited memory growth), degenerate input handling (empty, zero, negative, NaN, Inf), configuration injection risks.</p> <p>Catches: Input validation gaps, user-reachable panics, resource exhaustion, degenerate input failures, injection risks.</p>"},{"location":"contributing/pr-workflow/#step-3-human-review-of-plan","title":"Step 3: Human Review of Plan","text":"<p>Final human review of the plan (after automated review).</p> <p>Focus areas: 1. Part 1 (Design Validation) \u2014 Review behavioral contracts, component interaction, risks 2. Part 2 (Executable Tasks) \u2014 Verify task breakdown makes sense, no dead code 3. Deviation Log \u2014 Check if deviations from source document are justified 4. Appendix \u2014 Spot-check file-level details for accuracy</p> <p>Common issues to catch: - Behavioral contracts too vague or missing edge cases - Tasks not properly ordered (dependencies) - Missing test coverage for contracts - Deviations from source document not justified - Dead code or scaffolding</p> <p>Outcome: \u2705 Approve plan \u2192 proceed to Step 4 (implementation). \u274c Need revisions \u2192 iterate, re-review (Step 2.5), then approve.</p> <p>Note: The plan will be committed together with the implementation in Step 5 (single commit for entire PR).</p>"},{"location":"contributing/pr-workflow/#step-4-implement-the-plan","title":"Step 4: Implement the Plan","text":"<p>Implement the tasks from the approved plan using TDD:</p> <p>For each task: 1. Write the failing test 2. Run test to verify it fails 3. Implement minimal code to pass 4. Run test to verify it passes 5. Run lint: <code>golangci-lint run ./path/to/package/...</code> 6. Commit with contract reference</p> <p>Execute all tasks sequentially. Stop only on test failure, lint failure, or build error.</p> <p>Automation</p> <p><code>/superpowers:executing-plans @docs/plans/&lt;name&gt;-plan.md</code> executes all tasks continuously without pausing. On failure, use <code>/superpowers:systematic-debugging</code> for structured root-cause analysis.</p>"},{"location":"contributing/pr-workflow/#step-45-review-the-code","title":"Step 4.5: Review the Code","text":"<p>Review the implementation from 10 targeted perspectives, applying the convergence protocol: zero CRITICAL + zero IMPORTANT = converged; fix and re-run entire round otherwise. Max 10 rounds. Same two-stage structure as Step 2.5 (holistic pre-pass, then formal convergence), but the 10 perspectives differ: plan review checks design soundness; code review checks implementation quality.</p> <p>Two-stage review:</p> <ol> <li>Holistic pre-pass: Single deep review to catch cross-cutting issues.</li> <li>Formal convergence: Run all 10 perspectives below in parallel.</li> </ol> <p>Automation</p> <p>Stage 1: <code>/pr-review-toolkit:review-pr</code>. Stage 2: <code>/convergence-review pr-code</code>. See Skills &amp; Plugins.</p> <p>Why two stages? The holistic sweep catches emergent cross-cutting issues. In past PRs, this pre-pass found issues (runtime-breaking regressions, stale panic message prefixes) that individual targeted perspectives missed because they were each focused on their narrow lens. Fixing those first reduces convergence rounds.</p> <p>Why 10 perspectives in parallel? Each catches issues the others miss. In the standards-audit-hardening PR, Perspective 1 (substance) found a runtime-breaking regression, Perspective 3 (tests) found weakened coverage, Perspective 7 (vLLM expert) confirmed CLI validation matches real server semantics, and Perspective 10 (security) found pre-existing factory validation gaps. Domain-specific perspectives (DES, vLLM, distributed platform) catch issues that generic code-quality reviewers miss.</p>"},{"location":"contributing/pr-workflow/#perspective-1-substance-design_1","title":"Perspective 1: Substance &amp; Design","text":"<p>Check for: logic bugs, design mismatches between contracts and implementation, mathematical errors, silent regressions. Does the implementation actually achieve what the behavioral contracts promise? Check from first principles \u2014 not just structural patterns.</p> <p>Catches: Design bugs, formula errors, silent regressions, semantic mismatches between intent and implementation.</p>"},{"location":"contributing/pr-workflow/#perspective-2-code-quality-error-handling","title":"Perspective 2: Code Quality + Error Handling","text":"<p>Check for: (1) Any new error paths that use <code>continue</code> or early <code>return</code> \u2014 do they clean up partial state? (2) Any map iteration that accumulates floats \u2014 are keys sorted? (3) Any struct field added \u2014 are all construction sites updated? (4) Does library code (<code>sim/</code>) call <code>logrus.Fatalf</code> anywhere in new code? (5) Any exported mutable maps \u2014 should they be unexported with <code>IsValid*()</code> accessors? (6) Any YAML config fields using <code>float64</code> instead of <code>*float64</code> where zero is valid? (7) Any division where the denominator derives from runtime state without a zero guard? (8) Any new interface with methods only meaningful for one implementation? (9) Any method &gt;50 lines spanning multiple concerns (scheduling + latency + metrics)? (10) Any changes to <code>docs/contributing/standards/</code> files \u2014 are CLAUDE.md working copies updated to match?</p> <p>Catches: Logic errors, nil pointer risks, silent failures (discarded return values), panic paths reachable from user input, CLAUDE.md convention violations, dead code, silent <code>continue</code> data loss, non-deterministic map iteration, construction site drift, library code calling <code>os.Exit</code>, exported mutable maps, YAML zero-value ambiguity, division by zero in runtime computation, leaky interfaces, monolith methods, documentation drift.</p>"},{"location":"contributing/pr-workflow/#perspective-3-test-behavioral-quality","title":"Perspective 3: Test Behavioral Quality","text":"<p>Check for: Are all tests truly behavioral (testing WHAT, not HOW)? Rate each test as Behavioral, Mixed, or Structural. Would they survive a refactor? Are there golden dataset tests that lack companion invariant tests? Golden tests encode current behavior as \"correct\" \u2014 if the code had a bug when golden values were captured, the test perpetuates the bug. Flag golden tests whose expected values are not independently validated by an invariant test.</p> <p>Catches: Structural tests (Go struct assignment, trivial getters), type assertions in factory tests, exact-formula assertions instead of behavioral invariants, tests that pass even if the feature is broken, golden-only tests that would perpetuate pre-existing bugs (issue #183: codellama golden dataset encoded a silently-dropped request as the expected value since its initial commit).</p>"},{"location":"contributing/pr-workflow/#perspective-4-getting-started-experience","title":"Perspective 4: Getting-Started Experience","text":"<p>Check for: Simulate the journey of (1) a user doing capacity planning with the CLI, and (2) a contributor adding a new algorithm. Where would they get stuck? What's missing? Check for missing example files, undocumented output metrics, incomplete contributor guide, unclear extension points, README not updated for new features.</p> <p>Catches: Missing example files, undocumented output metrics, incomplete contributor guide, unclear extension points, README not updated for new features.</p>"},{"location":"contributing/pr-workflow/#perspective-5-automated-reviewer-simulation","title":"Perspective 5: Automated Reviewer Simulation","text":"<p>Check for: What GitHub Copilot, Claude, and Codex would flag. Exported mutable globals, user-controlled panic paths, YAML typo acceptance, NaN/Inf validation gaps, redundant code, style nits.</p> <p>Catches: Exported mutable globals, user-controlled panic paths, YAML typo acceptance, NaN/Inf validation gaps, redundant code, style nits.</p>"},{"location":"contributing/pr-workflow/#perspective-6-des-expert_1","title":"Perspective 6: DES Expert","text":"<p>Check for: event ordering bugs, clock monotonicity violations, stale signal propagation between event types, heap priority errors, event-driven race conditions, work-conserving property violations.</p> <p>Catches: Event ordering violations, clock regression, stale-signal bugs, work-conserving property gaps.</p>"},{"location":"contributing/pr-workflow/#perspective-7-vllmsglang-expert_1","title":"Perspective 7: vLLM/SGLang Expert","text":"<p>Check for: batching semantics that don't match real continuous-batching servers, KV cache eviction mismatches, chunked prefill behavior errors, preemption policy differences, missing scheduling features. Flag any assumption about LLM serving that this code gets wrong.</p> <p>Catches: Batching model inaccuracies, KV cache behavior mismatches, scheduling assumption violations.</p>"},{"location":"contributing/pr-workflow/#perspective-8-distributed-inference-platform-expert_1","title":"Perspective 8: Distributed Inference Platform Expert","text":"<p>Check for: multi-instance coordination bugs, routing load imbalance, stale snapshot propagation, admission control edge cases, horizontal scaling assumptions, prefix-affinity routing correctness.</p> <p>Catches: Load imbalance, stale routing state, scaling assumption violations, cross-instance bugs.</p>"},{"location":"contributing/pr-workflow/#perspective-9-performance-scalability_1","title":"Perspective 9: Performance &amp; Scalability","text":"<p>Check for: algorithmic complexity issues, unnecessary allocations in hot paths, map iteration in O(n) loops, benchmark-sensitive changes, memory growth patterns, changes that would degrade performance at 1000+ requests or 10+ instances.</p> <p>Catches: Complexity regressions, hot-path allocations, memory growth, scalability bottlenecks.</p>"},{"location":"contributing/pr-workflow/#perspective-10-security-robustness_1","title":"Perspective 10: Security &amp; Robustness","text":"<p>Check for: input validation completeness, panic paths reachable from user input, resource exhaustion vectors, degenerate input handling (empty, zero, NaN, Inf), configuration injection risks.</p> <p>Catches: Validation gaps, user-reachable panics, resource exhaustion, degenerate input failures.</p>"},{"location":"contributing/pr-workflow/#filing-pre-existing-issues","title":"Filing Pre-Existing Issues","text":"<p>Review passes naturally surface pre-existing bugs in surrounding code. These are valuable discoveries but outside the current PR's scope.</p> <p>Rule: File a GitHub issue immediately. Do not fix in the current PR.</p> <pre><code>gh issue create --title \"Bug: &lt;concise description&gt;\" --body \"&lt;location, impact, discovery context&gt;\" --label bug\n</code></pre> <p>Label guide: Use <code>bug</code> for code defects, <code>design</code> for design limitations, <code>enhancement</code> for feature gaps, <code>hardening</code> for correctness/invariant issues. Every issue must have at least one label \u2014 unlabeled issues are invisible in filtered views.</p> <p>After filing: Reference the issue number in the PR description under a \"Discovered Issues\" section so reviewers know it was found and tracked.</p> <p>Why not fix in-PR? - Scope creep \u2014 muddies the diff, makes review harder, risks introducing regressions in unrelated code - Attribution \u2014 the fix deserves its own tests and its own commit history - Tracking \u2014 issues that aren't filed are issues that are lost</p>"},{"location":"contributing/pr-workflow/#after-convergence-verification-gate","title":"After Convergence: Verification Gate","text":"<p>After fixing issues from all passes, run the verification gate to ensure all claims are backed by evidence:</p> <pre><code>go build ./...            # Build passes\ngo test ./... -count=1    # All tests pass (with counts)\ngolangci-lint run ./...   # Zero lint issues\ngit status                # Working tree status\n</code></pre> <p>Report: build exit code, test pass/fail counts, lint issue count, working tree status. Wait for user approval before proceeding.</p> <p>Why a gate instead of informal checking? In PR9, the manual \"run these commands\" instruction was easy to skip or half-execute. Making verification a formal gate with expected output makes it non-optional and evidence-based.</p> <p>Automation</p> <p><code>/superpowers:verification-before-completion</code> enforces running these commands and confirming output before making any success claims.</p>"},{"location":"contributing/pr-workflow/#step-475-pre-commit-self-audit","title":"Step 4.75: Pre-Commit Self-Audit","text":"<p>Stop, think critically, and answer each question below from your own reasoning. Do not delegate to automated tools \u2014 review each dimension yourself using critical thinking. Report all issues found. If you find zero issues, explain why you're confident for each dimension.</p> <p>Why this step exists: In PR9, the 4-perspective automated code review (Step 4.5) found 0 new issues in the final perspective. Then the user asked \"are you confident?\" and Claude found 3 real bugs by thinking critically: a wrong reference scale for token throughput normalization, non-deterministic map iteration in output, and inconsistent comment patterns. Automated review perspectives check structure; this step checks substance.</p> <p>Self-audit dimensions \u2014 think through each one:</p> <ol> <li>Logic bugs: Trace through the core algorithm mentally. Are there edge cases where the math breaks? Division by zero? Off-by-one? Wrong comparisons?</li> <li>Design bugs: Does the design actually achieve what the contracts promise? Would a user get the expected behavior? Are there scale mismatches, unit confusions, or semantic errors?</li> <li>Determinism (R2, INV-6): Is all output deterministic? Any map iteration used for ordered output? Any floating-point accumulation order dependencies?</li> <li>Consistency: Are naming patterns consistent across all changed files? Do comments match code? Do doc strings match implementations? Are there stale references?</li> <li>Documentation: Would a new user find everything they need? Would a contributor know how to extend this? Are CLI flags documented everywhere (CLAUDE.md, README, <code>--help</code>)?</li> <li>Defensive edge cases: What happens with zero input? Empty collections? Maximum values? What if the user passes unusual but valid flag combinations?</li> <li>Test epistemology (R7, R12): For every test that compares against a golden value, ask: \"How do I know this expected value is correct?\" If the answer is \"because the code produced it,\" that test catches regressions but not pre-existing bugs. Verify a corresponding invariant test validates the result from first principles. (See issue #183: a golden test perpetuated a silently-dropped request for months.)</li> <li>Construction site uniqueness (R4): Does this PR add fields to existing structs? If so, are ALL construction sites updated? Grep for <code>StructName{</code> across the codebase. Are there canonical constructors, or are structs built inline in multiple places?</li> <li>Error path completeness (R1, R5): For every error/failure path in new code, what happens to partially-mutated state? Does every <code>continue</code> or early <code>return</code> clean up what was started? Is there a counter or log so the failure is observable?</li> <li>Documentation DRY (source-of-truth map): Does this PR modify content that exists as a working copy elsewhere? Check the source-of-truth map in <code>docs/contributing/standards/principles.md</code>. If a canonical source was updated (rules.md, invariants.md, principles.md, extension-recipes.md), verify all working copies listed in the map are also updated. If a new file or section was added, verify it appears in the File Organization tree. If a hypothesis experiment was completed, verify <code>hypotheses/README.md</code> is updated.</li> </ol> <p>Why no agent? Agents are good at pattern-matching (finding style violations, checking structure). They're bad at stepping back and asking \"does this actually make sense?\" That requires the kind of critical thinking that only happens when you deliberately pause and reflect.</p> <p>Fix all issues found. Then wait for user approval before Step 5.</p>"},{"location":"contributing/pr-workflow/#step-5-commit-push-and-create-pr","title":"Step 5: Commit, Push, and Create PR","text":"<p>Stage your changes, commit, push, and create a PR:</p> <pre><code>git add &lt;files&gt;\ngit commit -m \"feat(scope): &lt;description&gt;\n\n- Implement BC-1: &lt;brief description&gt;\n- Implement BC-2: &lt;brief description&gt;\n\nCo-Authored-By: Claude &lt;noreply@anthropic.com&gt;\"\ngit push -u origin &lt;branch-name&gt;\ngh pr create --title \"&lt;title&gt;\" --body \"&lt;description with behavioral contracts&gt;\"\n</code></pre> <p>The PR description should include a summary, behavioral contracts (GIVEN/WHEN/THEN), testing verification, and GitHub closing keywords from the plan's <code>Closes:</code> field (e.g., <code>Fixes #183, fixes #189</code>).</p> <p>Automation</p> <p><code>/commit-commands:commit-push-pr</code> handles staging, committing, pushing, and PR creation in one command. It analyzes current git state, creates an appropriate commit message referencing behavioral contracts, and opens the PR.</p>"},{"location":"contributing/pr-workflow/#workflow-variants","title":"Workflow Variants","text":""},{"location":"contributing/pr-workflow/#subagent-driven-development-in-session","title":"Subagent-Driven Development (In-Session)","text":"<p>Alternative to Step 4 for simpler PRs where you want tighter iteration. Executes in the current session with a fresh subagent per task and immediate code review after each task.</p> <p>Trade-offs: \u2705 Faster for simple PRs (no session switching), better for iterative refinement. \u26a0\ufe0f Uses current session's context (can grow large), review after every task (vs continuous execution).</p> <p>Automation</p> <p>Use the <code>superpowers:subagent-driven-development</code> skill to implement the plan with fresh subagent per task.</p>"},{"location":"contributing/pr-workflow/#pr-size-tiers","title":"PR Size Tiers","text":"<p>Not all PRs need the same level of review. Use these objective criteria:</p> Tier Criteria Plan Review (Step 2.5) Code Review (Step 4.5) Self-Audit (Step 4.75) Small Docs-only with no process/workflow semantic changes (typo fixes, formatting, comment updates, link fixes), OR \u22643 files changed AND only mechanical changes (renames, formatting) AND no behavioral logic changes AND no new interfaces/types AND no new CLI flags Skip convergence review; single pre-pass sufficient Skip convergence review; single pre-pass sufficient Full (all 10 dimensions) Medium 4\u201310 files changed, OR new policy template behind existing interface Full two-stage (pre-pass + convergence) Full two-stage (pre-pass + convergence) Full (all 10 dimensions) Large &gt;10 files, OR new interfaces/modules, OR architecture changes Full two-stage (pre-pass + convergence) Full two-stage (pre-pass + convergence) Full (all 10 dimensions) <p>Rules: - Steps 1, 2, 3, 4, 5 are always required \u2014 worktree, plan, human review, execution, and commit apply to all tiers. - Self-audit is always full \u2014 the 10-dimension critical thinking check catches substance bugs that no automated review can. It costs 5 minutes and has caught 3+ real bugs in every PR where it was applied. - When in doubt, tier up \u2014 if you're unsure whether a change is Small or Medium, use Medium. - Human reviewer can override \u2014 if the human reviewer at Step 3 believes the tier is wrong, they can request a different tier.</p>"},{"location":"contributing/pr-workflow/#example-walkthrough","title":"Example Walkthrough","text":"<p>A typical PR from a macro plan section:</p> <ol> <li>Create worktree: <code>git worktree add .worktrees/pr8-routing -b pr8-routing &amp;&amp; cd .worktrees/pr8-routing</code></li> <li>Write plan: Follow micro-plan template, save to <code>docs/plans/pr8-routing-plan.md</code></li> <li>Review plan: Run all 10 perspectives (Stage 1 pre-pass, then Stage 2 convergence). Fix issues, re-run until converged.</li> <li>Human review: Read plan \u2014 contracts, tasks, appendix. Approve to proceed.</li> <li>Implement: Execute TDD tasks: test \u2192 fail \u2192 implement \u2192 pass \u2192 lint \u2192 commit.</li> <li>Review code: Same two-stage review as plan. Fix issues, re-run until converged. Run verification gate.</li> <li>Self-audit: Think through all 10 dimensions. Fix issues.</li> <li>Commit + PR: Push branch, create PR with closing keywords.</li> </ol> <p>The workflow is the same regardless of source (macro plan, design doc, GitHub issues). Only the source document passed to the planning step differs.</p>"},{"location":"contributing/pr-workflow/#tips-for-success","title":"Tips for Success","text":"<ol> <li>Use automated reviews proactively \u2014 run reviews after plan creation and after implementation (don't wait for human review to catch issues)</li> <li>Fix critical issues immediately \u2014 don't proceed with known critical issues (they compound)</li> <li>Re-run targeted reviews after fixes \u2014 verify fixes worked</li> <li>Use worktrees for complex PRs \u2014 avoid disrupting main workspace</li> <li>Review after execution \u2014 use automated code review (Step 4.5) after all tasks complete</li> <li>Reference contracts in commits \u2014 makes review easier and more traceable</li> <li>Update CLAUDE.md immediately \u2014 don't defer documentation</li> <li>Keep source documents updated \u2014 mark PRs as completed in macro plan; close resolved issues</li> <li>Don't trust automated passes alone \u2014 the self-audit (Step 4.75) catches substance bugs that pattern-matching agents miss. In PR9, 3 real bugs were found by critical thinking after 4 automated passes found 0 issues.</li> <li>Checkpoint long sessions \u2014 for PRs with 8+ tasks or multi-round reviews, write a checkpoint summary to <code>.claude/checkpoint.md</code> after each major phase. If you hit context limits, read the checkpoint first.</li> </ol>"},{"location":"contributing/pr-workflow/#headless-mode-for-reviews-context-overflow-workaround","title":"Headless Mode for Reviews (Context Overflow Workaround)","text":"<p>If multi-agent review passes hit context overflow during consolidation, run each review as an isolated invocation that writes findings to a file, then consolidate in a lightweight final pass.</p> <p>Requires Claude Code</p> <p>This workaround uses the <code>claude</code> CLI tool. Contributors without Claude Code can run reviews manually using the perspective checklists in Steps 2.5 and 4.5.</p> <pre><code>#!/bin/bash\n# headless-review.sh \u2014 Run review agents with full context each\nBRANCH=$(git branch --show-current)\nmkdir -p .review\n\n# Run each pass in its own context (no overflow)\nclaude -p \"Pass 1: Code quality review of branch $BRANCH. Write findings to .review/01-code-quality.md\" \\\n  --allowedTools \"Read,Grep,Glob,Bash\" &amp;\nclaude -p \"Pass 2: Test behavioral quality review. Write findings to .review/02-test-quality.md\" \\\n  --allowedTools \"Read,Grep,Glob,Bash\" &amp;\nclaude -p \"Pass 3: Getting-started review. Write findings to .review/03-getting-started.md\" \\\n  --allowedTools \"Read,Grep,Glob,Bash\" &amp;\nwait\n\n# Lightweight consolidation\nclaude -p \"Read .review/*.md files. Produce a consolidated summary sorted by severity.\" \\\n  --allowedTools \"Read,Glob\"\n</code></pre> <p>When to use: When Step 2.5 or Step 4.5 hits context limits. Not needed for most PRs.</p>"},{"location":"contributing/pr-workflow/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"contributing/pr-workflow/#issue-plan-too-generic-agents-ask-clarifying-questions","title":"Issue: Plan too generic, agents ask clarifying questions","text":"<p>Solution: Add specific guidance in the invocation. Claude reads the full source document and extracts context automatically. If still too generic, add explicit notes about integration points.</p>"},{"location":"contributing/pr-workflow/#issue-tasks-miss-behavioral-contracts-during-execution","title":"Issue: Tasks miss behavioral contracts during execution","text":"<p>Solution: After execution completes, verify all contracts are tested: \"Confirm all contracts are tested: BC-1: Show test results. BC-2: Show test results.\"</p>"},{"location":"contributing/pr-workflow/#issue-lint-fails-at-the-end-with-many-issues","title":"Issue: Lint fails at the end with many issues","text":"<p>Solution: Ensure lint runs in each task step: <code>golangci-lint run ./path/to/modified/package/...</code></p>"},{"location":"contributing/pr-workflow/#issue-dead-code-introduced-unused-functions-fields","title":"Issue: Dead code introduced (unused functions, fields)","text":"<p>Solution: In Step 3 plan review, check every struct field is used by end of task or a later task. Every method called by tests or production code. Every parameter actually needed.</p>"},{"location":"contributing/pr-workflow/#issue-review-finds-many-critical-issues-overwhelming-to-fix","title":"Issue: Review finds many critical issues, overwhelming to fix","text":"<p>Solution: Fix issues in priority order: (1) Fix all critical, re-run. (2) Fix important, re-run. (3) Consider suggestions. Use targeted review after fixes.</p>"},{"location":"contributing/pr-workflow/#issue-uncertain-if-review-findings-are-valid","title":"Issue: Uncertain if review findings are valid","text":"<p>Solution: Review agents provide file:line references. Check the specific code location. If uncertain, ask Claude to explain. If agent is wrong, document why and proceed.</p> Appendix: Workflow Evolution  **v1.0 (pre-2026-02-14):** Manual agent team prompts, separate design/execution plans **v2.0 (2026-02-14):** Unified planning with `writing-plans` skill, batch execution with `executing-plans` skill, automated two-stage review with `pr-review-toolkit:review-pr`, simplified invocations with @ file references **v2.1 (2026-02-16):** Same-session worktree workflow (project-local `.worktrees/` no longer requires new session); continuous execution replaces batch checkpoints (tasks run without pausing, stop only on failure) **v2.2 (2026-02-16):** Focused review passes replace generic review-pr invocations. Step 2.5 expanded to 3 passes (cross-doc consistency, architecture boundary, codebase readiness). Step 4.5 expanded to 4 passes (code quality, test behavioral quality, getting-started experience, automated reviewer simulation). Based on PR8 experience where each focused pass caught issues the others missed. **v2.3 (2026-02-16):** Step 2.5 expanded to 4 passes \u2014 added Pass 4 (structural validation: task dependencies, template completeness, executive summary clarity, under-specified task detection). Based on PR9 experience where deferred items fell through cracks in the macro plan, and an under-specified documentation task would have confused the executing agent. **v2.4 (2026-02-16):** Four targeted skill integrations addressing real failure modes: (1) `review-plan` as Pass 0 in Step 2.5 \u2014 external LLM review catches design bugs that self-review misses (PR9: fitness normalization bug passed 3 focused passes). (2) `superpowers:systematic-debugging` as on-failure handler in Step 4 \u2014 structured root-cause analysis instead of ad-hoc debugging. (3) `superpowers:verification-before-completion` replaces manual verification prose after Step 4.5 \u2014 makes build/test/lint gate non-skippable. (4) `commit-commands:clean_gone` as pre-cleanup in Step 1 \u2014 prevents stale branch accumulation. **v2.5 (2026-02-16):** Three additions from `/insights` analysis of 212 sessions: (1) Step 4.75 (pre-commit self-audit) \u2014 deliberate critical thinking step with no agent, checking logic/design/determinism/consistency/docs/edge-cases. In PR9, this step found 3 real bugs that 4 automated passes missed. (2) Headless mode documentation for review passes. (3) Checkpointing tip for long sessions. **v2.6 (2026-02-18):** Two additions: (1) \"Filing Pre-Existing Issues\" subsection to Step 4.5. (2) Antipattern prevention from hardening audit \u2014 Step 4.75 expanded to 9 self-audit dimensions; Step 4.5 Pass 1 prompt expanded with 4 antipattern checks. **v2.7 (2026-02-18):** Generalized workflow from \"macro plan only\" to any source document (macro plan sections, design docs, GitHub issues, feature requests). **v2.8 (2026-02-18):** Auto-close issues on PR merge. Added `Closes:` field to micro plan header template. **v2.9 (2026-02-20):** Convergence re-run protocol for both Step 2.5 and Step 4.5. **v3.0 (2026-02-23):** Multi-perspective rounds replace sequential passes. External LLM review removed. Convergence redefined as property of a clean round. **v4.0 (2026-02-27):** Human-first rewrite (#464). Steps describe human actions; skills in admonition callouts. Manual path is primary; automation is additive. Templates split into human-readable format descriptions + agent prompt companions."},{"location":"contributing/standards/experiments/","title":"BLIS Experiment Standards","text":"<p>Hypothesis-driven experimentation is a first-class activity in BLIS \u2014 equal in rigor to implementation and design. The experiment framework is grounded in Verification, Validation, and Uncertainty Quantification (VV&amp;UQ) from simulation science.</p>"},{"location":"contributing/standards/experiments/#vvuq-framing","title":"VV&amp;UQ Framing","text":"<p>Every hypothesis falls into one of three VV&amp;UQ categories. This determines what kind of evidence is needed.</p> Category Question Evidence required Examples Verification Does the code implement the intended math/logic? Exact invariant checks. Failure = bug. H12 (conservation), H13 (determinism), H22 (input validation) Validation Does the model match expected system behavior? Statistical comparison against analytical baselines or real data within a pre-specified accuracy interval. Cross-validation against M/M/k; H19 (roofline vs blackbox) Uncertainty Quantification How confident are we in the region where a finding holds? Confidence intervals on thresholds; probability statements on properties. H8 (preemption cliff at 2100\u00b1? blocks); H10 (28% improvement at this operating point \u2014 what about others?) <p>Most current experiments are Verification (invariant checking) or informal Validation (metric comparison). Future experiments should increasingly incorporate UQ \u2014 every threshold finding should include a confidence interval, every \"confirmed\" result should quantify the probability of holding under parameter variation.</p>"},{"location":"contributing/standards/experiments/#purposes","title":"Purposes","text":"<p>The VV&amp;UQ categories above classify what kind of evidence an experiment needs. But experiments also serve broader purposes beyond their category:</p> <ul> <li>Verification and Validation \u2014 the primary purposes, aligned with the VV&amp;UQ categories above</li> <li>Discovery \u2014 a secondary outcome of any experiment. Bugs, design gaps, and undocumented limitations often surface during verification or validation experiments. Discovery is not a separate VV&amp;UQ category \u2014 it's a valuable byproduct. Example: H5 (a Validation experiment) discovered that the per-input-token cost model makes burst smoothing structurally impossible \u2014 a design limitation finding that was not the hypothesis being tested.</li> </ul>"},{"location":"contributing/standards/experiments/#how-to-choose-your-vvuq-category","title":"How to choose your VV&amp;UQ category","text":"<pre><code>Is your hypothesis about whether the CODE is correct?\n  \u2192 Yes: Verification (e.g., \"conservation holds,\" \"deterministic output\")\n  \u2192 No: \u2193\nIs your hypothesis comparing the MODEL's output to expected behavior?\n  \u2192 Yes: Validation (e.g., \"policy A beats B,\" \"TTFT \u221d input tokens\")\n  \u2192 No: \u2193\nIs your hypothesis about the BOUNDARIES or CONFIDENCE of a finding?\n  \u2192 Yes: UQ (e.g., \"preemption cliff at 2100\u00b1100 blocks,\" \"P(stable) &gt; 0.95\")\n</code></pre> <p>The VV&amp;UQ category determines what counts as evidence: - Verification: Exact invariant checks. One failure = bug. Single seed sufficient. - Validation: Statistical comparison within pre-specified accuracy interval. 3+ seeds. Formal tests (KS, Mann-Whitney U). - UQ: Confidence intervals on thresholds. Parameter sweeps. Sensitivity analysis.</p>"},{"location":"contributing/standards/experiments/#formal-statistical-rigor","title":"Formal statistical rigor","text":"<p>Experiments involving statistical claims must use proper hypothesis tests, not ad-hoc thresholds:</p> <ul> <li>Distribution validation (workload/arrival family): Kolmogorov-Smirnov test \u2014 compares a sample against a theoretical CDF. Reject if p &lt; 0.05. In Python: <code>from scipy.stats import kstest; stat, p = kstest(samples, 'expon', args=(0, 1/rate))</code>.</li> <li>Metric comparison (cross-policy family): Mann-Whitney U test \u2014 non-parametric comparison of two independent samples. Report effect size AND confidence interval, not just \"X% better.\" In Python: <code>from scipy.stats import mannwhitneyu; stat, p = mannwhitneyu(a_values, b_values)</code>.</li> <li>Threshold estimation (performance-regime family): Report thresholds with confidence intervals. \"Preemption cliff at 2100 blocks\" \u2192 \"Preemption cliff at 2100 \u00b1 100 blocks (95% CI across seeds 42, 123, 456).\"</li> <li>Invariant probability (scheduler invariants family): For stochastic invariants, estimate P(invariant holds) with a confidence interval, not just \"holds for 3 seeds.\"</li> </ul> <p>Note on scipy: The tests above use <code>scipy.stats</code>. Install with <code>pip install scipy</code> if needed. For experiments that only use standard-library Python, the legacy thresholds (below) remain acceptable.</p> <p>Legacy thresholds (still valid for experiments without scipy): - &gt;20% improvement consistent across all seeds = significant - &lt;10% in any seed = inconclusive - Within 5% across all seeds = equivalent (for equivalence tests)</p> <p>These thresholds were chosen pragmatically \u2014 20% ensures the effect is visible above seed-to-seed variance in typical BLIS experiments; 5% accounts for floating-point and timing noise. They are not derived from formal power analysis. New experiments should prefer formal tests where scipy is available.</p>"},{"location":"contributing/standards/experiments/#cross-validation-against-analytical-models","title":"Cross-validation against analytical models","text":"<p>Where applicable, validate DES outputs against analytically-tractable models under matching assumptions. This grounds the simulator in theory.</p> <ul> <li>M/M/k baseline: M/M/k is the standard queueing model with Markovian (Poisson) arrivals, Markovian (exponential) service times, and k servers. Under matching assumptions, compare DES queue length distribution against the M/M/k analytical solution. Caveat: BLIS uses batching and deterministic service times (alpha/beta coefficients), so exact M/M/k matching is not possible. The comparison requires configuring BLIS with <code>--max-batch-size 1</code> and interpreting the service time distribution as approximately exponential. Divergence may indicate modeling errors OR fundamental architectural differences from M/M/k assumptions.</li> <li>Little's Law: For any stable configuration, verify L = \u03bbW (average queue length = arrival rate \u00d7 average wait time). This is a universal law that must hold. In BLIS terms: L = mean <code>still_queued</code> from per-instance metrics; \u03bb = <code>injected_requests / (sim_duration_us / 1e6)</code>; W = mean scheduling delay from <code>scheduling_delay_p99_ms</code> (approximate). Extract from JSON output.</li> <li>Phase structure: Verify that prefill time \u221d prompt tokens and decode time \u221d output tokens by fitting linear models and checking R\u00b2 &gt; 0.95. In BLIS terms: prefill time \u2248 TTFT (time to first token); decode time \u2248 E2E - TTFT. Vary <code>input_distribution</code> mean while holding <code>output_distribution</code> constant, and vice versa.</li> </ul>"},{"location":"contributing/standards/experiments/#experiment-classification","title":"Experiment Classification","text":"<p>Every hypothesis must be classified before designing the experiment. The classification determines rigor requirements.</p>"},{"location":"contributing/standards/experiments/#type-1-deterministic-experiments","title":"Type 1: Deterministic Experiments","text":"<p>Definition: Verify exact properties \u2014 invariants, conservation laws, error handling boundaries. Same seed = same result, guaranteed.</p> <p>Requirements: - Single seed sufficient (determinism is the point) - Pass/fail is exact \u2014 the invariant holds or it doesn't - Failure is ALWAYS a bug (never noise) - No statistical analysis needed</p> <p>Examples: - H12: Request conservation (INV-1) holds across 10 policy configurations (67 checks) - H13: Same seed produces byte-identical output - H22: Zero KV blocks panics at CLI boundary, not deep in simulation</p> <p>Pass criteria: The invariant holds for every configuration tested. One failure = bug.</p>"},{"location":"contributing/standards/experiments/#type-2-statistical-experiments","title":"Type 2: Statistical Experiments","text":"<p>Definition: Compare metrics (TTFT, throughput, distribution uniformity) across configurations. Results vary by seed.</p> <p>Requirements: - Minimum 3 seeds (42, 123, 456) for each configuration - Effect size thresholds:   - Significant: &gt;20% improvement consistent across ALL seeds   - Inconclusive: &lt;10% in any seed   - Equivalent: within 5% across all seeds (for equivalence tests) - Directional consistency: the predicted direction must hold across ALL seeds. One contradicting seed = hypothesis not confirmed - Report: mean, min, max across seeds for primary metric. Include per-seed values for transparency.</p> <p>Subtypes:</p>"},{"location":"contributing/standards/experiments/#dominance","title":"Dominance","text":"<p>A is strictly better than B on metric M.</p> <ul> <li>Analysis: Compare metric M for A vs B across all seeds. Compute ratio per seed.</li> <li>Pass: A beats B on M for all seeds, with &gt;20% effect size in every seed.</li> <li>Examples: H3 \u2014 queue-depth TTFT is 1.7-2.8x better than kv-utilization across 3 seeds. H14 \u2014 <code>always-busiest</code> routing produces 4.6x worse TTFT and routes all 500 requests to a single instance.</li> </ul>"},{"location":"contributing/standards/experiments/#monotonicity","title":"Monotonicity","text":"<p>Increasing X should monotonically increase/decrease Y.</p> <ul> <li>Analysis: Run at &gt;=3 values of X. Verify Y changes monotonically.</li> <li>Pass: Y is strictly monotonic in X across all seeds. No inversions.</li> <li>Example: H8 \u2014 reducing total KV blocks increases preemption frequency. H9 \u2014 increasing prefix_length decreases TTFT.</li> </ul>"},{"location":"contributing/standards/experiments/#equivalence","title":"Equivalence","text":"<p>A ~ B within tolerance (baseline sanity checks).</p> <ul> <li>Analysis: Compare metric M for A vs B. Compute percentage difference per seed.</li> <li>Pass: |A - B| / max(A, B) &lt; 5% across all seeds.</li> <li>Example: H4 \u2014 round-robin ~ least-loaded for uniform workloads at low rates. H23 \u2014 all policies equivalent at near-zero load.</li> </ul>"},{"location":"contributing/standards/experiments/#pareto","title":"Pareto","text":"<p>No single configuration dominates all metrics simultaneously.</p> <ul> <li>Analysis: Run N configurations, measure multiple metrics. Identify Pareto-optimal set.</li> <li>Pass: At least 2 configurations are Pareto-optimal (each best on &gt;=1 metric).</li> <li>Example: H17 \u2014 different scorer weights optimize for different objectives (TTFT vs throughput).</li> </ul>"},{"location":"contributing/standards/experiments/#hypothesis-formation","title":"Hypothesis Formation","text":"<p>Hypotheses must be conceptual and behavioral, not code-grounded. This is the experimental analogue of behavioral vs structural testing.</p>"},{"location":"contributing/standards/experiments/#conceptual-hypotheses-test-system-behavior","title":"Conceptual hypotheses test system behavior","text":"<p>A good hypothesis is an intuitive claim about system behavior: \"burst smoothing should reduce tail latency,\" \"tiered storage should reduce preemptions,\" \"same seed should produce identical output.\" These claims are based on systems thinking, not on reading the implementation.</p>"},{"location":"contributing/standards/experiments/#do-not-read-the-code-before-forming-hypotheses","title":"Do NOT read the code before forming hypotheses","text":"<p>Reading the code before hypothesizing is like writing structural tests \u2014 you end up testing the implementation, not the behavior. The value of hypothesis-driven experimentation is that conceptual claims failing against the implementation surfaces design limitations that code-aware experiments would avoid.</p> <p>Evidence: If H5 had read <code>admission.go:45</code> before hypothesizing, the experimenter would have designed a \"correct\" experiment with cap=100K, confirmed a tiny effect, and missed the discovery that the per-input-token cost model makes burst smoothing structurally impossible at practical parameters. The conceptual hypothesis exposed a design limitation that a code-grounded hypothesis would have sidestepped.</p>"},{"location":"contributing/standards/experiments/#mechanism-not-plausible-is-a-valid-resolution","title":"\"Mechanism not plausible\" is a valid resolution","text":"<p>When a conceptual hypothesis fails because the implementation doesn't support the assumed mechanism, this is the resolution \"Refuted \u2014 mechanism not plausible.\" This is a design limitation finding, not an experimenter error. The hypothesis did its job \u2014 it revealed a gap between how users think the system works and how it actually works.</p>"},{"location":"contributing/standards/experiments/#hypothesis-families","title":"Hypothesis Families","text":"<p>Every hypothesis belongs to a family (what domain is being tested) AND a type (how rigor is assessed). These are orthogonal \u2014 a scheduler invariant can be deterministic or statistical; a cross-policy comparison is always statistical.</p>"},{"location":"contributing/standards/experiments/#the-six-families","title":"The six families","text":"Family Tests Hypothesis shape Typical type Examples Workload/arrival Input generation: distributions, rates, burstiness, mix proportions \"Generator X produces arrivals matching distribution D within tolerance T\" Statistical H16, H20 Scheduler invariants (safety/liveness) Conservation, determinism, lifecycle, livelock protection \"For ALL configurations, property P holds\" (universally quantified) Deterministic H12 (conservation), H13 (determinism), H25 Performance-regime (scaling laws) Saturation curves, throughput-latency tradeoffs, horizontal scaling \"Metric M is monotonic/convex in parameter P\" Statistical/Monotonicity H7 (scaling), H8 (KV pressure), H11 (batch formation) Structural model DES model assumptions: phase structure, KV mechanics, signal freshness, prefix caching \"Component C behaves according to model assumption A\" Mixed H3 (signal freshness), H9 (prefix caching), H10 (tiered KV), H26 Robustness/failure-mode Overload, misconfiguration, degenerate inputs, pathological policies \"Under stress condition S, the system exhibits defined behavior B (not undefined state)\" Deterministic or Statistical H5 (token-bucket), H14 (pathological), H21, H22, H24 Cross-policy comparative Policy ordering, Pareto frontiers, robustness to workload shifts \"There EXISTS a workload where policy A beats B on metric M\" (existentially quantified) Statistical/Dominance or Pareto H1, H2, H4, H6, H15, H17, H18, H19, H23"},{"location":"contributing/standards/experiments/#family-specific-hypothesis-sentence-patterns","title":"Family-specific hypothesis sentence patterns","text":"<p>Use these templates when generating new hypotheses. Each family has a characteristic sentence shape that ensures testability. See also <code>docs/contributing/hypothesis.md</code> for the full generation guide.</p> Family Sentence pattern Example Workload/arrival \"Generator G with parameters P should produce distribution D with property X within tolerance T\" \"Gamma sampler with CV=3.5 should produce inter-arrival times with CV within 10% of 3.5 over 10K samples\" Scheduler invariants \"For ALL configurations C, invariant I holds at simulation end\" \"For all routing \u00d7 scheduling \u00d7 admission combinations, injected == completed + queued + running\" Performance-regime \"Metric M should be monotonically non-decreasing/non-increasing in parameter P across range [a, b]\" \"TTFT P99 should be monotonically non-decreasing in offered load from 500 to 5000 req/s\" Structural model \"Component C should behave according to assumption A, verified by observable O\" \"Prefill time should be proportional to input token count (R\u00b2 &gt; 0.95 for linear fit)\" Robustness \"Under stress condition S, the system should exhibit behavior B and NOT exhibit behavior X\" \"Under 10x overload, the system should reject excess requests and NOT deadlock or panic\" Cross-policy \"Under workload W, policy A should produce better metric M than policy B because of mechanism Z\" \"Under mixed-SLO workload, priority-FCFS should produce lower realtime TTFT than FCFS because realtime requests get scheduled first\""},{"location":"contributing/standards/experiments/#family-type-matrix","title":"Family \u00d7 Type matrix","text":"Deterministic Statistical/Dominance Statistical/Monotonicity Statistical/Equivalence Statistical/Pareto Workload/arrival Seed reproducibility Distribution match Rate scaling \u2014 \u2014 Scheduler invariants Primary (INV-1 through INV-6) \u2014 \u2014 \u2014 \u2014 Performance-regime \u2014 \u2014 Primary (scaling curves) Baseline sanity Knee behavior Structural model Phase structure Signal freshness Cache effectiveness \u2014 \u2014 Robustness Input validation Overload behavior \u2014 \u2014 \u2014 Cross-policy \u2014 Primary (A vs B) \u2014 Low-load equivalence Multi-scorer tradeoffs"},{"location":"contributing/standards/experiments/#family-determines-rigor-requirements","title":"Family determines rigor requirements","text":"<ul> <li>Scheduler invariants: Single seed sufficient. Pass/fail is exact. One failure = bug.</li> <li>Cross-policy comparative: 3+ seeds minimum. Must control confounding variables (ED-1, ED-6).</li> <li>Performance-regime: Sweep points (\u22653 values of the independent variable), not just pairwise comparison.</li> <li>Workload/arrival: Statistical tests on generated distributions. Long runs for accurate rate estimation.</li> <li>Structural model: Code-level verification (RCV-1, RCV-4) is essential \u2014 these test implementation assumptions.</li> <li>Robustness: Must test BOTH the defined behavior AND verify no undefined states (deadlock, panic, data loss).</li> </ul>"},{"location":"contributing/standards/experiments/#relationship-to-existing-invariants-and-rules","title":"Relationship to existing invariants and rules","text":"Family Related invariants Related rules Scheduler invariants INV-1 (conservation), INV-2 (lifecycle), INV-3 (clock monotonicity), INV-5 (causality), INV-6 (determinism) R1 (no silent data loss), R5 (transactional mutation) Structural model INV-4 (KV conservation), INV-7 (signal freshness) R2 (sort map keys), R11 (guard division), R17 (signal freshness) Robustness \u2014 R3 (validate CLI flags), R19 (livelock protection), R20 (degenerate inputs) Cross-policy \u2014 R18 (CLI flag precedence)"},{"location":"contributing/standards/experiments/#experiment-design-rules","title":"Experiment Design Rules","text":""},{"location":"contributing/standards/experiments/#ed-1-controlled-comparison","title":"ED-1: Controlled comparison","text":"<p>Vary exactly one dimension between configurations. Everything else held constant (same model, same instances, same workload, same seed). If the experiment requires varying multiple dimensions, decompose into separate sub-experiments.</p>"},{"location":"contributing/standards/experiments/#ed-2-rate-awareness","title":"ED-2: Rate awareness","text":"<p>Many effects are rate-dependent (e.g., signal freshness only matters at high rates). When the hypothesis involves load-dependent behavior: - Run at the target rate where the effect is expected - Also run at a rate where the effect should vanish (to confirm the mechanism, not just the outcome) - Document the rate-dependent transition point if observed</p>"},{"location":"contributing/standards/experiments/#ed-3-precondition-verification","title":"ED-3: Precondition verification","text":"<p>Before comparing configurations, verify the experiment preconditions hold. Examples: - Testing SJF vs FCFS? Verify queue depth exceeds batch size (otherwise both produce identical batches). - Testing cache hit benefit? Verify KV blocks are large enough to hold the prefix (otherwise LRU eviction destroys it).</p> <p>Document the precondition check in the experiment script (not just in prose).</p>"},{"location":"contributing/standards/experiments/#ed-4-workload-seed-independence","title":"ED-4: Workload seed independence","text":"<p>Resolved (#284): CLI <code>--seed</code> now overrides the workload-spec YAML <code>seed:</code> field when explicitly passed. Behavior: - <code>--seed N --workload-spec w.yaml</code> \u2192 workload uses seed N (CLI override) - <code>--workload-spec w.yaml</code> (no <code>--seed</code>) \u2192 workload uses YAML <code>seed:</code> value (backward compatible) - CLI-generated workloads (<code>--rate</code>, <code>--num-requests</code>) \u2192 <code>--seed</code> controls everything (unchanged)</p> <p>For multi-seed experiments: simply vary <code>--seed</code> on the command line. No need to generate per-seed YAML copies.</p> <p>Note: The YAML <code>seed:</code> field still serves as the default seed for the workload when <code>--seed</code> is not explicitly specified. This enables the \"shareable workload\" pattern \u2014 distributing a YAML file that always produces the same workload by default.</p>"},{"location":"contributing/standards/experiments/#ed-5-reproducibility","title":"ED-5: Reproducibility","text":"<p>Every experiment must be reproducible from its artifacts alone: - <code>run.sh</code> must build the binary and run all variants - Exact seed values documented - Exact commit hash recorded (or the experiment is tied to a specific branch/PR) - No manual steps between script invocation and results</p>"},{"location":"contributing/standards/experiments/#ed-6-config-diff-against-reference-experiments","title":"ED-6: Config diff against reference experiments","text":"<p>When an experiment reuses calibration data from a prior experiment (e.g., \"H8 found the preemption cliff at 2100 blocks, so we use 2100\"), diff every CLI flag and YAML field between the two experiments. Document any differences. Even a single changed flag (e.g., routing policy) can invalidate the calibration.</p> <p>Evidence: H10 used <code>--routing-policy least-loaded</code> while H8 used the default <code>round-robin</code>. This shifted the preemption cliff, producing zero preemptions where H8 found 11%. The mismatch was not caught until post-publication code review.</p>"},{"location":"contributing/standards/experiments/#root-cause-verification","title":"Root Cause Verification","text":"<p>After analyzing results and before finalizing FINDINGS.md, every experiment MUST verify its causal explanations. This step exists because plausible narratives can pass review without being correct.</p>"},{"location":"contributing/standards/experiments/#rcv-1-every-causal-claim-must-cite-fileline","title":"RCV-1: Every causal claim must cite <code>file:line</code>","text":"<p>A root cause analysis that says \"the tiered cache increases total capacity\" without citing the code that does this is a hypothesis about the root cause, not a verified root cause. Trace the claim through the code: - Which function implements the claimed behavior? - What are the exact conditions under which it fires? - Does the claimed mechanism actually change the measured metric? - Tracing depth: The citation must trace to the code that directly modifies the measured metric, not just to the constructor or factory that creates the relevant object. Citing <code>NewKVStore</code> and claiming \"this creates a tiered cache with more capacity\" is insufficient \u2014 you must verify that the GPU block count actually changes in the created object.</p> <p>Evidence: H10 claimed \"CPU tier increases total effective capacity\" \u2014 but <code>NewKVStore</code> (<code>kv_store.go:31-36</code>) does not change GPU block count. The actual mechanism was <code>maybeOffload</code> preserving prefix hashes (<code>sim/kv/tiered.go</code>).</p>"},{"location":"contributing/standards/experiments/#rcv-2-every-surprise-must-have-a-first-principles-calculation","title":"RCV-2: Every \"surprise\" must have a first-principles calculation","text":"<p>Before labeling a result as \"surprising,\" compute the expected value from the system's parameters. If the result matches the calculation, it is not a surprise \u2014 it is the expected outcome of a mechanism you didn't initially consider.</p> <p>Evidence: H5 labeled 96% rejection as a \"surprise.\" But <code>admission.go:45</code> charges <code>len(req.InputTokens)</code> per request (mean=512). Token demand (1,024,000 tokens/s) exceeds supply (400 tokens/s) by 2,560x. The 96% rejection is the mathematically inevitable steady state.</p>"},{"location":"contributing/standards/experiments/#rcv-3-check-the-mechanism-not-just-the-direction","title":"RCV-3: Check the mechanism, not just the direction","text":"<p>Confirming that \"A is better than B\" is necessary but not sufficient. The root cause analysis must explain why through a specific code path. A correct directional result with an incorrect explanation is a ticking time bomb \u2014 the explanation will mislead future experiments.</p> <p>Paradox flag: If the proposed mechanism predicts the opposite direction of what would be intuitive (e.g., \"fewer cache hits improving performance\"), treat this as a red flag. Before accepting a paradoxical explanation, independently verify the underlying data. In H10, the claim \"fewer cache hits \u2192 better TTFT\" survived two rounds because the data (from a buggy analyzer) appeared to support it. The corrected data showed cache hits increased, resolving the paradox. When mechanism and intuition disagree, verify the data first.</p>"},{"location":"contributing/standards/experiments/#rcv-4-validate-causal-claims-with-control-experiments","title":"RCV-4: Validate causal claims with control experiments","text":"<p>When a mechanism is proposed (e.g., \"<code>maybeOffload</code> causes the TTFT improvement\"), design a control experiment that disables only that mechanism (e.g., <code>--kv-offload-threshold 1.0</code>). If the effect vanishes, the mechanism is confirmed. If it persists, the explanation is wrong.</p> <p>Evidence: H10 proposed <code>maybeOffload</code> as the mechanism. The control experiment (threshold=1.0) produced output byte-identical to single-tier, confirming <code>maybeOffload</code> as the sole cause. Without this control, the mechanism question (\"does maybeOffload cause the TTFT improvement?\") would have remained unverified.</p>"},{"location":"contributing/standards/experiments/#rcv-5-confirmation-bias-guard-devils-advocate","title":"RCV-5: Confirmation bias guard (Devil's Advocate)","text":"<p>Before sending FINDINGS.md to external review, the experimenter must write a Devil's Advocate section: 2-3 sentences arguing the opposite of the conclusion. This is a pre-review self-check that forces consideration of alternative interpretations.</p> <pre><code>## Devil's Advocate\n\n**If this is \"Confirmed,\" argue why it might be Refuted:**\nThe 69x TTFT improvement could be entirely from load shedding (96% rejection)\nrather than burst smoothing. A firewall that blocks all traffic also has great\nlatency for the requests that pass.\n\n**If this is \"Refuted,\" argue why it might be Confirmed:**\nThe calibrated bucket (cap=100K) showed a 4% improvement \u2014 small but consistent\nacross 2 of 3 seeds. This might be a real but tiny burst-smoothing effect masked\nby workload noise.\n</code></pre> <p>The reviewers see both the conclusion AND the counter-argument. This prevents the failure mode where the experimenter writes \"Confirmed\" and the reviewers are anchored by that label.</p> <p>Evidence: H5 was labeled \"Confirmed\" for three rounds. Nobody argued the alternative until Round 3's honest reassessment. A Devil's Advocate section in Round 1 would have surfaced \"could this be load shedding?\" immediately.</p>"},{"location":"contributing/standards/experiments/#rcv-6-mandatory-scope-and-limitations","title":"RCV-6: Mandatory Scope and Limitations","text":"<p>Every FINDINGS.md must include a Scope and Limitations section documenting: - Exact operating point tested (blocks, rate, seeds, instances, routing) - Parameters the findings depend on - What was NOT tested that could change the conclusion - Whether the finding generalizes or is specific to the tested configuration</p> <p>Evidence: H10's \"28% TTFT improvement\" is specific to GPU=2100 blocks near the preemption cliff. Without the scope section, this number would be cited as a general property of tiered KV caching.</p>"},{"location":"contributing/standards/experiments/#iterative-review-protocol","title":"Iterative Review Protocol","text":"<p>Canonical source: <code>docs/contributing/convergence.md</code>. If this section diverges, convergence.md is authoritative.</p> <p>Every hypothesis experiment iterates until convergence (max 10 rounds per gate) through three review gates, each using the universal convergence protocol:</p> <ol> <li>Design Review (5 perspectives) \u2014 after experiment design, before implementation</li> <li>Code Review (5 perspectives) \u2014 after implementing run.sh/analyze.py, before execution</li> <li>FINDINGS Review (10 perspectives) \u2014 after documenting results, before finalization</li> </ol> <p>Convergence: Zero CRITICAL and zero IMPORTANT items from any reviewer perspective in the current round. No minimum round count \u2014 convergence in Round 1 is valid if no reviewer flags any CRITICAL or IMPORTANT item. SUGGESTION-level items do not block convergence. See <code>docs/contributing/convergence.md</code> for the full protocol and severity definitions, and <code>docs/contributing/hypothesis.md</code> for reviewer prompts and perspective checklists.</p> <p>Why internal agents instead of external LLM reviews: Internal Task agents can read the actual source files, verify <code>file:line</code> citations, and cross-reference analyzer regexes against simulator output format strings \u2014 capabilities external LLM reviews lack. See <code>docs/contributing/hypothesis.md</code> for the full evidence and comparison table.</p>"},{"location":"contributing/standards/experiments/#hypothesis-resolution","title":"Hypothesis Resolution","text":"<p>Every hypothesis resolves to a status (did the prediction hold?) and a resolution (what do we do about it?). These are distinct \u2014 a \"confirmed\" hypothesis can still have a wrong-mechanism resolution that changes user guidance entirely.</p>"},{"location":"contributing/standards/experiments/#status-the-prediction","title":"Status (the prediction)","text":"Status Definition Example Confirmed The predicted directional outcome holds across all seeds H13: same seed \u2192 byte-identical output Confirmed with nuance The prediction holds but the mechanism or practical implications differ from expected H5: token-bucket reduces TTFT 69x but via 96% load shedding, not burst smoothing; no practical sweet spot Partially confirmed Some predictions hold, others don't, or the experiment tested something different than intended H14: routing pathological confirmed, scheduling showed double-inversion cancellation Refuted The predicted outcome does not hold across seeds (not yet observed \u2014 the refutation IS the value) Inconclusive Effect is within noise (&lt;10% in any seed) or parameter-dependent H5 exp4: calibrated bucket shows &lt;5% TTFT improvement"},{"location":"contributing/standards/experiments/#resolution-what-we-learned-and-what-to-do","title":"Resolution (what we learned and what to do)","text":"Resolution Definition Action Example Clean confirmation Hypothesis holds, mechanism matches prediction Document. No further action. H13, H3, H8 Confirmation with wrong mechanism Prediction holds directionally but the underlying cause differs Correct the explanation. May change user guidance entirely. H5: improvement is load shedding, not burst smoothing Confirmation with bug discovery Prediction holds but experiment surfaces code defects File issues (<code>--label bug</code>). Fix in separate PRs. H12: conservation holds but preemption panics. H14: routing works but 3 detector bugs. H10: tiered KV confirmed but analyzer bug masked preemptions for 2 rounds. Partial confirmation with surprise Some predictions fail; unexpected useful insights emerge Document surprise. May spawn new hypotheses. (use when the experiment finds something valuable but different from what was hypothesized) Refuted \u2014 mechanism not plausible The hypothesis assumed a mechanism that the implementation doesn't support File design issue if the mechanism should exist but doesn't. Document the actual mechanism. H5: hypothesis assumed burst smoothing, but per-input-token cost model (<code>admission.go:45</code>) makes burst smoothing structurally impossible at practical parameters Refuted \u2014 system design flaw Prediction fails because system doesn't work as designed File design issue (<code>--label design</code>). May require architectural change. (not yet observed) Refuted \u2014 wrong mental model Prediction fails because experimenter's assumptions were wrong Correct understanding. Document what the system actually does. (not yet observed) Inconclusive \u2014 parameter-dependent Effect exists at some parameters but not others Document the parameter boundary. May need recalibration. H5 exp4: &lt;5% effect with calibrated bucket Converged to open question Mechanism identified but directional explanation requires different tooling Mark as open. Propose specific tooling needed. (use when remaining questions require code instrumentation, not more experiment sweeps)"},{"location":"contributing/standards/experiments/#choosing-status-vs-resolution","title":"Choosing status vs resolution","text":"<p>The status answers \"did the number go the way we predicted?\" The resolution answers \"do we understand why and what to do about it?\" Always report both:</p> <pre><code>**Status:** Confirmed with nuance\n**Resolution:** Confirmation with wrong mechanism \u2014 token-bucket reduces TTFT\nvia load shedding (96% rejection), not burst smoothing. No practical sweet spot\nunder Gamma CV=3.5.\n</code></pre> <p>A common mistake is declaring \"Confirmed\" and stopping. The resolution is where the real value lives.</p>"},{"location":"contributing/standards/experiments/#findings-classification","title":"Findings Classification","text":"<p>Every experiment produces individual findings. Each finding MUST be classified independently of the hypothesis status:</p> Finding Type Definition Action Required Confirmation The hypothesis holds; the system works as designed Document in FINDINGS.md. No issues needed. Bug discovery The hypothesis failed due to a code defect File GitHub issue with <code>--label bug</code>. Fix in separate PR. New rule The experiment revealed a pattern that should be checked in all future PRs Add to <code>docs/contributing/standards/rules.md</code> with evidence. File issue with <code>--label enhancement</code> if code changes needed. New invariant The experiment revealed a property that must always hold Add to <code>docs/contributing/standards/invariants.md</code>. Design limitation The system works as coded but has an undocumented behavioral limitation Document in FINDINGS.md + file issue with <code>--label design</code> for design doc update. Surprise An unexpected result that doesn't fit other categories Document in FINDINGS.md. May spawn new hypotheses. Open question Mechanism identified but explanation incomplete; requires different tooling to resolve Mark explicitly in FINDINGS.md with proposed tooling/experiment."},{"location":"contributing/standards/experiments/#the-audit-step","title":"The Audit Step","text":"<p>After analyzing results, EVERY experiment MUST audit findings against <code>docs/contributing/standards/</code>:</p> <ol> <li>Do any findings reveal violations of existing rules or principles?</li> <li>Do any findings suggest a new rule, invariant, or principle is needed?</li> <li>Do any findings confirm that existing rules/invariants hold under new conditions?</li> </ol> <p>This audit is what makes experiments a feedback loop into the standards. Example: H3 confirmed that the llm-d default config is robust (confirmation) AND revealed that KV utilization is stale at high rates (design limitation -&gt; new rule R17 + new invariant INV-7 + 3 issues).</p>"},{"location":"contributing/standards/experiments/#experiment-artifacts","title":"Experiment Artifacts","text":"<p>Each hypothesis experiment lives in <code>hypotheses/&lt;name&gt;/</code> with:</p> File Purpose <code>run.sh</code> Self-contained script: builds binary, runs all variants, calls analyzer <code>analyze.py</code> Output parser producing formatted comparison tables <code>FINDINGS.md</code> Results, root cause analysis, findings classification, standards audit <code>*.yaml</code> (optional) Custom workload specs for this experiment <p>Scripts must be reproducible \u2014 running <code>./run.sh</code> on the same commit produces deterministic output.</p>"},{"location":"contributing/standards/invariants/","title":"BLIS System Invariants","text":"<p>Invariants are properties that must hold at all times during and after simulation. They are verified by invariant tests (see R7) and checked during self-audit (Step 4.75).</p> <p>Hypothesis family mapping: INV-1 through INV-3, INV-5, and INV-6 belong to the Scheduler invariants (safety/liveness) family. INV-4 (KV cache conservation), INV-7 (signal freshness), and INV-8 (work-conserving property) belong to the Structural model family. See <code>docs/contributing/standards/experiments.md</code> for hypothesis family definitions.</p>"},{"location":"contributing/standards/invariants/#inv-1-request-conservation","title":"INV-1: Request Conservation","text":"<p>Statement: <code>injected_requests == completed_requests + still_queued + still_running + dropped_unservable</code> at simulation end (all levels).</p> <p>Full pipeline: <code>num_requests == injected_requests + rejected_requests</code> (from anomaly counters).</p> <p>Verification: <code>sim/cluster/cluster_test.go</code> \u2014 conservation tests. Conservation fields (<code>still_queued</code>, <code>still_running</code>, <code>injected_requests</code>) are included in CLI JSON output.</p> <p>Evidence: Issue #183 \u2014 a silently-dropped request violated conservation for months.</p> <p>Experimental validation: H12 confirmed conservation across 10 policy configurations (67 invariant checks) \u2014 including round-robin, least-loaded, weighted (multiple scorer configs), SJF, priority-FCFS, token-bucket admission, and always-busiest. H8 confirmed conservation under extreme KV pressure (15 configurations). Full preemption-path validation is blocked by the panic bug (#293).</p>"},{"location":"contributing/standards/invariants/#inv-2-request-lifecycle","title":"INV-2: Request Lifecycle","text":"<p>Statement: Requests transition <code>queued -&gt; running -&gt; completed</code>. No invalid transitions. Requests not completed before horizon remain in current state.</p> <p>Verification: State machine assertions in request processing code.</p>"},{"location":"contributing/standards/invariants/#inv-3-clock-monotonicity","title":"INV-3: Clock Monotonicity","text":"<p>Statement: Simulation clock never decreases. Every event's timestamp &gt;= the previous event's timestamp.</p> <p>Verification: Clock is advanced in the event loop only via min-heap extraction, which guarantees non-decreasing order.</p>"},{"location":"contributing/standards/invariants/#inv-4-kv-cache-conservation","title":"INV-4: KV Cache Conservation","text":"<p>Statement: <code>allocated_blocks + free_blocks = total_blocks</code> at all times.</p> <p>Verification: Checked after every allocation/deallocation. Transactional allocation with rollback on mid-loop failure (R5).</p> <p>Operational note (H8): KV cache pressure exhibits a sharp cliff, not gradual degradation. In H8's workload, performance was identical above ~2200 blocks and collapsed below it (4.7x TTFT P99 increase with just 4.5% fewer blocks). Below ~1000 blocks, the preempt-requeue cycle can livelock (see R19). Capacity planning formula: <code>threshold \u2248 rate / num_instances \u00d7 (input_tokens + output_tokens) / block_size</code>.</p>"},{"location":"contributing/standards/invariants/#inv-5-causality","title":"INV-5: Causality","text":"<p>Statement: <code>arrival_time &lt;= enqueue_time &lt;= schedule_time &lt;= completion_time</code> for every request.</p> <p>Verification: Per-request metric timestamps recorded at each lifecycle stage. Invariant tests verify ordering for all completed requests.</p>"},{"location":"contributing/standards/invariants/#inv-6-determinism","title":"INV-6: Determinism","text":"<p>Statement: Same seed must produce byte-identical stdout across runs.</p> <p>Verification: Run same configuration twice with same seed; diff stdout. Wall-clock timing goes to stderr (not stdout).</p> <p>Common violation sources: - Go map iteration feeding output ordering (R2) - Floating-point accumulation order dependencies - Wall-clock-dependent randomness (must use PartitionedRNG) - Stateful scorers with non-deterministic internal state</p>"},{"location":"contributing/standards/invariants/#inv-7-signal-freshness-hierarchy","title":"INV-7: Signal Freshness Hierarchy","text":"<p>Statement: Routing snapshot signals have tiered freshness due to DES event ordering and configurable staleness.</p> Signal Owner Freshness (interval=0) Freshness (interval&gt;0) Updated By InFlightRequests Cluster Synchronous Synchronous <code>RoutingDecisionEvent.Execute()</code> (increment), completion detection (decrement) QueueDepth Instance Immediate Periodic <code>QueuedEvent.Execute()</code> BatchSize Instance Immediate Periodic <code>StepEvent.Execute()</code> KVUtilization Instance Immediate Periodic <code>FormBatch()</code> \u2192 <code>AllocateKVBlocks()</code> CacheHitRate Instance Immediate Periodic <code>FormBatch()</code> <p>Design implication: When <code>--snapshot-refresh-interval &gt; 0</code>, all Prometheus-sourced signals (QueueDepth, BatchSize, KVUtilization) share the same scrape interval \u2014 matching real vLLM deployments where all three are exposed via the same <code>/metrics</code> endpoint. <code>InFlightRequests</code> remains synchronous (gateway-local counter, not Prometheus-sourced).</p> <p><code>EffectiveLoad()</code> = <code>QueueDepth + BatchSize + InFlightRequests</code>. The synchronous <code>InFlightRequests</code> term compensates for Periodic staleness in the other two terms.</p> <p>Verification: H3 hypothesis experiment (<code>hypotheses/h3-signal-freshness/</code>), H29 (<code>hypotheses/h29-snapshot-staleness/</code>).</p> <p>Evidence: Issues #282, #283. At rate=5000, kv-utilization-only routing produces 200x worse distribution uniformity than queue-depth. Issue #463: unified Prometheus staleness model.</p>"},{"location":"contributing/standards/invariants/#inv-8-work-conserving-property","title":"INV-8: Work-Conserving Property","text":"<p>Statement: After every step completion, if <code>WaitQ.Len() &gt; 0</code>, a <code>StepEvent</code> must exist in the event queue. The simulator must not idle while there is work waiting.</p> <p>Verification: <code>sim/simulator_test.go</code> \u2014 <code>TestWorkConserving_StepRestartsWhenWaitQNonEmpty</code>. Deterministic test with <code>MaxRunningReqs=1</code>, two requests arriving simultaneously. Without the property, the second request is stranded forever (no arrival to trigger a new StepEvent). With the property, both complete.</p> <p>Evidence: H-MMK experiment (PR #325) \u2014 without the work-conserving fix, W_q error was 151,000% at \u03c1=0.3. After fix, error dropped to 47% (remaining gap is discrete step processing, not a bug).</p> <p>Code location: Search for <code>// Work-conserving:</code> comment in <code>sim/simulator.go</code> \u2014 the <code>else</code> branch of <code>len(remaining) &gt; 0</code> checks <code>WaitQ.Len() &gt; 0</code> and schedules a new <code>StepEvent</code>.</p> <p>Hypothesis family: Structural model (same as INV-4, INV-7).</p>"},{"location":"contributing/standards/principles/","title":"BLIS Engineering Principles","text":"<p>Principles guide design decisions. The antipattern rules are specific, checkable manifestations of these principles. The invariants are properties that must always hold.</p>"},{"location":"contributing/standards/principles/#separation-of-concerns","title":"Separation of Concerns","text":"<ul> <li><code>sim/</code> is a library \u2014 never call <code>os.Exit</code>, <code>logrus.Fatalf</code>, or terminate the process. Return errors. Only <code>cmd/</code> may terminate. (Enforced by R6)</li> <li>Cluster-level policies (admission, routing) receive <code>*RouterState</code> with global view. Instance-level policies (priority, scheduler) receive only local data. Never leak cluster state to instance-level code.</li> <li>Bridge types (<code>RouterState</code>, <code>RoutingSnapshot</code>) live in <code>sim/</code> to avoid import cycles.</li> <li>Unidirectional dependency: <code>cmd/ -&gt; sim/cluster/ -&gt; sim/</code> and <code>sim/cluster/ -&gt; sim/trace/</code>. <code>sim/</code> never imports subpackages.</li> </ul>"},{"location":"contributing/standards/principles/#interface-design","title":"Interface Design","text":"<ul> <li>Single-method interfaces where possible (<code>AdmissionPolicy</code>, <code>RoutingPolicy</code>, <code>PriorityPolicy</code>, <code>InstanceScheduler</code>).</li> <li>Query methods must be pure \u2014 no side effects, no state mutation, no destructive reads. Separate <code>Get()</code> and <code>Consume()</code> for query-and-clear.</li> <li>Factory functions must validate inputs: <code>IsValid*()</code> check + switch/case + panic on unknown.</li> <li>Interfaces defined by behavioral contract, not one implementation's data model. (Enforced by R13)</li> <li>Methods operate within a single module's responsibility. (Enforced by R14)</li> </ul>"},{"location":"contributing/standards/principles/#configuration-design","title":"Configuration Design","text":"<ul> <li>Group configuration by module. (Enforced by R16)</li> <li>Each module's config independently specifiable and validatable.</li> </ul>"},{"location":"contributing/standards/principles/#canonical-constructors","title":"Canonical Constructors","text":"<ul> <li>Every struct constructed in multiple places needs a canonical constructor. Struct literals appear in exactly one place. (Enforced by R4)</li> <li>Before adding a field, grep for ALL construction sites.</li> </ul>"},{"location":"contributing/standards/principles/#output-channel-separation","title":"Output Channel Separation","text":"<ul> <li>stdout (deterministic): simulation results \u2014 metrics JSON, fitness scores, anomaly counters, KV cache metrics, per-SLO metrics, trace summaries. Use <code>fmt.Println</code>/<code>fmt.Printf</code>.</li> <li>stderr (diagnostic): configuration echoes, progress markers, warnings, errors. Use <code>logrus.*</code>, controlled by <code>--log</code>.</li> <li>Rule of thumb: if a user piping to a file would want to capture it, use <code>fmt</code>. If it's debugging context, use <code>logrus</code>.</li> </ul>"},{"location":"contributing/standards/principles/#error-handling-boundaries","title":"Error Handling Boundaries","text":"Layer Strategy Example CLI (<code>cmd/</code>) <code>logrus.Fatalf</code> for user errors Invalid <code>--rate</code> value Library (<code>sim/</code>) <code>panic()</code> for invariant violations Unknown policy name in factory Library (<code>sim/</code>) <code>error</code> return for recoverable failures File I/O, parse errors Runtime (<code>sim/</code>) <code>bool</code> return for expected conditions KV allocation failure -&gt; preemption <p>Never use <code>continue</code> in an error path without propagating, counting, or documenting why it's safe. (Enforced by R1)</p>"},{"location":"contributing/standards/principles/#bddtdd-development","title":"BDD/TDD Development","text":"<ol> <li>Write behavioral contracts first (GIVEN/WHEN/THEN)</li> <li>Implement tests before code</li> <li>Use table-driven tests</li> <li>Test laws, not just values \u2014 invariant tests alongside golden tests (Enforced by R7)</li> <li>Refactor survival test: \"Would this test still pass if the implementation were completely rewritten but the behavior preserved?\"</li> <li>THEN clauses drive test quality \u2014 structural THEN produces structural test</li> </ol> <p>Prohibited assertion patterns (structural \u2014 break on refactor): - Type assertions: <code>policy.(*ConcreteType)</code> - Internal field access: <code>obj.internalField</code> - Exact formula reproduction: <code>assert.Equal(score, 0.6*cache + 0.4*load)</code></p> <p>Required assertion patterns (behavioral \u2014 survive refactor): - Observable output: <code>assert.Equal(policy.Compute(req, clock), 0.0)</code> - Invariant verification: <code>assert.Equal(completed+queued+running+dropped, injected)</code> - Ordering/ranking: <code>assert.True(scoreA &gt; scoreB)</code></p>"},{"location":"contributing/standards/principles/#test-suite-performance","title":"Test Suite Performance","text":"<p>As the test suite grows (invariant tests, golden tests, hypothesis-promoted regression tests), keep total <code>go test ./...</code> time manageable:</p> <ul> <li>Individual test budget: No single test should exceed 5 seconds without using <code>testing.Short()</code> to provide a fast-path skip. Tests that run full cluster simulations (e.g., 10K requests across 8 instances) should check <code>testing.Short()</code> and reduce to a minimal configuration.</li> <li>CI target: Total <code>go test ./...</code> should complete in under 60 seconds. If it exceeds this, audit for tests that can use smaller configurations without losing behavioral coverage.</li> <li>Benchmark isolation: Performance benchmarks (<code>Benchmark*</code> functions) run only with <code>go test -bench=.</code>, never in the default <code>go test ./...</code> path. This is Go's default behavior \u2014 just don't put benchmark assertions in regular tests.</li> </ul>"},{"location":"contributing/standards/principles/#documentation-single-source-of-truth","title":"Documentation Single Source of Truth","text":"<p>Every piece of documentation lives in exactly one canonical location. Other files may contain working copies (summaries for quick reference) with explicit canonical-source headers.</p> <p>The canonical-source pattern:</p> <p>Canonical source: <code>docs/contributing/standards/rules.md</code>. If this section diverges, rules.md is authoritative.</p> <p>When updating any standard, invariant, rule, or recipe: 1. Update the canonical source FIRST 2. Then update any working copies that reference it 3. If you can't update working copies immediately, the canonical-source header ensures readers know which version to trust</p> <p>Single-source-of-truth map:</p> Content Canonical Source Working Copies Antipattern rules (R1-R20) <code>docs/contributing/standards/rules.md</code> CLAUDE.md (table), CONTRIBUTING.md (checklist) System invariants (INV-1\u2013INV-8) <code>docs/contributing/standards/invariants.md</code> CLAUDE.md (summary), <code>docs/concepts/core-engine.md</code> (formulas), <code>docs/concepts/architecture.md</code> (signal freshness) Engineering principles <code>docs/contributing/standards/principles.md</code> CLAUDE.md (summary) Extension recipes (policies, scorers, KV tiers) <code>docs/contributing/extension-recipes.md</code> \u2014 Design process <code>docs/contributing/design-process.md</code> CONTRIBUTING.md (summary) Macro-plan process <code>docs/contributing/macro-planning.md</code> CONTRIBUTING.md (summary) File organization and architecture CLAUDE.md (File Organization tree) README.md (Project Structure tree) Hypothesis catalog and specifications <code>docs/plans/research.md</code> \u2014 Experiment status and coverage <code>hypotheses/README.md</code> \u2014 Experiment standards <code>docs/contributing/standards/experiments.md</code> \u2014 (note: review protocol subsection references <code>docs/contributing/convergence.md</code> as canonical) Convergence protocol <code>docs/contributing/convergence.md</code> <code>docs/contributing/hypothesis.md</code> (summary), <code>docs/contributing/pr-workflow.md</code> (summary), <code>docs/contributing/standards/experiments.md</code> (review protocol summary), <code>.claude/skills/convergence-review/SKILL.md</code> (protocol copy) Hypothesis experiment workflow <code>docs/contributing/hypothesis.md</code> CONTRIBUTING.md (summary), hypotheses/README.md (step list), <code>.claude/skills/hypothesis-experiment/SKILL.md</code> (workflow steps), <code>.claude/skills/hypothesis-experiment/review-prompts.md</code> (perspective prompts) PR workflow <code>docs/contributing/pr-workflow.md</code> CONTRIBUTING.md (summary), <code>.claude/skills/convergence-review/pr-prompts.md</code> (perspective prompts)"},{"location":"contributing/standards/rules/","title":"BLIS Antipattern Rules","text":"<p>Every rule traces to a real bug, design failure, or hypothesis finding. Rules are enforced at three checkpoints: - PR template \u2014 checklist before merge - Micro-plan Phase 8 \u2014 checklist before implementation - Self-audit Step 4.75 \u2014 deliberate critical thinking before commit</p> <p>For the full process, see docs/contributing/pr-workflow.md.</p>"},{"location":"contributing/standards/rules/#priority-tiers","title":"Priority Tiers","text":"<p>New contributors: focus on Critical rules first. These protect correctness \u2014 violating them produces wrong results or crashes. Important rules protect code quality and maintainability. Hygiene rules keep the codebase clean over time.</p> Tier Rules Why Critical (correctness) R1, R4, R5, R6, R11, R19 Violations produce silent data loss, panics, conservation invariant breaks, or infinite loops Important (quality) R2, R3, R7, R8, R9, R10, R13, R14, R17, R18, R20 Violations produce non-determinism, validation gaps, silent misconfig, interface debt, or undetected anomalies Hygiene (maintenance) R12, R15, R16 Violations produce stale references, config sprawl, or misleading test baselines <p>All 20 rules apply to every PR. The tiers help you prioritize during review \u2014 check Critical rules first.</p>"},{"location":"contributing/standards/rules/#rules","title":"Rules","text":""},{"location":"contributing/standards/rules/#r1-no-silent-data-loss","title":"R1: No silent data loss","text":"<p>Every error path must either return an error, panic with context, or increment a counter. A <code>continue</code> or early <code>return</code> that silently drops a request, metric, or allocation is a correctness bug.</p> <p>Evidence: Issue #183 \u2014 a KV allocation failure silently dropped a request. The golden test perpetuated the bug for months because it captured \"499 completions\" as the expected value.</p> <p>Additional evidence: H14 hypothesis experiment \u2014 HOL blocking detector silently returns 0 instead of flagging the most extreme imbalance case when <code>always-busiest</code> routes all traffic to one instance (bug #291).</p> <p>Check: For every <code>continue</code> or early <code>return</code> in new code, verify the error is propagated, counted, or documented as safe.</p> <p>Enforced: PR template, micro-plan Phase 8, self-audit dimension 9.</p>"},{"location":"contributing/standards/rules/#r2-sort-map-keys-before-float-accumulation","title":"R2: Sort map keys before float accumulation","text":"<p>Go map iteration is non-deterministic. Any <code>for k, v := range someMap</code> that feeds a running sum (<code>total += v</code>) or determines output ordering must sort keys first. Unsorted iteration violates the determinism invariant (INV-6).</p> <p>Evidence: Five sites iterated Go maps to accumulate floats or determine output ordering, violating determinism.</p> <p>Check: For every <code>range</code> over a map, check if the loop body accumulates floats or produces ordered output. If so, sort keys first.</p> <p>Enforced: PR template, micro-plan Phase 8, self-audit dimension 3.</p>"},{"location":"contributing/standards/rules/#r3-validate-all-numeric-cli-flags","title":"R3: Validate ALL numeric CLI flags","text":"<p>Every numeric flag (<code>--rate</code>, <code>--fitness-weights</code>, <code>--kv-cpu-blocks</code>, etc.) must be validated for: zero, negative, NaN, Inf, and empty string. Missing validation causes infinite loops (Rate=0) or wrong results (NaN weights).</p> <p>Evidence: <code>--rate 0</code> caused an infinite loop deep in the simulation. <code>--snapshot-refresh-interval</code> was added without validation (#281).</p> <p>Check: For every new CLI flag, add validation in <code>cmd/root.go</code> with <code>logrus.Fatalf</code> for invalid values.</p> <p>Enforced: PR template, micro-plan Phase 8, self-audit dimension 6.</p>"},{"location":"contributing/standards/rules/#r4-construction-site-audit","title":"R4: Construction site audit","text":"<p>Before adding a field to a struct, find every place that struct is constructed as a literal. If there are multiple sites, either add a canonical constructor or update every site. Missing a site causes silent field-zero bugs.</p> <p>Evidence: Issue #181 \u2014 adding <code>InstanceID</code> to per-request metrics required changes in 4 files. Three construction sites for <code>RequestMetrics</code> existed, and one was missed initially.</p> <p>Check: <code>grep 'StructName{' across the codebase</code>. List every site. Update all or refactor to canonical constructor.</p> <p>Enforced: PR template, micro-plan Phase 0 + Phase 8, self-audit dimension 8.</p>"},{"location":"contributing/standards/rules/#r5-transactional-state-mutation","title":"R5: Transactional state mutation","text":"<p>Any loop that allocates resources (blocks, slots, counters) must handle mid-loop failure by rolling back all mutations from previous iterations. A partial allocation that returns <code>false</code> without cleanup violates conservation invariants.</p> <p>Evidence: KV block allocation (<code>AllocateKVBlocks</code>) had a mid-loop failure path that didn't roll back previously allocated blocks, violating KV conservation (INV-4).</p> <p>Additional evidence: H12 hypothesis experiment \u2014 preemption loop in <code>sim/simulator.go:383</code> accesses <code>RunningBatch.Requests[len-1]</code> without bounds check. When all running requests are evicted and the batch is empty, the code panics with index out of range (bug #293).</p> <p>Check: For every loop that mutates state, verify the failure path rolls back all mutations.</p> <p>Enforced: PR template, micro-plan Phase 8, self-audit dimension 9.</p>"},{"location":"contributing/standards/rules/#r6-no-logrusfatalf-in-library-code","title":"R6: No logrus.Fatalf in library code","text":"<p>The <code>sim/</code> package tree must never terminate the process \u2014 return errors so callers can handle them. Only <code>cmd/</code> may terminate. This enables embedding, testing, and adapters.</p> <p>Evidence: Library code that called <code>logrus.Fatalf</code> prevented test isolation and made the simulator non-embeddable.</p> <p>Check: <code>grep -r 'logrus.Fatal\\|os.Exit' sim/</code> must return zero results.</p> <p>Enforced: PR template, micro-plan Phase 8.</p>"},{"location":"contributing/standards/rules/#r7-invariant-tests-alongside-golden-tests","title":"R7: Invariant tests alongside golden tests","text":"<p>Golden tests (comparing against known-good output) are regression freezes, not correctness checks. If a bug exists when the golden values are captured, the golden test perpetuates the bug. Every subsystem that has golden tests must also have invariant tests that verify conservation laws, causality, and determinism.</p> <p>Evidence: Issue #183 \u2014 the codellama golden dataset expected 499 completions because one request was silently dropped. A conservation invariant test would have caught it on day one.</p> <p>Check: For every golden test, ask: \"If this expected value were wrong, would any other test catch it?\" If no, add an invariant test.</p> <p>Enforced: PR template, micro-plan Phase 6 + Phase 8, self-audit dimension 7.</p>"},{"location":"contributing/standards/rules/#r8-no-exported-mutable-maps","title":"R8: No exported mutable maps","text":"<p>Validation lookup maps (e.g., <code>validRoutingPolicies</code>) must be unexported. Expose through <code>IsValid*()</code> accessor functions. Exported maps allow callers to mutate global state, breaking encapsulation and enabling hard-to-trace bugs.</p> <p>Evidence: Exported mutable maps were found during hardening audit \u2014 callers could silently add entries to validation maps.</p> <p>Check: <code>grep -r 'var [A-Z].*map\\[' sim/</code> must return zero mutable map results.</p> <p>Enforced: PR template, micro-plan Phase 8.</p>"},{"location":"contributing/standards/rules/#r9-pointer-types-for-yaml-zero-value-ambiguity","title":"R9: Pointer types for YAML zero-value ambiguity","text":"<p>YAML config structs must use <code>*float64</code> (pointer) for fields where zero is a valid user-provided value, to distinguish \"not set\" (nil) from \"set to zero\" (0.0). Using bare <code>float64</code> causes silent misconfiguration when users intentionally set a value to zero.</p> <p>Evidence: YAML fields with bare <code>float64</code> couldn't distinguish \"user set this to 0\" from \"user didn't set this.\"</p> <p>Check: For every new YAML config field where zero is meaningful, use a pointer type.</p> <p>Enforced: Micro-plan Phase 8.</p>"},{"location":"contributing/standards/rules/#r10-strict-yaml-parsing","title":"R10: Strict YAML parsing","text":"<p>Use <code>yaml.KnownFields(true)</code> or equivalent strict parsing for all YAML config loading. Typos in field names must cause parse errors, not silent acceptance of malformed config.</p> <p>Evidence: YAML typos in field names were silently accepted, producing default behavior instead of the user's intended configuration.</p> <p>Check: Every <code>yaml.Unmarshal</code> or decoder usage must enable strict/known-fields mode.</p> <p>Enforced: Micro-plan Phase 8.</p>"},{"location":"contributing/standards/rules/#r11-guard-division-in-runtime-computation","title":"R11: Guard division in runtime computation","text":"<p>Any division where the denominator derives from runtime state (batch size, block count, request count, bandwidth) must guard against zero. CLI validation (R3) catches input zeros at the boundary; this rule catches intermediate zeros that arise during simulation.</p> <p>Evidence: <code>utilization = usedBlocks / totalBlocks</code> when no blocks are configured; <code>avgLatency = sum / count</code> when count is zero.</p> <p>Check: For every division, verify the denominator is either (a) guarded by an explicit zero check, or (b) proven non-zero by a documented invariant.</p> <p>Enforced: Micro-plan Phase 8.</p>"},{"location":"contributing/standards/rules/#r12-golden-dataset-regenerated-when-output-changes","title":"R12: Golden dataset regenerated when output changes","text":"<p>When a PR changes output format, metrics, or default behavior, the golden dataset must be regenerated and the regeneration command documented. Golden tests that pass with stale expected values provide false confidence.</p> <p>Evidence: Present in CONTRIBUTING.md and PR template but not in CLAUDE.md's numbered rules \u2014 an inconsistency this consolidation resolves.</p> <p>Check: If <code>go test ./sim/... -run Golden</code> fails after your changes, regenerate and document the command.</p> <p>Enforced: PR template, micro-plan Phase 8.</p>"},{"location":"contributing/standards/rules/#r13-interfaces-accommodate-multiple-implementations","title":"R13: Interfaces accommodate multiple implementations","text":"<p>New interfaces must accommodate at least two implementations (even if only one exists today). No methods that only make sense for one backend.</p> <p>Evidence: <code>KVStore</code> interface has methods exposing block-level semantics. A distributed KV cache like LMCache thinks in tokens and layers, not blocks. The interface encodes vLLM's implementation model rather than an abstract behavioral contract.</p> <p>Check: For every new interface, ask: \"Could a second backend implement this without dummy methods?\"</p> <p>Enforced: Micro-plan Phase 8.</p> <p>Previously: principle in CLAUDE.md \"Interface design\" section. Promoted to numbered rule for checkability.</p>"},{"location":"contributing/standards/rules/#r14-no-multi-module-methods","title":"R14: No multi-module methods","text":"<p>No method should span multiple module responsibilities (scheduling + latency estimation + metrics in one function). Extract each concern into its module's interface.</p> <p>Evidence: <code>Simulator.Step()</code> is 134 lines mixing scheduling, latency estimation, token generation, completion, and metrics. Impossible to swap the latency model without modifying this method.</p> <p>Check: If a method touches &gt;1 module's concern, extract each concern.</p> <p>Enforced: Micro-plan Phase 8.</p> <p>Previously: principle in CLAUDE.md \"Interface design\" section. Promoted to numbered rule for checkability.</p>"},{"location":"contributing/standards/rules/#r15-resolve-stale-pr-references","title":"R15: Resolve stale PR references","text":"<p>After completing a PR, grep for references to that PR number (<code>planned for PR N</code>, <code>TODO.*PR N</code>) in the codebase. Resolve all stale references.</p> <p>Evidence: Multiple stale comments referencing completed PRs accumulated over time, misleading future developers about what was implemented vs planned.</p> <p>Check: <code>grep -rn 'planned for PR\\|TODO.*PR' --include='*.go' --include='*.md'</code> for the current PR number.</p> <p>Enforced: Micro-plan Phase 8.</p>"},{"location":"contributing/standards/rules/#r16-group-configuration-by-module","title":"R16: Group configuration by module","text":"<p>Configuration parameters must be grouped by module \u2014 not added to a monolithic config struct mixing unrelated concerns. Each module's config should be independently specifiable and validatable.</p> <p>Evidence: <code>SimConfig</code> previously combined hardware identity, model parameters, simulation parameters, and policy choices in 23 flat fields. Resolved in #350: <code>SimConfig</code> now embeds 6 module-scoped sub-configs (<code>KVCacheConfig</code>, <code>BatchConfig</code>, <code>LatencyCoeffs</code>, <code>ModelHardwareConfig</code>, <code>PolicyConfig</code>, <code>WorkloadConfig</code>). Factory signatures accept the narrowest sub-config (e.g., <code>NewKVStore(KVCacheConfig)</code>).</p> <p>Check: New config parameters go into the appropriate sub-config in <code>sim/config.go</code>, not directly into <code>SimConfig</code>.</p> <p>Enforced: Micro-plan Phase 8.</p> <p>Previously: principle in CLAUDE.md \"Configuration design\" section. Promoted to numbered rule for checkability.</p>"},{"location":"contributing/standards/rules/#r17-document-signal-freshness-for-routing-inputs","title":"R17: Document signal freshness for routing inputs","text":"<p>Routing snapshot signals have different freshness guarantees due to DES event ordering. Scorer authors must understand which signals are synchronously fresh and which are stale. Any scorer intended for high-rate routing must either use a synchronously-fresh signal or be combined with one that does.</p> <p>Evidence: H3 hypothesis experiment (#279) \u2014 kv-utilization scorer produced 200x worse distribution uniformity than queue-depth at rate=5000. See issues #282, #283.</p> <p>Freshness hierarchy: - Synchronous (cluster-owned): InFlightRequests \u2014 always fresh (gateway counter) - Immediate/Periodic (instance-owned): QueueDepth, BatchSize, KVUtilization, CacheHitRate \u2014 Immediate when <code>--snapshot-refresh-interval=0</code>, Periodic when <code>&gt;0</code></p> <p>Check: When writing a new scorer, identify which snapshot fields it reads and their freshness. If using only Periodic signals, document why or combine with a synchronous scorer (InFlightRequests via EffectiveLoad).</p> <p>Enforced: Design review, scorer implementation review.</p>"},{"location":"contributing/standards/rules/#r18-cli-flag-precedence-over-defaults","title":"R18: CLI flag precedence over defaults","text":"<p>When the CLI binary loads default values from <code>defaults.yaml</code>, it must not silently overwrite user-provided flag values. Always check <code>cmd.Flags().Changed(\"&lt;flag&gt;\")</code> before applying a default. A user who explicitly passes <code>--total-kv-blocks 50</code> must get 50, not the model's default of 132,139.</p> <p>Evidence: H9 hypothesis experiment \u2014 <code>GetCoefficients()</code> unconditionally overwrote <code>totalKVBlocks</code> with the model default, silently destroying the CLI flag value. The entire H9 Experiment 3 (cache capacity independence) produced invalid results. Bug #285, fix cbb0de7.</p> <p>Check: For every assignment from <code>defaults.yaml</code> to a CLI-parsed variable, verify <code>cmd.Flags().Changed()</code> is checked first. Grep for <code>GetCoefficients</code> and <code>defaults.yaml</code> assignment patterns.</p> <p>Enforced: PR template, micro-plan Phase 8, self-audit dimension 6.</p>"},{"location":"contributing/standards/rules/#r19-livelock-protection-for-unbounded-retry-loops","title":"R19: Livelock protection for unbounded retry loops","text":"<p>Loops where the exit condition depends on resource availability that may never be satisfied (e.g., preempt \u2192 requeue \u2192 schedule \u2192 preempt) must have a circuit breaker: maximum iteration count, progress assertion, or bounded retry with error escalation. An infinite loop in a deterministic simulator is indistinguishable from a hang.</p> <p>Evidence: H8 hypothesis experiment \u2014 with total KV blocks below ~1000 (insufficient for any single request), the preempt-requeue cycle ran indefinitely with no termination condition, no max-retry limit, and no progress check.</p> <p>Check: For every loop that retries an operation after a resource failure, verify there is an explicit bound or progress check. Pay special attention to preemption, eviction, and reallocation loops.</p> <p>Enforced: PR template, micro-plan Phase 8, self-audit dimension 4.</p>"},{"location":"contributing/standards/rules/#r20-degenerate-input-handling-in-detectors-and-analyzers","title":"R20: Degenerate input handling in detectors and analyzers","text":"<p>Anomaly detectors and metric analyzers must explicitly handle degenerate inputs: empty sample sets, single-instance concentration, all-zero distributions, and cross-class comparisons. The degenerate case is often the most important one to detect \u2014 a detector that returns \"no anomaly\" when all traffic hits one instance is worse than useless.</p> <p>Evidence: H14 hypothesis experiment \u2014 two detector failures: (1) HOL blocking detector requires \u22652 instances with samples, but <code>always-busiest</code> routes ALL traffic to one instance, leaving 3 empty \u2014 detector returns 0 for the most extreme HOL case (bug #291). (2) Priority inversion detector uses a 2x threshold that conflates workload heterogeneity with scheduling unfairness \u2014 7,463 false positives with normal configs (bug #292).</p> <p>Check: For every detector or analyzer, identify what happens when one or more inputs are empty, zero, or maximally skewed. Write tests for these degenerate cases.</p> <p>Enforced: PR template, micro-plan Phase 8, self-audit dimension 9.</p>"},{"location":"contributing/standards/rules/#quick-reference-checklist","title":"Quick Reference Checklist","text":"<p>For PR authors \u2014 check each rule before submitting:</p> <ul> <li> R1: No silent <code>continue</code>/<code>return</code> dropping data</li> <li> R2: Map keys sorted before float accumulation or ordered output</li> <li> R3: Every new CLI flag validated (zero, negative, NaN, Inf)</li> <li> R4: All struct construction sites audited for new fields</li> <li> R5: Resource allocation loops handle mid-loop failure with rollback</li> <li> R6: No <code>logrus.Fatalf</code> or <code>os.Exit</code> in <code>sim/</code> packages</li> <li> R7: Invariant tests alongside any golden tests</li> <li> R8: No exported mutable maps</li> <li> R9: <code>*float64</code> for YAML fields where zero is valid</li> <li> R10: YAML strict parsing (<code>KnownFields(true)</code>)</li> <li> R11: Division by runtime-derived denominators guarded</li> <li> R12: Golden dataset regenerated if output changed</li> <li> R13: New interfaces work for 2+ implementations</li> <li> R14: No method spans multiple module responsibilities</li> <li> R15: Stale PR references resolved</li> <li> R16: Config params grouped by module</li> <li> R17: Routing scorer signals documented for freshness tier</li> <li> R18: CLI flag values not silently overwritten by defaults.yaml</li> <li> R19: Unbounded retry/requeue loops have circuit breakers</li> <li> R20: Detectors and analyzers handle degenerate inputs (empty, skewed, zero)</li> </ul>"},{"location":"contributing/standards/rules/#rule-lifecycle","title":"Rule Lifecycle","text":"<p>Rules are born from real bugs and live as long as they prevent real bugs. As the codebase evolves, some rules may become automated, consolidated, or no longer applicable.</p>"},{"location":"contributing/standards/rules/#lifecycle-states","title":"Lifecycle States","text":"State Meaning Action Active Rule prevents a class of bugs that can still occur Check in every PR review Automated Rule is enforced by CI (linter, test, build) Note the enforcement mechanism; keep for documentation but skip manual checks Consolidated Rule merged into a broader rule Redirect to the parent rule; remove from checklist Retired The class of bugs is no longer possible (e.g., the vulnerable code path was removed) Move to a \"Retired Rules\" appendix with rationale"},{"location":"contributing/standards/rules/#when-to-consolidate","title":"When to Consolidate","text":"<p>If two rules address the same root principle and checking one always catches the other, consolidate them. Example: if a linter rule were added that caught all R2 violations (unsorted map iteration), R2 could move to \"Automated\" state.</p>"},{"location":"contributing/standards/rules/#quarterly-review","title":"Quarterly Review","text":"<p>Every ~10 PRs or quarterly (whichever comes first), scan the rule list: 1. Can any rule be automated by a linter or CI check? 2. Are any two rules always checked together and catching the same class of bugs? 3. Has the code path that motivated any rule been removed?</p> <p>File an issue for each proposed state change. Do not retire rules silently.</p>"},{"location":"contributing/standards/rules/#current-state","title":"Current State","text":"<p>All 20 rules (R1-R20) are Active as of 2026-02-26. No rules have been automated, consolidated, or retired.</p>"},{"location":"contributing/templates/design-guidelines/","title":"BLIS Design Guidelines: Principles for Robust Simulation Design and Modular Extension","text":"<p>Date: 2026-02-18 Status: Draft (pending review) Species: System Overview</p>"},{"location":"contributing/templates/design-guidelines/#1-purpose-scope","title":"1. Purpose &amp; Scope","text":"<p>This document serves two audiences: 1. Design doc authors (human or Claude) \u2014 guidance on writing design docs that stay durable and useful across the project lifecycle 2. Module developers \u2014 guidance on extending BLIS with new modules that fit the architecture and enable parallel development</p> <p>What this document IS: - A target architecture specification that BLIS will be refactored toward - A set of principles grounded in DES methodology and project experience (12 completed PRs, 20+ issues) - A reference for evaluating whether a design doc or a new module meets BLIS's quality bar</p> <p>What this document is NOT: - Not an implementation plan (refactoring happens in separate PRs, each following <code>docs/contributing/pr-workflow.md</code>) - Not a replacement for CLAUDE.md (which captures engineering rules and code-level patterns) - Not a replacement for the micro-plan template (which captures PR-level planning structure)</p>"},{"location":"contributing/templates/design-guidelines/#relationship-to-existing-docs","title":"Relationship to Existing Docs","text":"<pre><code>Design Guidelines (this doc)          \u2190 Principles, target architecture, extension framework\n    \u2193 informs\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Design Docs  \u2190\u2192  Macro Plan                    \u2502\n\u2502       \u2193                \u2193                        \u2502\n\u2502  Micro Plans     Micro Plans                    \u2502\n\u2502       \u2193                \u2193                        \u2502\n\u2502  CLAUDE.md (updated by each PR)                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Scenario Path Large multi-PR feature Design doc \u2192 Macro plan \u2192 Micro plans per PR Single-PR feature Design doc \u2192 Micro plan directly Refactoring / bug fix Issue or design doc \u2192 Micro plan directly Macro plan PR Macro plan section \u2192 Micro plan <p>Key distinction: This document describes the target state and design principles. CLAUDE.md describes the current state and implementation rules. Where they diverge, this document is aspirational and CLAUDE.md is authoritative for today's code.</p>"},{"location":"contributing/templates/design-guidelines/#2-des-design-foundations","title":"2. DES Design Foundations","text":"<p>BLIS is a discrete-event simulator. Design docs for BLIS should be informed by established DES methodology (Banks et al., Discrete-Event System Simulation; Misra, Distributed Discrete-Event Simulation, 1986).</p> <p>Core Principle: Abstraction must be justified. Every state variable, event type, and random input should be defensible in terms of the questions the simulator will answer \u2014 not fidelity for its own sake.</p>"},{"location":"contributing/templates/design-guidelines/#21-model-scoping-banks-et-al","title":"2.1 Model Scoping (Banks et al.)","text":"<p>Before including a component in BLIS, evaluate it against these six criteria:</p> <ol> <li>Will including it significantly affect the accuracy of results for the target analysis questions?</li> <li>What level of accuracy is actually required for the analysis to be useful?</li> <li>Can the component's data requirements be satisfied (alpha/beta coefficients, hardware specs, workload traces)?</li> <li>What is the cost of inclusion \u2014 code complexity, maintenance burden, configuration surface?</li> <li>What breaks if we omit it? (Sensitivity analysis \u2014 if removing it changes results by &lt;5%, defer it)</li> <li>What is the simplest version that answers the same questions? (Start coarse, refine only with evidence)</li> </ol> <p>These criteria operationalize \"YAGNI\" for simulation design. Example: the PR12 pre-design chose synchronous transfer latency over event-based transfers because it answered the same questions with ~100 fewer LOC.</p>"},{"location":"contributing/templates/design-guidelines/#22-event-design","title":"2.2 Event Design","text":"<ul> <li>Events must be minimal and atomic \u2014 each event corresponds to exactly one state change.</li> <li>Classify new events as exogenous (arrivals, environment changes \u2014 driven by workload input) or endogenous (completions, scheduling decisions, scaling actions \u2014 driven by internal state transitions). This classification must appear in design docs.</li> <li>New events must specify their priority constant for tie-breaking within BLIS's <code>(timestamp, priority, seqID)</code> ordering scheme.</li> </ul>"},{"location":"contributing/templates/design-guidelines/#23-state-vs-statistics-separation","title":"2.3 State vs. Statistics Separation","text":"<ul> <li>State variables evolve the system (queues, caches, clocks, request lifecycle) \u2014 they are inputs to event handlers.</li> <li>Statistics are derived from state trajectories (TTFT distributions, throughput, utilization) \u2014 they are outputs for analysis.</li> <li>These must be decoupled. A module that mixes state mutation and metric computation in the same method is violating this principle.</li> </ul>"},{"location":"contributing/templates/design-guidelines/#24-verification-and-validation","title":"2.4 Verification and Validation","text":"<ul> <li>Verification: the code correctly implements the conceptual model (behavioral tests, invariant tests, golden tests).</li> <li>Validation: the model accurately represents real-system behavior (calibration against real servers, sensitivity analysis, comparison with published benchmarks).</li> <li>A verified simulator can be an invalid model. (BLIS issue #183: golden tests verified the code did what it did, but an invariant test would have caught the dropped request.)</li> <li>Design docs must specify both: how will correctness be verified (which invariants?) AND how will fidelity be validated (against what real-system data?).</li> </ul>"},{"location":"contributing/templates/design-guidelines/#25-randomness-as-first-class-concern","title":"2.5 Randomness as First-Class Concern","text":"<ul> <li>All randomness flows through <code>PartitionedRNG</code> with named subsystems.</li> <li>New modules that introduce randomness must declare their subsystem name and justify that their random draws don't interfere with existing streams.</li> <li>Design docs should consider whether the feature enables common random numbers experiments (paired comparison of two configurations using the same random stream).</li> </ul>"},{"location":"contributing/templates/design-guidelines/#26-des-design-review-checklist","title":"2.6 DES Design Review Checklist","text":"<p>Every BLIS design doc must answer these questions:</p> Question Principle What analysis questions does this design help answer? Model scoping What is modeled, simplified, and deliberately omitted? (Table format) Model scoping What events are introduced or modified? Exogenous or endogenous? Event design How do new events interact with existing tie-breaking rules? Event design What new state is introduced? Who owns it? State/statistics separation What new metrics are derived? Collected incrementally or on demand? State/statistics separation How will correctness be verified? (Which invariants?) Verification How will fidelity be validated? (Against what data?) Validation Does this introduce new randomness? Which PartitionedRNG subsystem? Randomness What is the simplest version that answers the same questions? Model scoping"},{"location":"contributing/templates/design-guidelines/#3-design-doc-guidelines","title":"3. Design Doc Guidelines","text":""},{"location":"contributing/templates/design-guidelines/#31-the-staleness-test","title":"3.1 The Staleness Test","text":"<p>Before including any content in a design doc, apply this test:</p> <p>\"If the implementation changes this detail during micro-planning, will the design doc silently mislead future readers?\"</p> <ul> <li>Durable content (include): invariants, modeling decisions, fidelity trade-offs, extension points described behaviorally, decision rationale with alternatives considered.</li> <li>Fragile content (exclude): Go struct field lists, method implementations, file paths with line numbers, specific parameter names.</li> </ul> <p>The dividing line: describe what crosses a boundary and why, not how the boundary is implemented.</p>"},{"location":"contributing/templates/design-guidelines/#32-four-design-doc-species","title":"3.2 Four Design Doc Species","text":"<p>Not all design docs serve the same purpose. Choose the right species based on scope:</p> Species When to Use Structure Example Decision Record Single-PR architectural choices that need trade-off analysis Numbered decisions, each with Problem / Decision / Rationale / Alternatives PR12 pre-design (9 decisions) Specification New subsystem with precise behavioral requirements Behavioral contracts, math/formulas, input/output schemas, validation criteria Workload generator design Problem Analysis Refactoring motivated by identified friction or bugs Extension scenario analysis, antipattern catalog with evidence, phased fix plan Hardening design System Overview Multi-PR feature spanning multiple modules Concept model, module interactions, invariants, phased roadmap Evolutionary policy optimization design <p>A design doc should declare its species at the top so readers know what to expect.</p>"},{"location":"contributing/templates/design-guidelines/#33-required-sections-all-species","title":"3.3 Required Sections (All Species)","text":"<p>Every BLIS design doc, regardless of species, must include:</p> <ol> <li>Motivation \u2014 What problem does this solve? What can't users do today? (2-5 sentences, no jargon)</li> <li>Scope \u2014 What's in, what's explicitly out, what's deferred to later</li> <li>Modeling Decisions \u2014 What is modeled, simplified, and omitted (table format per Section 2.1)</li> <li>Invariants \u2014 What must always hold after this design is implemented? What must never happen? (Named: INV-1, INV-2, ...)</li> <li>Decisions with Trade-offs \u2014 For each non-obvious choice: what alternatives were considered, why this one won, what breaks if it's wrong</li> <li>Extension Points \u2014 Where do future extensions plug in? What is the default behavior? What would a non-default look like?</li> <li>Validation Strategy \u2014 How will correctness be verified (invariants) and fidelity be validated (calibration, comparison)?</li> <li>DES Checklist \u2014 Completed checklist from Section 2.6</li> </ol>"},{"location":"contributing/templates/design-guidelines/#34-prohibited-content","title":"3.4 Prohibited Content","text":"<p>Do NOT include in design docs (with rationale from project experience):</p> Content Why Not What to Write Instead Go struct definitions with field lists Diverged within 2 PRs in the original design doc \u2014 \"aspirational signatures\" Describe what data crosses the boundary and its semantics Method implementations Changed during micro-planning in every PR Describe the behavioral contract (GIVEN/WHEN/THEN) File paths with line numbers Stale after any refactoring Name the module and its responsibility Specific parameter/field names Renamed during implementation Describe the concept (\"load metric combining queue depth and batch size\") Interface signatures in Go syntax Froze prematurely in original design doc; actual interfaces were simpler Describe the interface's contract: single method? what it observes? what it returns? <p>Exception: Decision Records (species 1) may include brief code snippets when the decision IS about a specific implementation choice (e.g., \"use <code>math.Ceil</code> not integer division\"). Keep these minimal.</p>"},{"location":"contributing/templates/design-guidelines/#35-abstraction-levels-across-document-tiers","title":"3.5 Abstraction Levels Across Document Tiers","text":"Content Type Design Doc Macro Plan Micro Plan System invariants Define (named: INV-1, ...) Reference Refine to GIVEN/WHEN/THEN Modeling decisions (modeled/simplified/omitted) Define with justification Summarize N/A Module boundaries (behavioral) Define contract Reference + annotate per-PR Implement Interface signatures (Go code) No Frozen post-freeze PR Full code File paths No Inventory + per-PR Exact <code>file:line</code> Fidelity trade-offs Define with alternatives Reference Deviation log if changed"},{"location":"contributing/templates/design-guidelines/#4-module-architecture-principles","title":"4. Module Architecture Principles","text":""},{"location":"contributing/templates/design-guidelines/#41-two-layer-architecture","title":"4.1 Two-Layer Architecture","text":"<p>BLIS is organized as two layers:</p> <p>Layer 1: Simulation Kernel \u2014 domain-agnostic DES infrastructure - Event queue (min-heap with deterministic tie-breaking) - Clock management (next-event time advance) - Randomness (PartitionedRNG with named subsystems) - Statistics collection (accumulators decoupled from state) - Experiment control (seed, horizon, configuration)</p> <p>Layer 2: Domain Modules \u2014 inference-platform-specific model logic, each behind an interface</p> <p>The kernel provides the execution substrate. Domain modules define what is being simulated. The kernel never contains inference-specific logic; domain modules never manage the event queue or clock directly.</p>"},{"location":"contributing/templates/design-guidelines/#42-domain-module-map","title":"4.2 Domain Module Map","text":"<p>BLIS models an extensible distributed inference platform \u2014 not any single system. llm-d, vLLM, SGLang, Mooncake, and LMCache are all target systems whose behaviors should be expressible through BLIS's module composition.</p> <pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502            Cluster Orchestrator              \u2502\n                    \u2502  (shared clock, event dispatch, aggregation) \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502          \u2502          \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 Admission\u2502 \u2502 Router  \u2502 \u2502AutoScaler\u2502\n                    \u2502 Policy   \u2502 \u2502 Policy  \u2502 \u2502 Policy   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502                       \u2502                       \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Instance 0  \u2502        \u2502 Instance 1  \u2502        \u2502 Instance N  \u2502\n        \u2502\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502        \u2502             \u2502        \u2502             \u2502\n        \u2502\u2502 Scheduler  \u2502\u2502        \u2502   . . .     \u2502        \u2502   . . .     \u2502\n        \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502        \u2502             \u2502        \u2502             \u2502\n        \u2502\u2502 Latency   \u2502\u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502\u2502 Model     \u2502\u2502\n        \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\n        \u2502\u2502 KV Cache  \u2502\u2502\n        \u2502\u2502 Manager   \u2502\u2502\n        \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\n        \u2502\u2502 Batch     \u2502\u2502\n        \u2502\u2502 Formation \u2502\u2502\n        \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Module Responsibility Interface Today Status Admission Accept/reject requests at cluster entry <code>AdmissionPolicy</code> (single method) Implemented, frozen Router Select target instance for admitted requests <code>RoutingPolicy</code> (single method) Implemented, frozen Scheduler Order queued requests within an instance <code>InstanceScheduler</code> (single method) Implemented, frozen Priority Compute request priority for scheduler <code>PriorityPolicy</code> (single method) Implemented, frozen KV Cache Manager Allocate/release/cache KV blocks <code>KVStore</code> (11 methods) Implemented AutoScaler Add/remove instances based on load signals <code>AutoScalePolicy</code> (planned) Target \u2014 PR11 Latency Model Estimate step execution time <code>LatencyModel</code> (5 methods) Implemented \u2014 <code>NewLatencyModel</code> factory Batch Formation Select requests from queue for next step <code>BatchFormation</code> (1 method: <code>FormBatch</code>) Implemented \u2014 <code>NewBatchFormation</code> factory Workload Generator Produce request streams from specs/traces <code>GenerateRequests()</code> function Implemented Trace Recorder Record decisions for analysis <code>SimulationTrace</code> Implemented Metrics Collector Aggregate per-request and system-level metrics <code>CollectRawMetrics()</code> function Implemented <p>\"Target\" means: the module exists as embedded logic today but lacks an interface boundary. Refactoring PRs will extract the interface. The guidelines define what that interface contract should look like.</p>"},{"location":"contributing/templates/design-guidelines/#43-module-contract-template","title":"4.3 Module Contract Template","text":"<p>Every module (current or target) is defined by this contract:</p> <ol> <li>Observes \u2014 what state does this module read? (Its inputs)</li> <li>Controls \u2014 what decisions does this module make? (Its outputs)</li> <li>Owns \u2014 what mutable state does this module exclusively manage?</li> <li>Invariants \u2014 what must always hold for this module?</li> <li>Events \u2014 what events does this module produce or consume? (Exogenous/endogenous classification)</li> <li>Extension friction \u2014 how many files must change to add one more variant of this module?</li> </ol> <p>Example \u2014 Router module contract:</p> Aspect Contract Observes Request metadata (tokens, prefix, SLO class, tenant), per-instance snapshots (queue depth, batch size, KV utilization, cache hit rate, pending requests), cluster clock Controls Target instance selection, priority hint for downstream scheduler Owns No mutable state (stateless policy; any affinity tracking is internal to the policy instance) Invariants Must select from non-empty snapshot list. Must return a valid instance ID. Selection must be deterministic given same inputs and RNG state. Events Consumes: <code>AdmissionDecisionEvent</code> (admitted=true). Produces: <code>RoutingDecisionEvent</code>. Extension friction 3 files to add a new routing algorithm (policy file, bundle.go registration, cmd/root.go validation message)"},{"location":"contributing/templates/design-guidelines/#44-real-system-correspondence","title":"4.4 Real-System Correspondence","text":"<p>BLIS modules map to real inference system components, but the mapping is many-to-many \u2014 BLIS must be able to express behaviors from multiple real systems:</p> BLIS Module llm-d vLLM SGLang Mooncake Router Endpoint Picker N/A (single-instance) N/A (single-instance) Global scheduler Scheduler N/A (engine-internal) <code>Scheduler</code> class <code>Scheduler</code> class Prefill/decode scheduler KV Cache Manager N/A (engine-internal) <code>BlockManager</code> <code>RadixCache</code> Distributed KV pool Latency Model N/A (real latency) N/A (real latency) N/A (real latency) N/A (real latency) AutoScaler HPA / custom N/A N/A N/A Batch Formation N/A (engine-internal) Continuous batching Chunked prefill Disaggregated batching <p>The design implication: module interfaces must be abstract enough to express all these variants, but concrete enough to capture the behavioral differences that matter for analysis. The modeling decisions table (Section 2.1) determines where on this spectrum each module sits.</p>"},{"location":"contributing/templates/design-guidelines/#45-the-touch-point-rule","title":"4.5 The Touch-Point Rule","text":"<p>When a design doc introduces a new module boundary, it must specify the expected touch-point count for adding one more variant. The following are reference targets based on what works well in the current codebase:</p> Extension Type Reference Target Current Reality New policy template ~3 files 3 files (meets target) New KV cache tier ~4 files 4-5 files (acceptable) New config parameter ~2 files 2 files (meets target \u2014 post-#381) New observable metric ~3 files 6 files (exceeds \u2014 known friction) New latency model backend ~2 files 2 files (meets target) New batch formation strategy ~2 files 2 files (meets target) <p>If a design exceeds the reference target, the design doc must acknowledge the friction and explain whether it's acceptable (justified complexity) or whether structural improvement should happen first or concurrently. The goal is awareness, not rigidity \u2014 some modules genuinely require more touch points, but that should be a conscious choice, not an accident.</p>"},{"location":"contributing/templates/design-guidelines/#46-parallel-development-enablement","title":"4.6 Parallel Development Enablement","text":"<p>Module boundaries enable parallel development when:</p> <ol> <li>Interfaces are stable \u2014 frozen after a designated PR (as done with policy interfaces after PR8)</li> <li>Contracts are testable independently \u2014 each module can be tested with mock implementations of adjacent modules</li> <li>No shared mutable state \u2014 modules communicate through defined inputs/outputs, not through shared globals or reaching through struct fields</li> <li>Bridge types at boundaries \u2014 when two modules in different packages need shared types, bridge types live in the lower-level package (e.g., <code>RouterState</code> in <code>sim/</code> not <code>sim/cluster/</code>)</li> </ol> <p>A design doc for a new module must demonstrate that two developers could work on different implementations of that module's interface simultaneously, with only behavioral contract tests to keep them aligned.</p>"},{"location":"contributing/templates/design-guidelines/#5-extension-framework","title":"5. Extension Framework","text":""},{"location":"contributing/templates/design-guidelines/#51-extension-taxonomy","title":"5.1 Extension Taxonomy","text":"<p>There are four fundamentally different ways to extend BLIS:</p> Type What It Is Example Scope Policy Template New algorithm behind an existing interface New routing algorithm (e.g., power-of-two-choices) Single file + registration Subsystem Module New module with its own interface, events, and state AutoScaler, P/D disaggregation New interface + integration Backend Swap Alternative implementation of an internal module SGLang latency model alongside vLLM, RadixCache alongside block-based KV New implementation behind existing interface Tier Composition Layering an existing module with additional behavior NVMe KV tier wrapping GPU+CPU tiers, network latency wrapping base latency Decorator/delegation over existing module <p>Understanding which type an extension is determines which recipe to follow.</p>"},{"location":"contributing/templates/design-guidelines/#52-recipe-policy-template","title":"5.2 Recipe: Policy Template","text":"<p>Adding a new algorithm behind a frozen interface.</p> <p>This is the lightest extension type. The interface already exists, the factory already exists, the CLI integration pattern is established.</p> <p>Prerequisites: The target policy interface must exist and be stable.</p> <p>Contract the new template must satisfy: - Implements the interface (single method) - Deterministic given same inputs and RNG state - No side effects beyond its owned state (if stateful) - Handles edge cases defined by the interface (e.g., empty snapshot list for routing policies)</p> <p>Steps: Documented in CLAUDE.md \"Adding New Policy Templates\" \u2014 these guidelines don't duplicate, they reference.</p> <p>Parallel development: Multiple policy templates for the same interface can be developed simultaneously by different contributors with zero coordination, since they share only the interface contract.</p>"},{"location":"contributing/templates/design-guidelines/#53-recipe-subsystem-module","title":"5.3 Recipe: Subsystem Module","text":"<p>Adding an entirely new module with its own behavioral contract.</p> <p>This is the most architecturally significant extension type. Examples: AutoScaler (PR11), framework adapters (PR15).</p> <p>Design doc must define:</p> <ol> <li>Module contract (per Section 4.3 template \u2014 observes, controls, owns, invariants, events, extension friction)</li> <li>Interface design \u2014 prefer single-method interfaces where possible. If multiple methods are needed, justify each one. The interface must be testable with a mock.</li> <li>Event integration \u2014 what new event types are added to the cluster event queue? What are their priority constants? How do they interact with existing events?</li> <li>State ownership \u2014 what new mutable state does this module introduce? Who creates it, who reads it, who mutates it? No shared mutable state across module boundaries.</li> <li>Failure modes \u2014 what happens when the module fails? (Error return? Panic? Graceful degradation?) Which error handling boundary applies? (See CLAUDE.md Engineering Principles)</li> <li>Default behavior \u2014 what does BLIS do when this module is not configured? (A no-op default must exist so existing workflows are unaffected)</li> <li>Configuration surface \u2014 what CLI flags / YAML config does this add? Validated how?</li> </ol> <p>Design doc must demonstrate: - The module can be tested in isolation with mocked dependencies - Adding the module doesn't change behavior of existing tests (no-op default) - The extension friction for adding a second implementation is within reference targets from Section 4.5</p> <p>Parallel development: Once the interface is agreed and frozen, the module implementation and its integration into the cluster orchestrator can proceed independently.</p>"},{"location":"contributing/templates/design-guidelines/#54-recipe-backend-swap","title":"5.4 Recipe: Backend Swap","text":"<p>Alternative implementation of an internal module that currently has no interface.</p> <p>This is the extension type that requires refactoring first. Examples: SGLang latency model, continuous-vs-chunked batching.</p> <p>Two-phase approach:</p> <p>Phase A \u2014 Extract interface (refactoring PR): - Identify the hardcoded logic (e.g., <code>Step()</code> calling the blackbox latency estimator directly) - Define an interface that captures the behavioral contract of the existing implementation - Extract the existing implementation behind the new interface - Verify: all existing tests pass, no behavior change, the factory returns the existing implementation by default</p> <p>Phase B \u2014 Add alternative (extension PR): - Implement the new backend behind the extracted interface - Add configuration to select between backends (CLI flag or YAML) - Add behavioral tests for the new backend - Verify: existing tests still pass when default backend is selected</p> <p>Design doc must cover both phases. Phase A is often the harder design challenge \u2014 getting the interface abstraction right so that it accommodates both the existing and new backends without over-generalizing.</p> <p>Key principle: The interface should capture what the module does (behavioral contract), not how one particular backend does it. If the interface has methods that only make sense for one backend, it's too specific.</p> <p>Parallel development: After Phase A merges, multiple backends can be developed simultaneously.</p>"},{"location":"contributing/templates/design-guidelines/#55-recipe-tier-composition","title":"5.5 Recipe: Tier Composition","text":"<p>Layering additional behavior onto an existing module via delegation.</p> <p>This is the pattern used by <code>TieredKVCache</code> (wraps <code>KVCacheState</code>) and could be used for network latency (wraps base latency model), caching layers, or monitoring wrappers.</p> <p>Design doc must define: - Which existing interface is being composed - What new behavior the wrapper adds (offloading, caching, monitoring, latency injection) - How metrics aggregate across tiers (the wrapper must expose the same metrics interface as the inner module, combining results appropriately) - How configuration selects the composition (e.g., <code>--kv-cpu-blocks &gt; 0</code> triggers tiered wrapping)</p> <p>Key principle: The wrapper must satisfy the same interface contract as the inner module. Any caller that works with the inner module must work identically with the wrapper (Liskov substitution).</p> <p>Parallel development: Wrappers are naturally parallelizable \u2014 different tiers or decorators can be developed independently as long as they compose through the same interface.</p>"},{"location":"contributing/templates/design-guidelines/#56-extension-checklist","title":"5.6 Extension Checklist","text":"<p>Before submitting a design doc for any extension, verify:</p> <ul> <li> Extension type identified (policy template / subsystem module / backend swap / tier composition)</li> <li> Correct recipe followed</li> <li> Module contract defined (observes / controls / owns / invariants / events / friction)</li> <li> No-op default exists (existing behavior unchanged when extension not configured)</li> <li> Interface testable with mocks (no concrete dependencies leaked through interface)</li> <li> Parallel development path described (what can proceed independently after interface freeze?)</li> <li> Touch-point count specified for adding one more variant</li> <li> DES checklist from Section 2.6 completed</li> <li> New randomness declared (PartitionedRNG subsystem name)</li> <li> Event priority constants assigned (if new events introduced)</li> </ul>"},{"location":"contributing/templates/design-guidelines/#6-anti-patterns-with-evidence","title":"6. Anti-Patterns with Evidence","text":"<p>Every anti-pattern in this section traces to a real bug, a real friction point, or a real design doc failure from BLIS's development history.</p>"},{"location":"contributing/templates/design-guidelines/#61-design-doc-anti-patterns","title":"6.1 Design Doc Anti-Patterns","text":"Anti-Pattern What Happened Lesson The Type Catalog The original design doc contained ~600 lines of Go struct definitions. Within 2 PRs, <code>RouterState</code> went from a 5-section struct to <code>Snapshots + Clock</code>. <code>InstanceScheduler</code> went from 3 methods to 1. The macro plan called these \"aspirational signatures that diverged during implementation.\" Describe module boundaries behaviorally, not as Go types. Types change; contracts persist. Fidelity for Its Own Sake The original design doc specified <code>ShadowKVModel</code> with <code>PrefixHashes</code>, <code>EstimatedUtilization</code>, <code>EvictionQueue</code> \u2014 none of which were needed for the analysis questions BLIS answers today. Apply Banks et al.'s six model scoping criteria. If you can't name the analysis question a component answers, defer it. Silent Staleness The design doc's <code>RoutingDecision</code> struct had 8 fields. The implemented version has 4. No mechanism flagged the divergence. Meanwhile, micro-plan deviation logs caught discrepancies with the macro plan because there was an explicit comparison step. Higher-level docs need an explicit freshness mechanism. Design docs should version their decisions and mark which are implemented (e.g., a Decision Status column: Proposed / Implemented / Superseded). Missing Trade-off Rationale Several design doc decisions had no alternatives listed. When implementation revealed a better approach, there was no record of why the original choice was made. Every non-obvious decision must list alternatives considered and why they were rejected. This is what makes the PR12 pre-design valuable \u2014 each of its 9 decisions has explicit rationale."},{"location":"contributing/templates/design-guidelines/#62-module-architecture-anti-patterns","title":"6.2 Module Architecture Anti-Patterns","text":"Anti-Pattern What Happened Lesson Shotgun Surgery Adding <code>InstanceID</code> to per-request metrics (#181) required changes in 4 files. Three construction sites for <code>RequestMetrics</code> existed, and one was missed initially. Use canonical constructors. Design docs for new types must specify whether a canonical constructor is needed. Destructive Read <code>KVStore.PendingTransferLatency()</code> both queried and cleared the accumulated latency. Callers couldn't distinguish \"no latency\" from \"already consumed.\" Identified as blocking LMCache integration. Query methods must be pure. If a method needs to both query and clear state, provide separate <code>Get()</code> and <code>Consume()</code> methods. Interface Leaking Implementation <code>KVStore</code> interface has 11 methods, several exposing block-level semantics. A distributed KV cache like LMCache doesn't think in blocks \u2014 it thinks in tokens and layers. The interface encodes vLLM's implementation model, not the abstract behavioral contract. (#246) Design interfaces around the behavioral contract (allocate space, check cache, release space), not around one implementation's data model. Monolith Method <code>Simulator.Step()</code> was 152 lines mixing 4 concerns. Decomposed into named phase methods (<code>scheduleBatch</code>, <code>executeBatchStep</code>, <code>processCompletions</code>, <code>scheduleNextStep</code>). Each module's logic should be callable through its interface. When a method contains logic for multiple modules, extract each into its module's interface method. Config Mixing Concerns <code>SimConfig</code> combined 23 fields from 8 concerns. Decomposed into 6 embedded sub-configs with canonical constructors (<code>NewKVCacheConfig</code>, etc.). Adding a field now touches 2 files; the compiler catches all call sites. Group configuration by module. Each module's config should be independently specifiable and validatable."},{"location":"contributing/templates/design-guidelines/#63-des-specific-anti-patterns","title":"6.3 DES-Specific Anti-Patterns","text":"Anti-Pattern What Happened Lesson Golden Tests Without Invariant Tests The golden dataset test for codellama expected 499 completions. The code silently dropped one request on KV allocation failure (#183). The golden test encoded the bug as the expected value for months. Golden tests answer \"did the output change?\" Invariant tests answer \"is the output correct?\" Both are needed (Banks et al.: verification \u2260 validation). Non-Deterministic Map Iteration Five sites iterated Go maps to accumulate floats or determine output ordering. Go map iteration is randomized, violating the determinism invariant. All randomness must flow through PartitionedRNG. Map iteration is a hidden source of non-determinism. Sort keys before any iteration that affects output. Mixing Exogenous and Endogenous The cluster workload generator (exogenous) was tightly coupled to the cluster simulator (endogenous). Impossible to replay the same workload through different configurations without re-generating. PR10 decoupled them. Exogenous inputs must be separable from endogenous logic. This enables the fundamental simulation experiment: same input, different configuration, compare results."},{"location":"contributing/templates/design-guidelines/#64-the-meta-lesson","title":"6.4 The Meta-Lesson","text":"<p>All these anti-patterns share one root cause: the design was expressed in terms of implementation rather than behavior. When a design doc specifies Go structs instead of behavioral contracts, the implementation becomes the specification. When a module boundary is defined by its internal data model instead of its observable behavior, the boundary can't accommodate a second implementation.</p> <p>Describe what a module does and what it guarantees, not how it's built.</p> <p>If a design doc follows this principle, its content stays durable, its modules stay extensible, its interfaces accommodate multiple backends, and parallel development is naturally enabled \u2014 because contributors agree on behavior, not on implementation.</p>"},{"location":"contributing/templates/design-guidelines/#references","title":"References","text":"<ol> <li>Banks, J., Carson, J. S., Nelson, B. L., &amp; Nicol, D. M. Discrete-Event System Simulation (5th ed.). Pearson. \u2014 Foundational text on DES methodology, model scoping, input modeling, output analysis, V&amp;V.</li> <li>Misra, J. \"Distributed Discrete-Event Simulation.\" Computing Surveys, Vol. 18, No. 1, March 1986. \u2014 Formal correctness proofs for DES, causal ordering, simulation as formal property.</li> <li>BLIS Issue #183 \u2014 Golden dataset encoded a silently-dropped request. Conservation invariant test would have caught it on day one.</li> <li>BLIS PR12 Pre-Design (<code>docs/plans/archive/pr12-architectural-predesign.md</code>) \u2014 Gold standard for decision records: 9 decisions, each with trade-off analysis.</li> <li>BLIS Hardening Design (<code>docs/plans/archive/2026-02-18-hardening-antipattern-refactoring-design.md</code>) \u2014 Extension scenario analysis identifying friction points for autoscaling, LMCache, heterogeneous HW, new engines.</li> </ol>"},{"location":"contributing/templates/hypothesis/","title":"Hypothesis Experiment Template","text":"<p>For Claude: Use this template when creating a new hypothesis experiment in <code>hypotheses/&lt;name&gt;/</code>.</p>"},{"location":"contributing/templates/hypothesis/#findingsmd-structure","title":"FINDINGS.md Structure","text":"<p>Every experiment's <code>FINDINGS.md</code> MUST contain these sections:</p> <pre><code># &lt;Hypothesis Name&gt;\n\n**Status:** Confirmed | Confirmed with nuance | Partially confirmed | Refuted | Inconclusive\n**Resolution:** &lt;one of: Clean confirmation | Confirmation with wrong mechanism | Confirmation with bug discovery | Partial confirmation with surprise | Refuted \u2014 mechanism not plausible | Refuted \u2014 system design flaw | Refuted \u2014 wrong mental model | Inconclusive \u2014 parameter-dependent | Converged to open question&gt;\n**Family:** &lt;one of: Workload/arrival | Scheduler invariants | Performance-regime | Structural model | Robustness/failure-mode | Cross-policy comparative&gt;\n**VV&amp;UQ:** &lt;one of: Verification | Validation | UQ&gt;\n**Tier:** &lt;tier number \u2014 see hypotheses/README.md for definitions&gt;\n**Type:** Deterministic | Statistical (&lt;subtype&gt;)\n**Date:** YYYY-MM-DD\n**Rounds:** &lt;number of experiment-review rounds to convergence&gt;\n\n## Hypothesis\n\n&gt; &lt;Quoted hypothesis statement \u2014 intuitive claim about system behavior&gt;\n\n## Experiment Design\n\n**Classification:** &lt;Deterministic | Statistical/Dominance | Statistical/Monotonicity | Statistical/Equivalence | Statistical/Pareto&gt;\n\n**Configurations compared:**\n- A: &lt;description + exact CLI flags&gt;\n- B: &lt;description + exact CLI flags&gt;\n\n**Controlled variables:** &lt;what is held constant&gt;\n**Varied variable:** &lt;what differs between A and B&gt;\n**Seeds:** &lt;list of seeds used&gt;\n**Preconditions verified:** &lt;what was checked before running&gt;\n\n## Results\n\n&lt;Comparison tables with per-seed values&gt;\n\n## Root Cause Analysis\n\n&lt;Why the results are what they are \u2014 trace through the code/architecture.\nEvery causal claim MUST cite file:line (RCV-1).\nEvery \"surprise\" MUST include a first-principles calculation (RCV-2).\nMust explain the mechanism AND its direction (RCV-3).\nIf a mechanism is proposed, describe the control experiment that would confirm it (RCV-4).&gt;\n\n## Devil's Advocate (RCV-5)\n\n&lt;Before sending to review, argue the OPPOSITE of your conclusion.&gt;\n\n**If this is \"Confirmed,\" argue why it might be Refuted:**\n&lt;2-3 sentences&gt;\n\n**If this is \"Refuted,\" argue why it might be Confirmed:**\n&lt;2-3 sentences&gt;\n\n## Findings Classification\n\n| Finding | Type | Action |\n|---------|------|--------|\n| &lt;finding 1&gt; | Confirmation / Bug / New rule / New invariant / Design limitation / Surprise / Open question | &lt;issue number or \"documented here\"&gt; |\n\n## Standards Audit\n\nFindings checked against docs/contributing/standards/:\n- [ ] Any violations of existing rules? &lt;list or \"none found\"&gt;\n- [ ] Any new rules needed? &lt;list or \"none\"&gt;\n- [ ] Any new invariants needed? &lt;list or \"none\"&gt;\n- [ ] Any existing rules/invariants confirmed? &lt;list or \"none\"&gt;\n\n## Scope and Limitations (RCV-6)\n\n- **Operating point tested:** &lt;blocks, rate, seeds, instances, routing, etc.&gt;\n- **Parameters findings depend on:** &lt;what must be true for these results to hold&gt;\n- **What was NOT tested:** &lt;parameter ranges, workloads, configs not covered&gt;\n- **Generalizability:** &lt;does this finding generalize, or is it specific to this config?&gt;\n- **Uncertainty quantification:** &lt;for any threshold or boundary finding, report confidence intervals. For any \"confirmed\" result, estimate the probability of holding under parameter variation. If UQ was not performed, state \"UQ not performed \u2014 single operating point.\"&gt;\n\n## Evidence Quality\n\n| Metric | Value | Confidence |\n|--------|-------|------------|\n| &lt;primary metric&gt; | &lt;value&gt; | High / Medium / Low \u2014 &lt;why&gt; |\n| Sample size | &lt;seeds \u00d7 configs \u00d7 requests&gt; | &lt;assessment&gt; |\n| Mechanism | &lt;proposed mechanism&gt; | &lt;confidence + whether control confirms&gt; |\n\n## Implications for Users\n\n&lt;Practical guidance derived from this experiment&gt;\n\n## Reproducing\n\ncd hypotheses/&lt;name&gt;\n./run.sh\n</code></pre>"},{"location":"contributing/templates/hypothesis/#runsh-structure","title":"run.sh Structure","text":"<pre><code>#!/bin/bash\n# &lt;Hypothesis name&gt;\n# &lt;One-line description&gt;\n# Usage: ./run.sh [--rebuild]\n\nset -euo pipefail\nSCRIPT_DIR=\"$(cd \"$(dirname \"$0\")\" &amp;&amp; pwd)\"\nsource \"$SCRIPT_DIR/../lib/harness.sh\"\n\nsetup_experiment \"${1:-}\"\n\n# -- Experiment sections -----------------------------------------------\n# Each experiment: use blis_run with appropriate timeout tier.\n# NOTE: blis_run (not run_sim) \u2014 define your own run_sim() wrapper if needed.\n#\n# Example (basic):\n#   blis_run $TIMEOUT_STANDARD \"$RESULTS_DIR/config_a.txt\" \\\n#       --model \"$MODEL\" --num-instances 4 --seed 42 \\\n#       --workload-spec \"$WORKLOAD_YAML\" --log error\n#\n# Example (with stderr capture for robustness experiments):\n#   blis_run $TIMEOUT_STANDARD \"$RESULTS_DIR/config_a.txt\" \\\n#       --stderr \"$RESULTS_DIR/config_a_stderr.txt\" \\\n#       --model \"$MODEL\" --num-instances 4 --seed 42 --log error\n#\n# Example (with per-request JSON):\n#   blis_run $TIMEOUT_STANDARD \"$RESULTS_DIR/config_a.txt\" \\\n#       --model \"$MODEL\" --num-instances 4 --seed 42 --log error \\\n#       --results-path \"$RESULTS_DIR/config_a_results.json\"\n#\n# Example (robustness/stress \u2014 non-zero exit expected, use || true under set -e):\n#   blis_run $TIMEOUT_EXTENDED \"$RESULTS_DIR/stress.txt\" \\\n#       --stderr \"$RESULTS_DIR/stress_stderr.txt\" \\\n#       --model \"$MODEL\" --num-instances 4 --seed 42 --log error || true\n#\n# For KV-constrained experiments, add pre-flight check (advisory, never aborts):\n#   preflight_kv_check 800 16 512  # total_blocks, block_size (default: 16), max_input\n# ----------------------------------------------------------------------\n</code></pre>"},{"location":"contributing/templates/hypothesis/#analyzepy-structure","title":"analyze.py Structure","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Analysis script for &lt;hypothesis name&gt;.\n\nParses BLIS multi-block output and produces comparison tables.\n\"\"\"\nimport sys\nfrom pathlib import Path\n\n# Import shared helpers\nsys.path.insert(0, str(Path(__file__).resolve().parent.parent / \"lib\"))\nfrom analyze_helpers import parse_blis_output, check_for_timeout\n\n# -- Analysis code --------------------------------------------------------\n# Use parse_blis_output(filepath) to get metrics dict.\n# The dict includes a 'timed_out' flag \u2014 check it before computing ratios.\n#\n# Example:\n#   metrics = parse_blis_output(sys.argv[1])\n#   if metrics[\"timed_out\"]:\n#       print(f\"  SKIPPED (timeout)\", file=sys.stderr)\n#   else:\n#       print(f\"  TTFT mean: {metrics['ttft_mean']:.2f} ms\")\n# -------------------------------------------------------------------------\n</code></pre>"},{"location":"contributing/templates/macro-plan-prompt/","title":"Macro plan prompt","text":"<p>You are operating inside a real repository with full code access.</p> <p>You are tasked with producing a MACRO-LEVEL DESIGN PLAN for a:</p> <p>\"Major Feature Expansion with Architectural Changes.\"</p> <p>This is a program-level plan that defines objectives, a concept model, architectural evolution, and an ordered PR series.</p> <p>This is NOT implementation planning. This is NOT a micro-level design. This is NOT speculative architecture.</p> <p>You MUST inspect the real codebase before proposing changes.</p> <p>====================================================================== PREREQUISITE \u2014 DESIGN GUIDELINES ======================================================================</p> <p>Before writing a macro plan, read and internalize:</p> <ul> <li><code>docs/contributing/templates/design-guidelines.md</code> \u2014 BLIS design guidelines   covering DES foundations, module architecture, and extension framework.</li> </ul> <p>The macro plan must be consistent with these guidelines. Specifically: - Building blocks must use the MODULE CONTRACT TEMPLATE from Section 4.3   (observes / controls / owns / invariants / events / extension friction) - New events must be classified as EXOGENOUS or ENDOGENOUS (Section 2.2) - Modeling decisions must apply the SIX SCOPING CRITERIA (Section 2.1) - PRs must identify their EXTENSION TYPE (Section 5.1): policy template,   subsystem module, backend swap, or tier composition - Building blocks should map to REAL SYSTEM COMPONENTS (Section 4.4)</p> <p>If no design doc exists for the feature being planned, one must be created (per the guidelines) before or alongside the macro plan.</p> <p>====================================================================== ABSTRACTION LEVEL RULE (NON-NEGOTIABLE) ======================================================================</p> <p>The macro plan describes WHAT to build and in WHAT ORDER, not HOW to implement each piece. Enforce these boundaries strictly:</p> <p>ALLOWED in macro plan:   - Behavioral descriptions of module contracts (prose)   - Frozen interface signatures (Go code ONLY for interfaces whose     freeze PR has already merged \u2014 these are facts, not aspirations)   - File path inventories (which packages, which files, per PR)   - LOC estimates per PR (sizing heuristics)   - CLI flag names and configuration surface area   - Brief YAML/config examples   - Architecture diagrams (text-based)</p> <p>PROHIBITED in macro plan:   - Method implementations (belongs in micro plan)   - Struct field lists (belongs in micro plan)   - Pre-freeze interface signatures in Go syntax (describe behaviorally:     \"a single-method interface that selects a target instance given     request metadata and per-instance snapshots\")   - Factory function code (belongs in micro plan)   - Test code (belongs in micro plan)</p> <p>THE TEST: Is this content a FACT about merged code, or an ASPIRATION about code to be written? Facts are allowed. Aspirations must be described behaviorally, not as Go code.</p> <p>WHY THIS MATTERS: The original macro plan had ~200 lines of Go code including full method implementations (TokenBucket.Admit(), PartitionedRNG.ForSubsystem()). These diverged during micro-planning and became misleading. Behavioral descriptions survive intact because they describe WHAT, not HOW.</p> <p>====================================================================== PHASE 0 \u2014 REPOSITORY RECON (MANDATORY) ======================================================================</p> <p>Before proposing anything:</p> <p>1) Identify and summarize:    - Top-level packages/modules and responsibilities    - Core data structures and interfaces    - Key invariants and assumptions encoded in the system    - CLI entrypoints and current flag surface    - Configuration flow    - Existing extension points (map to design guidelines Section 4.2)    - Areas of tight coupling or fragility    - Current module boundaries vs. target module map (guidelines 4.2)</p> <p>2) Clearly separate:    - Confirmed facts (from inspection \u2014 cite file:line for every claim)    - Inferred behavior (explicitly labeled as inference)    - Open uncertainties</p> <p>3) Identify architectural constraints that must not be violated.</p> <p>No invented abstractions. No imagined extension points. Everything must be grounded in code inspection with source references.</p> <p>ANTI-HALLUCINATION RULE: For every behavioral claim about existing code, provide a file:line citation. If you cannot cite it, mark it as \"UNVERIFIED\" and do not rely on it in subsequent phases.</p> <p>====================================================================== PHASE 1 \u2014 HIGH-LEVEL OBJECTIVES AND MODEL SCOPING ======================================================================</p> <p>Define:</p> <ul> <li>3-7 crisp objectives</li> <li>Explicit non-goals</li> <li>Compatibility constraints</li> <li>Performance constraints</li> <li>Backward compatibility guarantees</li> <li>Operational/CLI stability expectations</li> </ul> <p>Be precise.</p> <p>MODEL SCOPING (required \u2014 applies Banks et al. criteria):</p> <p>For this feature expansion, answer:</p> <p>1) What ANALYSIS QUESTIONS does this feature help answer?    (e.g., \"What is the optimal routing policy for heterogeneous    hardware?\" or \"How does autoscaling latency affect tail TTFT?\")</p> <p>2) What must be MODELED to answer those questions?</p> <p>3) What can be SIMPLIFIED without affecting the analysis?    (e.g., \"model scaling latency as fixed delay, not warmup curve\")</p> <p>4) What can be OMITTED entirely?    (e.g., \"network partitions between instances \u2014 out of scope\")</p> <p>Present as a table:</p> Component Modeled Simplified Omitted Justification (example) Scaling latency -- Fixed delay, not warmup curve -- Same steady-state throughput; warmup matters only for sub-minute scale-up (example) Network partitions -- -- Yes Not needed for routing policy comparison; add if modeling failure recovery <p>For each \"Simplified\" entry, state what real-system behavior is lost and under what conditions it would matter in the Justification column. This is the fidelity trade-off record \u2014 it prevents \"fidelity for its own sake\" and enables future refinement with clear upgrade paths.</p> <p>====================================================================== PHASE 2 \u2014 CONCEPT MODEL ======================================================================</p> <p>Before diving into architecture, define the system at the level a human would explain it on a whiteboard:</p> <p>1) Building Blocks (3-7 named components)</p> <p>For EACH building block, provide the MODULE CONTRACT:    - Name and one-sentence responsibility    - OBSERVES: what state does this module read? (its inputs)    - CONTROLS: what decisions does this module make? (its outputs)    - OWNS: what mutable state does it exclusively manage?    - INVARIANTS: what must always hold for this module?    - EVENTS: what events does it produce or consume?      Classify each as EXOGENOUS (driven by external input) or      ENDOGENOUS (driven by internal state transitions).    - EXTENSION FRICTION: how many files must change to add one more      variant? (Reference targets from design guidelines Section 4.5)</p> <p>No building block may have more than one core responsibility.</p> <p>2) Interaction Model    - Who calls whom (directional arrows)    - Data flow between blocks (what crosses each boundary)    - Ownership transfer rules (when does data change owners?)</p> <p>3) System Invariants    - What must ALWAYS hold (e.g., \"clock never decreases\")    - What must NEVER happen (e.g., \"no cross-instance state mutation\")    - Causality constraints (ordering guarantees)    - DES-specific: state vs. statistics separation \u2014 which data is      simulation state (evolves the system) vs. derived statistics      (output for analysis)?</p> <p>4) Extension Points    - Where do new behaviors plug in? (behavioral description of      interface contract + responsibility)    - What is the default behavior for each extension point?    - What is the FIRST non-default implementation planned?</p> <p>5) State Ownership Map    - For every piece of mutable state: exactly one owner    - Shared state must be explicitly identified and justified</p> <p>6) Real-System Correspondence    - Map each building block to the real inference system component(s)      it models. Use a table:</p> Building Block llm-d vLLM SGLang Other <ul> <li>BLIS models an extensible distributed inference platform, not any      single system. The table ensures the architecture stays grounded      in real systems while remaining general enough to express      behaviors from multiple targets.</li> </ul> <p>THE CONCEPT MODEL MUST FIT IN UNDER 80 LINES. (Increased from 60 to accommodate module contracts and real-system correspondence. If it exceeds 80, the design is too complex \u2014 simplify before proceeding.)</p> <p>Every PR in Phase 6 must map to adding or modifying a specific building block from this model. If a PR cannot be described as a building block change, redesign the PR or the model.</p> <p>====================================================================== PHASE 3 \u2014 ARCHITECTURAL RISK REGISTER ======================================================================</p> <p>For every non-obvious architectural decision in the concept model:</p> Decision Assumption Validation Method Cost if Wrong Gate <ul> <li>DECISION: The choice being made</li> <li>ASSUMPTION: What must be true for this to work</li> <li>VALIDATION: How to test cheaply (mock study, prototype, analysis, spike)</li> <li>COST IF WRONG: What breaks \u2014 count the affected PRs</li> <li>GATE: When validation must complete (before which PR)</li> </ul> <p>Example row: | Shared-clock event loop | O(N) scan per event is fast for N&lt;=16 |   Benchmark N=16, 10K events | PR 3 rework | Before PR 3 merge |</p> <p>MANDATORY VALIDATION RULE: If cost-of-being-wrong &gt;= 3 PRs of rework, validation is MANDATORY. The plan must include a spike/mock study PR or pre-PR validation step.</p> <p>For each validation gate, specify: - Exact success criteria (not \"looks good\" \u2014 measurable outcomes) - Abort plan (what changes if validation fails)</p> <p>====================================================================== PHASE 4 \u2014 PROPOSED ARCHITECTURAL EVOLUTION ======================================================================</p> <p>Only after the concept model and risk register:</p> <ul> <li>Describe how the architecture evolves FROM current TO concept model.</li> <li>Map each structural change to a concept model building block.</li> <li>Identify refactors that are strictly enabling (no behavior change).</li> <li>Explicitly describe what remains unchanged.</li> <li>For each new extension point: what is the default implementation and   when does the first non-default implementation arrive?</li> </ul> <p>Highlight risks and invariants.</p> <p>No premature generalization. No extension point without a concrete non-default implementation planned.</p> <p>ABSTRACTION LEVEL CHECK: This section describes the evolution BEHAVIORALLY. Do NOT include Go code here. Describe what each module will do and what contract it will satisfy, not how it will be implemented. Interface signatures appear only in the output sections (Section G, defined in the Output Format section below) and only for already-frozen interfaces.</p> <p>FIDELITY TRADE-OFFS: For each architectural simplification, state: - What real-system behavior is being approximated - What analysis questions the approximation still answers correctly - Under what conditions the approximation breaks down - What the upgrade path looks like (which future PR refines this)</p> <p>====================================================================== PHASE 5 \u2014 CROSS-CUTTING INFRASTRUCTURE ======================================================================</p> <p>Plan ONCE for the entire PR series. Each item must be assigned to a specific PR (defined in Phase 6) or handled as a standalone preparatory PR. Phases 5 and 6 are co-developed: sketch the PR series first, then assign cross-cutting items, then finalize both.</p> <p>1) Shared Test Infrastructure    - First: identify existing shared test packages in the codebase.      Build on them rather than duplicating or replacing them.    - New test helper packages, shared fixtures, golden dataset types    - Which PR creates them? Which PRs consume them?    - How do golden datasets evolve as the system grows?    - INVARIANT TESTS: which system invariants must have companion tests?      (Golden tests alone are insufficient \u2014 see design guidelines 6.3)</p> <p>2) Documentation Maintenance    - CLAUDE.md update triggers: new packages, new files, changed file      organization, completed plan milestones, new CLI flags    - Who updates CLAUDE.md? (The PR that causes the change.)    - README update triggers and ownership    - Design guidelines compliance: does this feature expansion require      updating the target module map in the design guidelines?</p> <p>3) CI Pipeline Changes    - New test packages to add to CI    - New linter rules or build steps    - Performance regression benchmarks</p> <p>4) Dependency Management    - New external dependencies (justify each one)    - Version pinning strategy</p> <p>5) Interface Freeze Schedule    - Which PR freezes which interface?    - What must be validated before freezing?    - After freeze: parallel development of templates/implementations      can proceed independently</p> <p>No item may be left as \"address when needed.\" This applies to cross-cutting infrastructure (test helpers, CI, docs), not to feature packages which are detailed in Phase 6.</p> <p>====================================================================== PHASE 6 \u2014 ORDERED PR SERIES (PR0 ... PRN) ======================================================================</p> <p>Design an incremental, independently reviewable and mergeable PR sequence.</p> <p>For EACH PR, provide TWO TIERS:</p> <p>--- TIER 1: Human Review Summary (target 15 lines, max 25) ---</p> <ul> <li>Title</li> <li>Building Block Change: Which concept model block is added/modified?</li> <li>Extension Type (from design guidelines Section 5.1):<ul> <li>policy template: new algorithm behind existing interface</li> <li>subsystem module: new interface + new events</li> <li>backend swap: alternative implementation, requires interface extraction</li> <li>tier composition: decorator/wrapper over existing module</li> </ul> </li> <li>Motivation: Why does this PR exist? (1-2 sentences)</li> <li>Scope: In / Out (bullet points)</li> <li>Behavioral Guarantees: What MUST hold after this PR merges?   (Use named contracts: BC-1, BC-2, etc.)</li> <li>Risks: Top 1-2 risks and how they're mitigated</li> <li>Cross-Cutting: Which shared infra does this PR create or consume?</li> <li>Validation Gate: Does this PR depend on a risk register validation?</li> </ul> <p>--- TIER 2: Implementation Guide (for micro-planning) ---</p> <ul> <li>Architectural Impact (what changes structurally)</li> <li>API Surface Changes (new types, interfaces, methods \u2014 described   BEHAVIORALLY, not as Go code. E.g., \"New single-method interface   for latency estimation, replacing hardcoded Step() logic\")</li> <li>CLI Changes (new flags, changed behavior)</li> <li>Test Categories (unit, integration, regression, golden, invariant)</li> <li>Documentation Updates (CLAUDE.md, README, design guidelines if needed)</li> <li>Extension Friction: how many files to add one more variant of the   new type/interface? Compare against reference targets (guidelines 4.5)</li> <li>Parallel Development: after this PR merges, what can proceed   independently? (e.g., \"multiple routing policies can be developed   in parallel after the interface freeze in this PR\")</li> <li>Why this PR is independently reviewable</li> <li>Why it introduces no dead code</li> </ul> <p>Constraints:</p> <ul> <li>Each PR must deliver one cohesive building block change.</li> <li>Each PR must be exercisable immediately after merge.   \"Exercisable\" means: via CLI, OR via tests that demonstrate the   new behavior. Internal refactors exercised by passing existing tests   are valid. Scaffolding exercised only by future PRs is NOT valid.</li> <li>No speculative scaffolding.</li> <li>No unused interfaces.</li> <li>No flags that aren't exercised.</li> <li>Each PR must identify its extension type and follow the corresponding   recipe from the design guidelines (Section 5.2-5.5).</li> </ul> <p>====================================================================== PHASE 7 \u2014 DEPENDENCY DAG &amp; PARALLELISM ======================================================================</p> <p>Provide:</p> <ul> <li>A PR dependency graph (partial order).</li> <li>Parallelizable workstreams.</li> <li>Merge sequencing guidance.</li> <li>Validation gate placement (from risk register).</li> <li>Interface freeze points (from Phase 5, item 5: Interface Freeze   Schedule) \u2014 mark which PRs unlock parallel development of multiple   implementations.</li> <li>Integration risk notes.</li> </ul> <p>Maximize safe parallelism.</p> <p>====================================================================== PHASE 8 \u2014 DESIGN BUG PREVENTION ======================================================================</p> <p>Include:</p> <ul> <li>Invariants that must never be broken (reference concept model).</li> <li>Regression surfaces (which existing tests must keep passing).</li> <li>Cross-PR state migration risks (data format changes across PRs).</li> <li>Backward compatibility enforcement.</li> </ul> <p>Common architectural failure modes and how this plan prevents them:</p> <p>General:   - Scaffolding creep (dead code introduced \"for later\").     Prevention: every struct field, method, and flag must be exercised     by the end of the PR that introduces it.   - Documentation drift (CLAUDE.md diverges from reality).     Prevention: the PR that causes the change updates CLAUDE.md in the     same commit. No deferred documentation.   - Test infrastructure duplication (helpers copied across packages).     Prevention: shared test packages created in an early PR, consumed     by all subsequent PRs (specified in Phase 5, item 1).   - Golden dataset staleness (regression baselines not updated).     Prevention: every PR that changes output format includes a golden     dataset regeneration step with a verification command.   - Interface over-specification (freezing APIs too early).     Prevention: interfaces are frozen only after at least one     non-default implementation is designed (even if not yet built).     The freeze PR must demonstrate the interface accommodates two     implementations.</p> <p>DES-specific (from design guidelines Section 6):   - The Type Catalog trap: macro plan includes Go struct definitions     that diverge from implementation. Prevention: describe modules     behaviorally, not as Go types.   - Fidelity for its own sake: modeling components that don't affect     any analysis question. Prevention: every component must trace to     a modeling decision in Phase 1.   - Golden tests without invariant tests: characterization tests that     encode bugs as expected values. Prevention: every subsystem with     golden tests must have companion invariant tests.   - Mixing exogenous and endogenous: tight coupling between workload     generation and simulation logic that prevents replay experiments.     Prevention: exogenous inputs must be separable from endogenous     simulation logic.   - Interface leaking implementation: interfaces that encode one     backend's data model instead of the abstract behavioral contract.     Prevention: interfaces must accommodate at least two backends     (even if only one is implemented initially).</p> <p>Module architecture (from design guidelines Section 6.2):   - Shotgun surgery: multiple construction sites for the same type.     Prevention: canonical constructors for types constructed in &gt;1 place.   - Destructive reads: methods that both query and clear state.     Prevention: separate Get() and Consume() methods.   - Monolith methods: single methods containing logic for multiple     modules. Prevention: each module's logic callable through its     own interface.   - Config mixing concerns: single config struct combining unrelated     parameters. Prevention: group configuration by module.</p> <p>====================================================================== OUTPUT FORMAT (STRICT) ======================================================================</p> <p>A) Executive Summary (under 15 lines \u2014 synthesize the elevator pitch:    what is being built, why, how many PRs, key milestones) B) Repository Recon Summary C) High-Level Objectives + Non-Goals + Model Scoping Table D) Concept Model (under 80 lines \u2014 building blocks with module    contracts, interactions, invariants, extension points, real-system    correspondence) E) Architectural Risk Register F) Architectural Evolution (current -&gt; target, mapped to concept model,    described BEHAVIORALLY \u2014 no Go code) G) Frozen Interface Reference (ONLY for interfaces whose freeze PR    has already merged \u2014 Go signatures with per-PR annotations.    Include both interfaces frozen BY this plan's PRs and pre-existing    frozen interfaces that this plan depends on. Omit entirely if no    interfaces are frozen yet.) H) Cross-Cutting Infrastructure Plan I) PR Plan (PR0...PRN, Tier 1 + Tier 2 per PR) J) Dependency DAG K) Design Bug Prevention Checklist</p> <p>CONTEXT BUDGET RULE: Sections A, C, and D are the human-review core and must be concise. I-Tier-1 summaries should target 15 lines each (max 25). All other sections are reference material consulted on demand. The plan should be structured so a human can review the core sections (A + C + D + all I-Tier-1 summaries) without needing to read the rest.</p> <p>ABSTRACTION LEVEL CHECK (final gate): Before submitting, verify: - Section F contains ZERO lines of Go code - Section G contains ONLY frozen interface signatures (merged code) - Sections A-E and H-K contain ZERO Go code - All pre-freeze interfaces are described behaviorally - All module contracts use the template from Phase 2, not Go structs</p> <p>======================================================================</p> <p>Quality bar:</p> <ul> <li>Grounded in real code with file:line citations.</li> <li>No hallucinated modules or behaviors.</li> <li>No dead code.</li> <li>No bloated PRs.</li> <li>Must withstand expert review.</li> <li>Must be realistic and implementable.</li> <li>Concept model must be simple enough to explain verbally in 2 minutes.</li> <li>Consistent with design guidelines (docs/contributing/templates/design-guidelines.md).</li> <li>Every building block has a module contract.</li> <li>Every PR has an extension type.</li> <li>Every modeling decision traces to an analysis question.</li> </ul> <p>====================================================================== LIVING DOCUMENT PROTOCOL ======================================================================</p> <p>This plan will evolve. When updating:</p> <p>1) Add a dated revision note at the top explaining what changed and why. 2) If a risk register validation fails, document the finding and the    resulting plan changes explicitly. 3) Never silently change a PR's behavioral guarantees \u2014 if contracts    change, note the old contract, new contract, and reason. 4) Track completed PRs by marking their status in the PR plan section. 5) After each PR merges, check: does the concept model still accurately    describe the system? If not, update it. A stale concept model is    worse than no concept model.</p> <p>======================================================================</p> <p>Think deeply before answering. Inspect before designing. Validate before committing. Scope before modeling. Describe behavior, not implementation.</p>"},{"location":"contributing/templates/macro-plan/","title":"Macro Plan Template (Multi-PR Feature Plan)","text":"<p>This template defines the output format for a macro-level implementation plan. Use this when a feature spans 2+ PRs and requires a dependency DAG between them.</p> <p>For Claude Code users</p> <p>The <code>writing-plans</code> skill generates plans from this template automatically. The agent prompt version is at <code>macro-plan-prompt.md</code>.</p> <p>Prerequisite: Before writing a macro plan, read design-guidelines.md \u2014 it covers DES foundations, module architecture, and the extension framework. The macro plan must be consistent with these guidelines.</p> <p>For comprehensive guidance: The detailed prompt version contains analytical frameworks (Banks et al. model scoping criteria), constraints (concept model 80-line limit, mandatory validation when cost-of-being-wrong \u2265 3 PRs), and worked examples for each section.</p>"},{"location":"contributing/templates/macro-plan/#abstraction-level-rule","title":"Abstraction Level Rule","text":"<p>A macro plan describes what to build and in what order, not how to implement it.</p> <ul> <li>\u2705 Module contracts (observes / controls / owns / invariants / events / extension friction)</li> <li>\u2705 Frozen interface signatures (facts about already-merged code)</li> <li>\u274c Go struct definitions with field lists</li> <li>\u274c Method implementations</li> <li>\u274c File paths with line numbers</li> <li>\u274c Interface signatures for code that hasn't been written yet</li> </ul> <p>The staleness test: Would this content mislead if the implementation changes during micro-planning? If yes, it's too concrete for a macro plan.</p>"},{"location":"contributing/templates/macro-plan/#sections","title":"Sections","text":""},{"location":"contributing/templates/macro-plan/#a-executive-summary","title":"A) Executive Summary","text":"<p>Under 15 lines. What this feature adds, why it matters, and how it fits in the system. This is the human-review core \u2014 a reviewer should understand the plan's scope from this section alone.</p>"},{"location":"contributing/templates/macro-plan/#b-high-level-objectives-and-model-scoping","title":"B) High-Level Objectives and Model Scoping","text":"<ul> <li>3\u20137 crisp objectives</li> <li>Explicit non-goals</li> <li>Model scoping table: what is modeled, simplified, or omitted, with justification for each simplification</li> </ul> Component Modeled Simplified Omitted Justification (example) Scaling latency \u2014 Fixed delay \u2014 Same steady-state throughput"},{"location":"contributing/templates/macro-plan/#c-concept-model","title":"C) Concept Model","text":"<p>Building blocks and their interactions. Each block uses the module contract template from design-guidelines.md Section 4.3:</p> <ul> <li>Observes: What signals does this module read?</li> <li>Controls: What decisions does it make?</li> <li>Owns: What mutable state does it manage exclusively?</li> <li>Invariants: What properties must always hold?</li> <li>Events: What events does it produce and consume?</li> <li>Extension friction: How many files to add one more variant?</li> </ul> <p>Include a real-system correspondence table mapping building blocks to llm-d / vLLM / SGLang equivalents:</p> Building Block llm-d vLLM SGLang Other (example) Router EPP + routing filter N/A N/A \u2014 <p>The concept model must fit in under 80 lines.</p>"},{"location":"contributing/templates/macro-plan/#d-architectural-evolution","title":"D) Architectural Evolution","text":"<p>Current architecture \u2192 target architecture. What new packages, interfaces, or event types are introduced?</p>"},{"location":"contributing/templates/macro-plan/#e-pr-series-ordered","title":"E) PR Series (Ordered)","text":"<p>Each PR entry includes: - Scope and deliverables - Extension type: policy template / subsystem module / backend swap / tier composition - Module contract (what this PR guarantees to the next) - Dependencies (which PRs must merge before this one starts) - Whether interfaces are frozen or flexible at this stage</p>"},{"location":"contributing/templates/macro-plan/#f-frozen-interfaces","title":"F) Frozen Interfaces","text":"<p>Interfaces that are stable and can be developed against in parallel. Only include signatures for code that has already been merged. Aspirations about unwritten code use behavioral descriptions, not Go syntax.</p>"},{"location":"contributing/templates/macro-plan/#g-dependency-dag","title":"G) Dependency DAG","text":"<p>Visual or tabular dependency graph showing which PRs can be parallelized and which are sequential.</p>"},{"location":"contributing/templates/macro-plan/#h-risk-register","title":"H) Risk Register","text":"<p>For each non-obvious architectural decision: - Risk description - Cost of being wrong (in PRs of rework) - If cost-of-being-wrong \u2265 3 PRs, validation is MANDATORY with specific success criteria and an abort plan - Abort plan (what changes if validation fails)</p>"},{"location":"contributing/templates/macro-plan/#i-cross-cutting-infrastructure","title":"I) Cross-Cutting Infrastructure","text":"<p>Test infrastructure, documentation, and CI changes \u2014 each assigned to a specific PR. CLAUDE.md update ownership: the PR that causes the change updates it.</p>"},{"location":"contributing/templates/macro-plan/#j-extension-friction-assessment","title":"J) Extension Friction Assessment","text":"<p>For each new module boundary: how many files must change to add one more variant? Compare against reference targets in design-guidelines.md Section 4.5.</p>"},{"location":"contributing/templates/macro-plan/#k-design-bug-prevention","title":"K) Design Bug Prevention","text":"<p>Checklist to prevent common macro-plan anti-patterns:</p> <ul> <li> No scaffolding creep (every struct/method/flag exercised by end of introducing PR)</li> <li> No documentation drift (CLAUDE.md updated in same PR that causes the change)</li> <li> No test infrastructure duplication (shared packages created early)</li> <li> No golden dataset staleness (regeneration steps included)</li> <li> No DES-specific anti-patterns: Type Catalog trap, fidelity for its own sake, golden without invariant, mixing exogenous and endogenous events</li> <li> New events classified as exogenous (external arrivals) or endogenous (internal scheduling)</li> <li> State vs statistics separation maintained (event-driven state vs aggregated statistics)</li> <li> Model scoping applies Banks et al. criteria (what questions does this answer? what must be modeled vs simplified vs omitted?)</li> </ul>"},{"location":"contributing/templates/macro-plan/#appendix-repository-recon","title":"Appendix: Repository Recon","text":"<p>Before writing the plan, inspect the codebase and document: - Top-level packages and responsibilities - Core data structures and interfaces - Key invariants and assumptions - Current module boundaries vs target module map - Architectural constraints that must not be violated</p> <p>Separate: confirmed facts (with file:line citations), inferred behavior (labeled), and open uncertainties.</p>"},{"location":"contributing/templates/micro-plan-prompt/","title":"Micro plan prompt","text":"<p>You are operating inside a real repository with full code access.</p> <p>You are tasked with producing a PR-SPECIFIC IMPLEMENTATION PLAN that combines: 1. Design rigor (behavioral contracts, architecture validation) 2. Executable task breakdown (TDD, bite-sized steps, verifications)</p> <p>The source of work may be: - A section in an approved Macro Plan (e.g., \"Phase 2, PR 4\") - One or more GitHub issues (e.g., \"#183, #189, #195\") - A design document (e.g., \"docs/plans/2026-02-18-hardening-design.md\") - A feature request or bug report description</p> <p>This plan has TWO AUDIENCES: 1) A human reviewer who validates behavioral correctness 2) Automated agents (via executing-plans skill) who execute the tasks</p> <p>The plan must be comprehensive enough that agents can implement WITHOUT additional codebase exploration.</p> <p>====================================================================== DOCUMENT HEADER (REQUIRED) ======================================================================</p> <p>Every plan MUST start with this exact header format:</p> <pre><code># [PR Title] Implementation Plan\n\n&gt; **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.\n\n**Goal:** [One sentence a non-contributor could understand \u2014 what capability does this PR add? Avoid type names, package paths, or implementation jargon.]\n\n**The problem today:** [2-3 sentences explaining what's missing or broken without this PR. What can't users or the system do? Why does it matter?]\n\n**What this PR adds:** [Numbered list of 2-4 concrete capabilities, each explained in plain language with a brief example. E.g., \"Decision traces \u2014 a log of every routing decision: 'request_42 was sent to instance_2 because it had the highest score of 0.87'\"]\n\n**Why this matters:** [1-2 sentences connecting this PR to the broader project vision. How does this enable downstream work?]\n\n**Architecture:** [2-3 sentences about the technical approach \u2014 packages, key types, integration points. Implementation jargon is OK here since the motivation is already established above.]\n\n**Source:** [Link to the source of work. Examples:\n  - Macro plan: \"Phase 2, PR 4 in docs/plans/macro-plan.md\"\n  - Issues: \"GitHub issues #183, #189, #195, #196, #197, #198, #199, #200\"\n  - Design doc: \"docs/plans/2026-02-18-hardening-design.md\"\n  - Feature request: \"GitHub issue #42\"]\n\n**Closes:** [Issue numbers this PR will close on merge, using GitHub closing keywords.\n  Omit if the source is a macro plan section with no linked issues.\n  Examples:\n  - \"Fixes #183, fixes #189, fixes #195\"\n  - \"Closes #42\"\n  - \"N/A \u2014 source is macro plan, no linked issues\"]\n\n**Behavioral Contracts:** See Part 1, Section B below\n\n---\n</code></pre> <p>The header has TWO audiences reading in order: 1. A human reviewer who needs to understand WHY before HOW (Goal \u2192 Problem \u2192 What \u2192 Why) 2. An implementing agent who needs the technical approach (Architecture)</p> <p>====================================================================== PHASE 0 \u2014 COMPONENT CONTEXT ======================================================================</p> <p>Identify this PR's place in the system architecture:</p> <p>1) Which building block is being added or modified? 2) What are the adjacent blocks it interacts with? 3) What invariants does this PR touch? 4) What state ownership changes (if any)? 5) Construction Site Audit: For every struct this PR adds fields to,    grep for ALL places that struct is constructed (struct literals,    factory functions). List each site with file:line. If there are    multiple construction sites, the plan MUST either:    a) Add a canonical constructor and refactor all sites, OR    b) Update every site explicitly (list each in a task)</p> <p>Then inspect ONLY the relevant parts of the repository.</p> <p>List confirmed facts (with file:line citations). Flag anything from the source document (macro plan, design doc, or issue description) that doesn't match current code as a DEVIATION \u2014 these must be resolved before implementation begins.</p> <p>====================================================================== OUTPUT FORMAT (STRICT) ======================================================================</p> <p>--- PART 1: Design Validation (Human Review, target &lt;120 lines) ---</p> <p>A) Executive Summary (5-10 lines)    - What this PR builds (plain language, not type/package names)    - Where it fits in the system (what comes before it, what depends on it)    - Adjacent blocks it interacts with    - Any DEVIATION flags from Phase 0</p> <p>B) Behavioral Contracts (Phase 1)    - 3-15 named contracts (BC-1, BC-2, ...)    - Format: GIVEN / WHEN / THEN / MECHANISM    - Grouped: positive contracts, negative contracts, error handling</p> <p>C) Component Interaction (Phase 2)    - Component diagram (text)    - API contracts    - State changes and ownership</p> <p>D) Deviation Log (Phase 3)    - Compare micro plan vs source document    - Table: | Source Says | Micro Does | Reason |</p> <p>E) Review Guide (Phase 7-B)    - The tricky part    - What to scrutinize    - What's safe to skim    - Known debt</p> <p>--- PART 2: Executable Implementation (Agent Execution) ---</p> <p>F) Implementation Overview (Phase 4 summary)    - Files to create/modify (one-line each)    - Key decisions    - Confirmation: no dead code, all paths exercisable</p> <p>G) Task Breakdown (Phase 4 detailed)    - 6-12 tasks in TDD format (see Phase 4 template below)    - Continuous execution (no pause points between tasks)    - Each task: test \u2192 fail \u2192 implement \u2192 pass \u2192 lint \u2192 commit</p> <p>H) Test Strategy (Phase 6)    - Map contracts to tasks/tests    - Golden dataset update strategy    - Shared test infrastructure usage</p> <p>I) Risk Analysis (Phase 7-A)    - Risks with likelihood/impact/mitigation</p> <p>--- PART 3: Quality Assurance ---</p> <p>J) Sanity Checklist (Phase 8)    - Pre-implementation verification    - All items from Phase 8 template</p> <p>--- APPENDIX: File-Level Implementation Details ---</p> <p>K) Detailed specifications    - Complete function signatures with doc comments    - Struct definitions    - Event execution logic    - Metric aggregation rules    - RNG subsystem usage    - Any behavioral subtleties (file:line citations)</p> <p>====================================================================== PHASE 1 \u2014 BEHAVIORAL CONTRACTS (Human-Reviewable) ======================================================================</p> <p>This defines what this PR guarantees. Use named contracts (BC-1, BC-2, ...) that can be referenced in tests, reviews, and future PRs.</p> <p>For each contract:</p> <p>BC-N:    - GIVEN    - WHEN    - THEN    - MECHANISM:  (optional but recommended) <p>Group contracts into:</p> <p>1) Behavioral Contracts (what MUST happen)    - Normal operation    - Edge cases    - Backward compatibility</p> <p>2) Negative Contracts (what MUST NOT happen)    - Invariant violations this PR could cause    - Cross-boundary state leaks    - Performance regressions</p> <p>3) Error Handling Contracts    - What happens on invalid input    - What happens on resource exhaustion    - Panic vs error return vs log-and-continue (be explicit)</p> <p>TARGET: 3-15 contracts per PR. Pure refactoring PRs with no new behavior may have as few as 3. More than 15 means the PR may be too large.</p> <p>No vague wording. \"Should\" is banned \u2014 use \"MUST\" or \"MUST NOT.\"</p> <p>THEN CLAUSE QUALITY GATE: Every THEN clause must describe OBSERVABLE BEHAVIOR, not internal structure. The THEN clause directly becomes the test assertion \u2014 a structural THEN produces a structural test.</p> <p>Check each THEN clause against this filter: - Does it contain a concrete type name? \u2192 Rewrite to describe behavior   BAD:  \"THEN it returns a ConstantPriority\"   GOOD: \"THEN it returns a policy that computes 0.0 for any request\" - Does it reference an internal field? \u2192 Rewrite to describe output   BAD:  \"THEN the router's scoreCache has 3 entries\"   GOOD: \"THEN the next routing decision uses cached affinity\" - Does it reproduce a formula? \u2192 Rewrite to describe ordering/outcome   BAD:  \"THEN score equals 0.6cacheHit + 0.4*(1-load)\"   GOOD: \"THEN instances with higher cache hit rates rank higher\" - Does it survive a refactor? \u2192 If renaming a struct or changing an   internal algorithm would invalidate this THEN, it is structural</p> <p>====================================================================== PHASE 2 \u2014 COMPONENT INTERACTION (Human-Reviewable) ======================================================================</p> <p>Describe this PR's building block and how it connects to the system. This is the \"box-and-arrow\" view, NOT the file-level view.</p> <p>1) Component Diagram (text-based)    - This PR's component and its responsibility    - Adjacent components (existing or new)    - Data flow direction between them    - What crosses each boundary (types, not implementations)</p> <p>2) API Contracts    - New interfaces or types (signature + one-line semantics)    - Method preconditions and postconditions    - Failure modes and how callers handle them</p> <p>3) State Changes    - New mutable state and its owner    - State lifecycle (created when, destroyed when, accessed by whom)</p> <p>4) Extension Friction Assessment    - For the main new type/field this PR adds, count: how many files      must change to add ONE more field of the same kind?    - If &gt;3 files, document whether this is acceptable or whether a      structural improvement should happen first/concurrently    - This is not a blocker \u2014 it's awareness for the reviewer</p> <p>TARGET: under 40 lines. Infrastructure PRs that introduce multiple interacting types may go up to 60 lines with justification. Beyond 60 lines, the PR scope is likely too broad.</p> <p>====================================================================== PHASE 3 \u2014 DEVIATION LOG ======================================================================</p> <p>Compare this micro plan against the source document (macro plan section, design doc, or issue description).</p> <p>For each difference:</p> Source Says Micro Plan Does Reason <p>Categories of deviation: - SIMPLIFICATION: Source specified more than needed at this stage - CORRECTION: Source was wrong about existing code or behavior - DEFERRAL: Feature moved to a later PR (explain why) - ADDITION: Something the source missed - SCOPE_CHANGE: Issue description expanded or narrowed during investigation</p> <p>If there are zero deviations, state \"No deviations from source document.\"</p> <p>====================================================================== PHASE 4 \u2014 EXECUTABLE TASK BREAKDOWN ======================================================================</p> <p>Break implementation into 6-12 tasks following TDD principles. Each task is completable in one focused session (~30-45 minutes).</p> <p>Execution is continuous \u2014 all tasks run sequentially without pausing for human input. Execution only stops on test failure, lint failure, or build error. Group tasks into logical sections (e.g., core types, integration, edge cases) for readability, but these are NOT pause points.</p> <p>TASK TEMPLATE:</p>"},{"location":"contributing/templates/micro-plan-prompt/#task-n-componentfeature-name","title":"Task N: [Component/Feature Name]","text":"<p>Contracts Implemented: BC-X, BC-Y (reference Phase 1)</p> <p>Files: - Create: <code>exact/path/to/file.go</code> - Modify: <code>exact/path/to/existing.go:123-145</code> (line range if known) - Test: <code>exact/path/to/test_file.go</code></p> <p>Step 1: Write failing test for [specific contract]</p> <p>Context: [1-2 sentences explaining what we're testing and why]</p> <pre><code>// Complete test code here\n// Include setup, execution, assertions\nfunc TestComponent_Scenario_Behavior(t *testing.T) {\n    // GIVEN [precondition from contract]\n\n    // WHEN [action from contract]\n\n    // THEN [expected outcome from contract]\n    assert.Equal(t, expected, actual)\n}\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>go test ./path/to/package/... -run TestComponent_Scenario -v</code> Expected: FAIL with \"[expected error message]\"</p> <p>Step 3: Implement minimal code to satisfy contract</p> <p>Context: [1-2 sentences about the implementation approach]</p> <p>In <code>path/to/file.go</code>: <pre><code>// Complete implementation code\n// Include type definitions, method signatures, logic\ntype Component struct {\n    field1 Type1\n    field2 Type2\n}\n\nfunc (c *Component) Method(param Type) (ReturnType, error) {\n    // implementation\n}\n</code></pre></p> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>go test ./path/to/package/... -run TestComponent_Scenario -v</code> Expected: PASS</p> <p>Step 5: Run lint check</p> <p>Run: <code>golangci-lint run ./path/to/package/...</code> Expected: No new issues (pre-existing issues OK, don't fix them)</p> <p>Step 6: Commit with contract reference</p> <pre><code>git add path/to/file.go path/to/test_file.go\ngit commit -m \"feat(package): implement Component.Method (BC-X, BC-Y)\n\n- Add Component type with Method\n- Implement contract BC-X: [brief description]\n- Implement contract BC-Y: [brief description]\n\nCo-Authored-By: Claude &lt;noreply@anthropic.com&gt;\"\n</code></pre> <p>REPEAT TASK TEMPLATE for each task (6-12 total).</p> <p>IMPORTANT TASK DESIGN RULES:</p> <ol> <li> <p>Each task implements 1-3 related contracts - don't split single    contracts across tasks, don't pack unrelated contracts together</p> </li> <li> <p>Complete code in every step - no \"add validation\" or \"implement logic\"    without showing the exact code</p> </li> <li> <p>Exact commands with expected output - agent should know if verification    succeeded or failed</p> </li> <li> <p>Reference shared test infrastructure - use existing helpers from    shared packages (e.g., sim/internal/testutil), don't duplicate</p> </li> <li> <p>Golden dataset updates - if task changes output format or metrics,    include step to update testdata/goldendataset.json with regeneration command</p> </li> <li> <p>Dependency ordering - tasks must be ordered so each can build on    previous completed work</p> </li> <li> <p>No dead code - every struct field, every method, every parameter must    be used by the end of the task or a subsequent task in this PR</p> </li> <li> <p>Commit messages - use conventional commits format with contract references    (feat/fix/refactor/test/docs)</p> </li> <li> <p>Behavioral assertions only - every assertion in a test must    verify OBSERVABLE BEHAVIOR, not internal structure. Apply the    refactor survival test: \"Would this test still pass if the    implementation were completely rewritten but the behavior preserved?\"</p> </li> </ol> <p>PROHIBITED assertion patterns (structural \u2014 these break on refactor):    - Type assertions: <code>policy.(*ConcreteType)</code> \u2014 test behavior instead    - Internal field access: <code>obj.internalField</code> \u2014 test through public API    - Exact formula reproduction: <code>assert.Equal(score, 0.6*cache + 0.4*load)</code>      \u2014 test the ranking/ordering outcome instead    - Implementation count: <code>assert.Equal(len(obj.items), 3)</code> \u2014 test      what the items produce, not how many there are</p> <p>REQUIRED assertion patterns (behavioral \u2014 these survive refactor):    - Observable output: <code>assert.Equal(policy.Compute(req, clock), 0.0)</code>    - Behavioral outcome: <code>assert.Equal(decision.TargetInstance, 1)</code>    - Invariant verification: <code>assert.Equal(completed+queued+running+dropped, injected)</code>    - Ordering/ranking: <code>assert.True(scoreA &gt; scoreB)</code> when contract says      A should rank higher than B</p> <ol> <li> <p>THEN clauses must be behavioral - if a behavioral contract's     THEN clause contains a concrete type name, internal field name, or     implementation detail, rewrite the THEN clause BEFORE writing the     test. The THEN clause drives the assertion; a structural THEN     produces a structural test.</p> <p>BAD:  \"THEN it returns a *ConstantPriority\" GOOD: \"THEN it returns a policy that computes 0.0 for any request\"</p> <p>BAD:  \"THEN the router's scoreCache has 3 entries\" GOOD: \"THEN the next routing decision uses cached scores (latency &lt; uncached)\"</p> <p>BAD:  \"THEN the score equals 0.6cacheHit + 0.4(1-load)\" GOOD: \"THEN instances with higher cache hit rates score higher than        instances with lower cache hit rates, all else being equal\"</p> </li> </ol> <p>====================================================================== PHASE 5 \u2014 REMOVED (Merged into Phase 4 Task Verification) ======================================================================</p> <p>Exercisability is proven by the task-level verification steps. No separate section needed.</p> <p>====================================================================== PHASE 6 \u2014 TEST STRATEGY ======================================================================</p> <p>Map contracts to tasks and tests:</p> Contract Task Test Type Test Name / Description BC-1 Task 1 Unit TestFoo_GivenX_ThenY BC-2 Task 1 Unit TestFoo_GivenZ_ThenW BC-3 Task 2 Golden TestCluster_SingleInstance_MatchesGolden ... ... ... ... <p>Test types: - Unit: specific function/method behavior - Integration: cross-component or CLI-level - Golden: regression against known-good output (testdata/goldendataset.json) - Invariant: system law that must hold regardless of output values (see req 6) - Failure: error paths, panics, edge cases - Benchmark: performance-sensitive paths (optional)</p> <p>Additional requirements:</p> <ol> <li> <p>Shared test infrastructure: Use existing helpers from shared test    packages (e.g., sim/internal/testutil). If new helpers are needed, add    them to the shared package in an early task \u2014 not duplicated locally.</p> </li> <li> <p>Golden dataset updates: If this PR changes output format or adds new    metrics, document:</p> </li> <li>Which task updates the golden dataset</li> <li>Exact regeneration command</li> <li> <p>How to verify the update is correct (compare key metrics)</p> </li> <li> <p>Lint requirements: <code>golangci-lint run ./...</code> must pass with zero new    issues. Pre-existing issues are acceptable; do not fix unrelated lint    warnings (scope creep).</p> </li> <li> <p>Test naming convention: Use BDD-style names that describe the scenario:    <code>TestType_Scenario_Behavior</code> (e.g., <code>TestTokenBucket_CapacityExceeded_RejectsRequest</code>)</p> </li> <li> <p>Test isolation: Each test must be independently runnable (no order    dependencies). Use table-driven tests for multiple scenarios of the same behavior.</p> </li> <li> <p>Invariant tests alongside golden tests (MANDATORY): Golden dataset    tests are characterization tests \u2014 they capture what the code does, not    what the code should do. If the code has a bug when the golden dataset    is generated, the test encodes the bug as the expected value. This has    happened: issue #183 found that the codellama golden dataset expected 499    completions because one request was silently dropped \u2014 a bug that the    golden test perpetuated instead of catching.</p> </li> </ol> <p>Rule: Every golden dataset test MUST be paired with at least one    invariant test that verifies a system law derived from the specification    (not from running the code). Invariant tests answer \"is the code correct?\"    while golden tests answer \"did the code change?\"</p> <p>Key invariants for this simulator (derived from CLAUDE.md):    - Request conservation: completed + still_queued + still_running + dropped_unservable = injected    - KV block conservation: allocated_blocks + free_blocks = total_blocks    - Clock monotonicity: simulation clock never decreases    - Causality: arrival_time \u2264 enqueue_time \u2264 schedule_time \u2264 completion_time    - Determinism: same seed produces byte-identical output across runs</p> <p>When adding a golden test, ask: \"If this golden value were wrong, would    any other test catch it?\" If the answer is no, add an invariant test.    If this PR touches request lifecycle, KV cache, or metrics, at least one    invariant test MUST be added or extended.</p> <p>====================================================================== PHASE 7 \u2014 RISK ANALYSIS &amp; REVIEW GUIDE ======================================================================</p> <p>PART A: Risks</p> <p>For each risk: - Risk description - Likelihood (low/medium/high) - Impact (low/medium/high) - Mitigation (specific test or design choice) - Which task mitigates the risk</p> <p>PART B: Review Guide (for the human reviewer)</p> <p>In 5-10 lines, tell the reviewer:</p> <p>1) THE TRICKY PART: What's the most subtle or error-prone aspect? 2) WHAT TO SCRUTINIZE: Which contract(s) are hardest to verify? 3) WHAT'S SAFE TO SKIM: Which parts are mechanical/boilerplate? 4) KNOWN DEBT: Any pre-existing issues encountered but not fixed?</p> <p>This section exists because human attention is scarce. Direct it to where it matters most.</p> <p>====================================================================== PHASE 8 \u2014 DESIGN SANITY CHECKLIST ======================================================================</p> <p>Before implementation, verify:</p> <p>Plan-specific checks: - [ ] No unnecessary abstractions. - [ ] No feature creep beyond PR scope. - [ ] No unexercised flags or interfaces. - [ ] No partial implementations. - [ ] No breaking changes without explicit contract updates. - [ ] No hidden global state impact. - [ ] All new code will pass golangci-lint. - [ ] Shared test helpers used from existing shared test package (not duplicated locally). - [ ] CLAUDE.md updated if: new files/packages added, file organization       changed, plan milestone completed, new CLI flags added. - [ ] No stale references left in CLAUDE.md. - [ ] Documentation DRY: If this PR modifies a canonical source (docs/contributing/standards/rules.md, docs/contributing/standards/invariants.md, docs/contributing/standards/principles.md, docs/contributing/extension-recipes.md), all working copies in the source-of-truth map are updated. If a new file is added, it appears in the CLAUDE.md File Organization tree. - [ ] Deviation log reviewed \u2014 no unresolved deviations. - [ ] Each task produces working, testable code (no scaffolding). - [ ] Task dependencies are correctly ordered. - [ ] All contracts are mapped to specific tasks. - [ ] Golden dataset regeneration documented (if needed). - [ ] Construction site audit completed (Phase 0, item 5) \u2014 all struct       construction sites listed and covered by tasks. - [ ] If this PR is part of a macro plan, the macro plan status is updated.</p> <p>Antipattern rules (full details in docs/contributing/standards/rules.md): - [ ] R1: No silent <code>continue</code>/<code>return</code> dropping data - [ ] R2: Map keys sorted before float accumulation or ordered output - [ ] R3: Every new CLI flag validated (zero, negative, NaN, Inf) - [ ] R4: All struct construction sites audited for new fields - [ ] R5: Resource allocation loops handle mid-loop failure with rollback - [ ] R6: No <code>logrus.Fatalf</code> or <code>os.Exit</code> in <code>sim/</code> packages - [ ] R7: Invariant tests alongside any golden tests - [ ] R8: No exported mutable maps - [ ] R9: <code>*float64</code> for YAML fields where zero is valid - [ ] R10: YAML strict parsing (<code>KnownFields(true)</code>) - [ ] R11: Division by runtime-derived denominators guarded - [ ] R12: Golden dataset regenerated if output changed - [ ] R13: New interfaces work for 2+ implementations - [ ] R14: No method spans multiple module responsibilities - [ ] R15: Stale PR references resolved - [ ] R16: Config params grouped by module - [ ] R17: Routing scorer signals documented for freshness tier - [ ] R18: CLI flag values not silently overwritten by defaults.yaml - [ ] R19: Unbounded retry/requeue loops have circuit breakers - [ ] R20: Detectors and analyzers handle degenerate inputs (empty, skewed, zero)</p> <p>====================================================================== APPENDIX \u2014 FILE-LEVEL IMPLEMENTATION DETAILS ======================================================================</p> <p>This section has NO LENGTH LIMIT. It should contain everything needed to implement the PR without further codebase exploration.</p> <p>For each file to be created or modified, provide:</p> <p>File: <code>exact/path/to/file.go</code></p> <p>Purpose: [1-2 sentences]</p> <p>Complete Implementation:</p> <pre><code>// Package documentation\n\npackage name\n\nimport (\n    // all imports\n)\n\n// Complete type definitions with doc comments\n// Complete function implementations\n// Complete test code\n// Include all struct fields, all methods, all parameters\n\n// Behavioral notes:\n// - [Any subtlety, e.g., \"horizon boundary: requests at exactly\n//    horizon time are NOT completed\"]\n// - [Citation: existing behavior to preserve, with file:line]\n</code></pre> <p>Key Implementation Notes: - RNG usage: [Which subsystem from PartitionedRNG? e.g., \"SubsystemRouter\"] - Metrics: [What metrics are collected? Where aggregated?] - Event ordering: [Priority? Timestamp? Secondary tie-breaking?] - State mutation: [What gets modified? Who owns it?] - Error handling: [Panic, return error, log-and-continue?]</p> <p>Include this level of detail for EVERY file touched by this PR.</p> <p>====================================================================== EXECUTION HANDOFF ======================================================================</p> <p>After creating the plan, the workflow continues with:</p> <p>Option 1: Subagent-Driven Development (in current session) - Invoke superpowers:subagent-driven-development - Fresh subagent per task - Code review between tasks - Fast iteration</p> <p>Option 2: Worktree with executing-plans (recommended for complex PRs) - Create isolated worktree (superpowers:using-git-worktrees) - Continue in same session (.worktrees/) or open new session (sibling directory) - Invoke superpowers:executing-plans with this plan - Continuous execution (stops only on failure) - Invoke commit-commands:commit-push-pr when complete</p> <p>====================================================================== QUALITY BAR ======================================================================</p> <p>This plan must: - Survive expert review (behavioral contracts are sound) - Survive systems-level scrutiny (architecture is correct) - Eliminate dead code (all code exercisable immediately) - Reduce implementation bugs (TDD, explicit verifications) - Stay strictly within source document scope (deviations justified) - Pass golangci-lint with zero new issues - Enable automated execution (complete code, exact commands) - Map every contract to a task (traceability)</p> <p>====================================================================== LINTING REQUIREMENTS ======================================================================</p> <p>This project uses golangci-lint for static analysis. Version is pinned in CI (see .github/workflows/ci.yml).</p> <p>Local verification (run before submitting PR): <pre><code>golangci-lint run ./...\n</code></pre></p> <p>Rules: 1. All NEW code must pass lint with zero issues. 2. Do not fix pre-existing lint issues in unrelated code (scope creep). 3. If a lint rule seems wrong, document why and discuss before disabling.</p> <p>======================================================================</p> <p>Think carefully. Inspect deeply. Design defensively. Break into executable tasks. Verify every step. Direct the reviewer's attention wisely.</p>"},{"location":"contributing/templates/micro-plan/","title":"Micro Plan Template (Single-PR Implementation Plan)","text":"<p>This template defines the output format for a single-PR implementation plan. Use this when planning any PR \u2014 from bug fixes to new features.</p> <p>For Claude Code users</p> <p>The <code>writing-plans</code> skill generates plans from this template automatically. The agent prompt version is at <code>micro-plan-prompt.md</code>.</p> <p>The source of work may be a macro plan section, one or more GitHub issues, a design document, or a feature request.</p> <p>The plan has two audiences:</p> <ol> <li>A human reviewer who validates behavioral correctness (Part 1)</li> <li>An implementer (human or agent) who executes the tasks (Part 2)</li> </ol>"},{"location":"contributing/templates/micro-plan/#document-header","title":"Document Header","text":"<p>Every plan starts with this header:</p> <ul> <li>Goal: One sentence a non-contributor could understand \u2014 what capability does this PR add?</li> <li>The problem today: 2\u20133 sentences explaining what's missing or broken without this PR.</li> <li>What this PR adds: Numbered list of 2\u20134 concrete capabilities, each in plain language with a brief example.</li> <li>Why this matters: 1\u20132 sentences connecting this PR to the broader project vision.</li> <li>Architecture: 2\u20133 sentences about the technical approach (packages, key types, integration points).</li> <li>Source: Link to the source of work (macro plan section, issue numbers, design doc).</li> <li>Closes: GitHub issue numbers this PR will close on merge (e.g., <code>Fixes #183, fixes #189</code>).</li> <li>Behavioral Contracts: Reference to Part 1, Section B.</li> </ul>"},{"location":"contributing/templates/micro-plan/#phase-0-component-context-before-writing-the-plan","title":"Phase 0: Component Context (before writing the plan)","text":"<p>Before writing the plan, identify this PR's place in the system:</p> <ol> <li>Which building block is being added or modified?</li> <li>What are the adjacent blocks it interacts with?</li> <li>What invariants does this PR touch?</li> <li>Construction Site Audit: For every struct this PR adds fields to, grep for ALL places that struct is constructed. List each site. If there are multiple construction sites, the plan must either add a canonical constructor or update every site explicitly.</li> </ol>"},{"location":"contributing/templates/micro-plan/#part-1-design-validation-target-120-lines","title":"Part 1: Design Validation (target &lt;120 lines)","text":""},{"location":"contributing/templates/micro-plan/#a-executive-summary","title":"A) Executive Summary","text":"<p>5\u201310 lines describing what this PR builds (plain language), where it fits in the system, adjacent components it interacts with, and any deviation flags.</p>"},{"location":"contributing/templates/micro-plan/#b-behavioral-contracts","title":"B) Behavioral Contracts","text":"<p>3\u201315 named contracts defining what this PR guarantees:</p> <pre><code>BC-N: &lt;Name&gt;\n- GIVEN &lt;precondition&gt;\n- WHEN &lt;action&gt;\n- THEN &lt;observable outcome&gt;\n- MECHANISM: &lt;one sentence explaining how&gt; (optional)\n</code></pre> <p>Group into: positive contracts (what MUST happen), negative contracts (what MUST NOT happen), error handling contracts.</p> <p>Quality gate: Every THEN clause must describe observable behavior, not internal structure. If a THEN clause contains a concrete type name or internal field name, rewrite it. The THEN clause drives the test assertion \u2014 a structural THEN produces a structural test.</p>"},{"location":"contributing/templates/micro-plan/#c-component-interaction","title":"C) Component Interaction","text":"<p>Text-based component diagram showing this PR's building block, adjacent components, data flow direction, and what crosses each boundary. Include API contracts and state ownership. Target: under 40 lines.</p>"},{"location":"contributing/templates/micro-plan/#d-deviation-log","title":"D) Deviation Log","text":"<p>Table comparing the micro plan against the source document:</p> Source Says Micro Plan Does Reason ... ... SIMPLIFICATION / CORRECTION / DEFERRAL / ADDITION / SCOPE_CHANGE"},{"location":"contributing/templates/micro-plan/#e-review-guide","title":"E) Review Guide","text":"<p>5\u201310 lines telling the reviewer: the tricky part, what to scrutinize, what's safe to skim, and known debt.</p>"},{"location":"contributing/templates/micro-plan/#part-2-executable-implementation","title":"Part 2: Executable Implementation","text":""},{"location":"contributing/templates/micro-plan/#f-implementation-overview","title":"F) Implementation Overview","text":"<p>Files to create/modify (one-line each), key decisions, confirmation that no dead code exists.</p>"},{"location":"contributing/templates/micro-plan/#g-task-breakdown-612-tasks","title":"G) Task Breakdown (6\u201312 tasks)","text":"<p>Each task follows TDD format:</p> <ol> <li>Write failing test</li> <li>Run test to verify it fails</li> <li>Implement minimal code to pass</li> <li>Run test to verify it passes</li> <li>Run lint check</li> <li>Commit with contract reference</li> </ol> <p>Each task must specify: contracts implemented (BC-X, BC-Y), files (create/modify/test), complete code in every step, exact commands with expected output.</p> <p>Task design rules: Each task implements 1\u20133 related contracts. Complete code in every step (no \"add validation\" without showing exact code). Exact commands with expected output. Reference shared test infrastructure. Golden dataset updates if needed. Dependency ordering. No dead code. Behavioral assertions only (see standards/principles.md for prohibited/required assertion patterns).</p>"},{"location":"contributing/templates/micro-plan/#h-test-strategy","title":"H) Test Strategy","text":"<p>Map contracts to tasks and tests. Include invariant tests alongside golden tests \u2014 golden tests answer \"did the output change?\" while invariant tests answer \"is the output correct?\"</p> Contract Task Test Type Test Name BC-1 Task 1 Unit TestFoo_GivenX_ThenY ... ... ... ... <p>Key invariants for this simulator (see standards/invariants.md):</p> <ul> <li>Request conservation (INV-1): completed + still_queued + still_running + dropped_unservable = injected</li> <li>KV block conservation (INV-4): allocated_blocks + free_blocks = total_blocks</li> <li>Clock monotonicity (INV-3): simulation clock never decreases</li> <li>Causality (INV-5): arrival_time \u2264 enqueue_time \u2264 schedule_time \u2264 completion_time</li> <li>Determinism (INV-6): same seed produces byte-identical output across runs</li> </ul>"},{"location":"contributing/templates/micro-plan/#i-risk-analysis","title":"I) Risk Analysis","text":"<p>For each risk: description, likelihood (low/medium/high), impact (low/medium/high), mitigation (specific test or design choice), and which task mitigates it.</p>"},{"location":"contributing/templates/micro-plan/#part-3-quality-assurance","title":"Part 3: Quality Assurance","text":""},{"location":"contributing/templates/micro-plan/#j-sanity-checklist","title":"J) Sanity Checklist","text":"<p>Before implementation, verify:</p> <p>Plan-specific checks: - [ ] No unnecessary abstractions. - [ ] No feature creep beyond PR scope. - [ ] No unexercised flags or interfaces. - [ ] No partial implementations. - [ ] No breaking changes without explicit contract updates. - [ ] No hidden global state impact. - [ ] All new code will pass golangci-lint. - [ ] Shared test helpers used from existing shared test package (not duplicated locally). - [ ] CLAUDE.md updated if: new files/packages added, file organization changed, plan milestone completed, new CLI flags added. - [ ] No stale references left in CLAUDE.md. - [ ] Documentation DRY: If this PR modifies a canonical source (docs/contributing/standards/rules.md, docs/contributing/standards/invariants.md, docs/contributing/standards/principles.md, docs/contributing/extension-recipes.md), all working copies in the source-of-truth map are updated. If a new file is added, it appears in the CLAUDE.md File Organization tree. - [ ] Deviation log reviewed \u2014 no unresolved deviations. - [ ] Each task produces working, testable code (no scaffolding). - [ ] Task dependencies are correctly ordered. - [ ] All contracts are mapped to specific tasks. - [ ] Golden dataset regeneration documented (if needed). - [ ] Construction site audit completed \u2014 all struct construction sites listed and covered by tasks. - [ ] If this PR is part of a macro plan, the macro plan status is updated.</p> <p>Antipattern rules (full details in standards/rules.md): - [ ] R1: No silent <code>continue</code>/<code>return</code> dropping data - [ ] R2: Map keys sorted before float accumulation or ordered output - [ ] R3: Every new CLI flag validated (zero, negative, NaN, Inf) - [ ] R4: All struct construction sites audited for new fields - [ ] R5: Resource allocation loops handle mid-loop failure with rollback - [ ] R6: No <code>logrus.Fatalf</code> or <code>os.Exit</code> in <code>sim/</code> packages - [ ] R7: Invariant tests alongside any golden tests - [ ] R8: No exported mutable maps - [ ] R9: <code>*float64</code> for YAML fields where zero is valid - [ ] R10: YAML strict parsing (<code>KnownFields(true)</code>) - [ ] R11: Division by runtime-derived denominators guarded - [ ] R12: Golden dataset regenerated if output changed - [ ] R13: New interfaces work for 2+ implementations - [ ] R14: No method spans multiple module responsibilities - [ ] R15: Stale PR references resolved - [ ] R16: Config params grouped by module - [ ] R17: Routing scorer signals documented for freshness tier - [ ] R18: CLI flag values not silently overwritten by defaults.yaml - [ ] R19: Unbounded retry/requeue loops have circuit breakers - [ ] R20: Detectors and analyzers handle degenerate inputs (empty, skewed, zero)</p>"},{"location":"contributing/templates/micro-plan/#appendix-file-level-implementation-details","title":"Appendix: File-Level Implementation Details","text":"<p>This section has NO LENGTH LIMIT. It should contain everything needed to implement the PR without further codebase exploration.</p> <p>For each file to be created or modified, provide:</p> <p>File: <code>exact/path/to/file.go</code></p> <ul> <li>Purpose: 1\u20132 sentences</li> <li>Complete implementation: All type definitions, function implementations, test code</li> <li>Key implementation notes:<ul> <li>Event ordering: Priority? Timestamp? Secondary tie-breaking?</li> <li>RNG usage: Which subsystem from PartitionedRNG?</li> <li>Metrics: What metrics are collected? Where aggregated?</li> <li>State mutation: What gets modified? Who owns it?</li> <li>Error handling: Panic, return error, or log-and-continue?</li> </ul> </li> </ul>"},{"location":"getting-started/","title":"What is BLIS?","text":"<p>BLIS (Blackbox Inference Simulator) is a discrete-event simulator for LLM inference serving systems. It models multi-instance clusters with configurable admission control, request routing, KV-cache dynamics, scheduling policies, and token generation \u2014 all without requiring real GPUs.</p>"},{"location":"getting-started/#why-simulate-inference-serving","title":"Why Simulate Inference Serving?","text":"<p>Deploying LLM inference at scale requires answering capacity planning questions that are expensive to answer with real hardware:</p> <ul> <li>How many instances do I need to serve 1,000 requests/second at p99 TTFT &lt; 200ms?</li> <li>Which routing policy minimizes tail latency for my workload mix?</li> <li>How much KV cache memory do I need before preemptions degrade throughput?</li> <li>What happens at 2x traffic \u2014 does latency degrade gracefully or catastrophically?</li> </ul> <p>Running these experiments on real GPUs costs thousands of dollars and takes days. BLIS answers them in seconds on a laptop.</p>"},{"location":"getting-started/#who-should-use-blis","title":"Who Should Use BLIS","text":"Audience Use Case Capacity planners Determine instance counts, GPU memory, and TP configurations before procurement Platform engineers Compare routing policies, tune scorer weights, evaluate admission control strategies Researchers Run controlled experiments on scheduling, batching, and caching algorithms Developers Validate new policies against existing ones before deploying to production"},{"location":"getting-started/#what-blis-is-not","title":"What BLIS Is Not","text":"<p>Setting expectations</p> <ul> <li>Not a benchmark \u2014 BLIS simulates serving behavior, it does not generate real GPU load</li> <li>Not primarily a load generator \u2014 BLIS focuses on simulation. Real-mode traffic generation against OpenAI-compatible endpoints is available but experimental. For production load testing, use tools like <code>inference-perf</code> or <code>genai-perf</code></li> </ul>"},{"location":"getting-started/#key-features","title":"Key Features","text":"<p>See the Home page feature list for the full capabilities catalog, including the workload specification DSL, metrics pipeline, latency model backends, and policy framework.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li> Installation \u2014 Build BLIS from source</li> <li> Quick Start \u2014 Run your first simulation in 30 seconds</li> <li> Tutorial: Capacity Planning \u2014 End-to-end walkthrough</li> <li> User Guide \u2014 Task-oriented how-to guides</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Go 1.21+ \u2014 Download Go</li> <li>Git \u2014 for cloning the repository</li> </ul>"},{"location":"getting-started/installation/#build-from-source","title":"Build from Source","text":"<pre><code>git clone https://github.com/inference-sim/inference-sim.git\ncd inference-sim\ngo build -o blis main.go\n</code></pre>"},{"location":"getting-started/installation/#verify-the-build","title":"Verify the Build","text":"<pre><code>./blis run --model meta-llama/llama-3.1-8b-instruct --num-requests 10\n</code></pre> <p>You should see JSON output on stdout containing fields like <code>ttft_mean_ms</code>, <code>e2e_mean_ms</code>, and <code>responses_per_sec</code>. This confirms BLIS is working correctly.</p>"},{"location":"getting-started/installation/#optional-local-documentation","title":"Optional: Local Documentation","text":"<p>To preview the documentation site locally:</p> <pre><code>pip install mkdocs-material==9.7.3\nmkdocs serve\n</code></pre> <p>Then open http://localhost:8000.</p>"},{"location":"getting-started/installation/#optional-linter","title":"Optional: Linter","text":"<p>For contributors, install the linter used in CI:</p> <pre><code>go install github.com/golangci/golangci-lint/v2/cmd/golangci-lint@v2.9.0\ngolangci-lint run ./...\n</code></pre>"},{"location":"getting-started/installation/#whats-next","title":"What's Next","text":"<ul> <li>Quick Start \u2014 Run your first simulation and understand the output</li> <li>Tutorial: Capacity Planning \u2014 Complete walkthrough of a capacity planning exercise</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Run your first BLIS simulation in 30 seconds.</p>"},{"location":"getting-started/quickstart/#single-instance-simulation","title":"Single-Instance Simulation","text":"<pre><code>./blis run --model meta-llama/llama-3.1-8b-instruct\n</code></pre> <p>This runs 100 requests through a single inference instance using pre-trained coefficients for LLaMA 3.1 8B on an H100 GPU with TP=2.</p>"},{"location":"getting-started/quickstart/#reading-the-output","title":"Reading the Output","text":"<p>The JSON output on stdout contains:</p> <pre><code>{\n  \"ttft_mean_ms\": 12.34,\n  \"ttft_p99_ms\": 45.67,\n  \"e2e_mean_ms\": 1234.56,\n  \"e2e_p99_ms\": 2345.67,\n  \"itl_mean_ms\": 8.91,\n  \"responses_per_sec\": 5.67,\n  \"completed_requests\": 100\n}\n</code></pre> Metric What It Measures TTFT (Time To First Token) Latency from request arrival to first output token \u2014 measures responsiveness E2E (End-to-End) Total latency from arrival to final token \u2014 measures total request duration ITL (Inter-Token Latency) Time between consecutive output tokens \u2014 measures streaming smoothness responses_per_sec Throughput \u2014 requests completed per second"},{"location":"getting-started/quickstart/#cluster-mode","title":"Cluster Mode","text":"<p>Scale to 4 instances with routing:</p> <pre><code>./blis run \\\n  --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 4 \\\n  --routing-policy weighted \\\n  --rate 100 --num-requests 500\n</code></pre> <p>This simulates a 4-instance cluster receiving 100 requests/second. The <code>weighted</code> routing policy uses the default scorer profile (<code>prefix-affinity:3, queue-depth:2, kv-utilization:2</code>) to distribute requests across instances.</p>"},{"location":"getting-started/quickstart/#try-different-configurations","title":"Try Different Configurations","text":"<pre><code># Higher traffic rate\n./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 4 --rate 500 --num-requests 2000\n\n# With decision tracing (see where each request was routed)\n./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 4 --rate 100 --num-requests 500 \\\n  --trace-level decisions --summarize-trace\n\n# With roofline mode (no pre-trained coefficients needed)\n./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --roofline --hardware H100 --tp 2 \\\n  --num-instances 4 --rate 100 --num-requests 500\n</code></pre>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next","text":"<ul> <li>Tutorial: Capacity Planning \u2014 Full walkthrough: find the right instance count for your workload</li> <li>Routing Policies \u2014 Understand and compare routing strategies</li> <li>Configuration Reference \u2014 Complete CLI flag reference</li> </ul>"},{"location":"getting-started/tutorial/","title":"Tutorial: Capacity Planning","text":"<p>This tutorial walks through a complete capacity planning exercise: determining how many inference instances you need to serve a target request rate while meeting latency SLOs.</p> <p>Scenario: You're deploying LLaMA 3.1 8B on H100 GPUs with TP=2. Your SLO is TTFT p99 &lt; 500ms. You need to find the minimum number of instances for 500 requests/second.</p>"},{"location":"getting-started/tutorial/#step-1-estimate-instance-capacity","title":"Step 1: Estimate Instance Capacity","text":"<p>Before scaling up, measure the throughput of a single instance empirically. This works regardless of which latency model you use (blackbox or roofline).</p> <p>Run a single instance at low load to establish the baseline service rate:</p> <pre><code>./blis run \\\n  --model meta-llama/llama-3.1-8b-instruct \\\n  --rate 10 --num-requests 50\n</code></pre> <p>Check the <code>responses_per_sec</code> value in the output \u2014 this is the single-instance throughput at low utilization. For LLaMA 3.1 8B / H100 / TP=2 with default workload (512 input / 512 output tokens), you'll see roughly 57 requests/second.</p> <p>This means for 500 req/s, you need at minimum <code>ceil(500/57) \u2248 9</code> instances. Let's verify with simulation.</p>"},{"location":"getting-started/tutorial/#step-2-baseline-single-instance","title":"Step 2: Baseline \u2014 Single Instance","text":"<pre><code>./blis run \\\n  --model meta-llama/llama-3.1-8b-instruct \\\n  --rate 50 --num-requests 200\n</code></pre> <p>At 50 req/s (well below the ~57 req/s capacity), TTFT should be low. Note the <code>ttft_p99_ms</code> value \u2014 this is your best-case baseline.</p>"},{"location":"getting-started/tutorial/#step-3-scale-up-and-find-the-saturation-point","title":"Step 3: Scale Up and Find the Saturation Point","text":"<p>Run simulations at increasing instance counts for 500 req/s:</p> <pre><code># 4 instances (~228 req/s capacity \u2192 heavily overloaded)\n./blis run \\\n  --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 4 --rate 500 --num-requests 2000\n\n# 8 instances (~456 req/s capacity \u2192 near saturation)\n./blis run \\\n  --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 8 --rate 500 --num-requests 2000\n\n# 12 instances (~684 req/s capacity \u2192 comfortable headroom)\n./blis run \\\n  --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 12 --rate 500 --num-requests 2000\n</code></pre> <p>Compare <code>ttft_p99_ms</code> across runs. You'll see:</p> <ul> <li>4 instances: TTFT p99 extremely high (queue growing without bound)</li> <li>8 instances: TTFT p99 elevated (close to saturation, <code>excess = \u03bb/k - \u03bc</code> is small but positive)</li> <li>12 instances: TTFT p99 near baseline (sufficient capacity)</li> </ul> <p>Understanding DES saturation</p> <p>In a discrete-event simulator, saturation manifests as unbounded queue growth: when the arrival rate exceeds the per-instance service rate, the WaitQ grows at <code>excess = \u03bb/k - \u03bc</code> requests per second, where \u03bb is the total arrival rate, k is the instance count, and \u03bc is the per-instance service rate. Unlike real systems with CPU load metrics, the DES signal for saturation is queue depth growth rate.</p>"},{"location":"getting-started/tutorial/#step-4-identify-the-bottleneck-type","title":"Step 4: Identify the Bottleneck Type","text":"<p>When TTFT is high, there are three possible causes:</p> <ol> <li>Queue saturation \u2014 arrival rate exceeds service capacity \u2192 add instances</li> <li>Memory saturation \u2014 KV cache preemptions degrade throughput \u2192 add KV blocks or reduce batch size</li> <li>Compute saturation \u2014 step time dominates \u2192 reduce batch size or use chunked prefill</li> </ol> <p>Check the output for clues:</p> <ul> <li>High <code>preemption_count</code> \u2192 memory saturation</li> <li>High <code>scheduling_delay_p99_ms</code> \u2192 queue saturation (requests waiting in the WaitQ)</li> <li>Low <code>preemption_count</code> + low <code>scheduling_delay</code> + high TTFT \u2192 compute saturation</li> </ul>"},{"location":"getting-started/tutorial/#step-5-compare-routing-policies","title":"Step 5: Compare Routing Policies","text":"<p>With 12 instances at 500 req/s, compare routing strategies:</p> <pre><code># Round-robin (baseline)\n./blis run \\\n  --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 12 --rate 500 --num-requests 2000 \\\n  --routing-policy round-robin\n\n# Weighted (default profile)\n./blis run \\\n  --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 12 --rate 500 --num-requests 2000 \\\n  --routing-policy weighted\n\n# Least-loaded\n./blis run \\\n  --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 12 --rate 500 --num-requests 2000 \\\n  --routing-policy least-loaded\n</code></pre> <p>For prefix-heavy workloads (like RAG with shared system prompts), try the prefix-affinity-dominant profile:</p> <pre><code>./blis run \\\n  --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 12 --rate 500 --num-requests 2000 \\\n  --routing-policy weighted \\\n  --routing-scorers \"prefix-affinity:5,queue-depth:1\" \\\n  --prefix-tokens 512\n</code></pre>"},{"location":"getting-started/tutorial/#step-6-evaluate-with-fitness-scores","title":"Step 6: Evaluate with Fitness Scores","text":"<p>For automated comparison across many configurations, use fitness evaluation:</p> <pre><code>./blis run \\\n  --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 12 --rate 500 --num-requests 2000 \\\n  --routing-policy weighted \\\n  --fitness-weights \"p99_ttft:3,mean_e2e:1,throughput:2\"\n</code></pre> <p>Fitness score normalization</p> <p>Fitness scores use <code>1/(1+x/1000)</code> normalization for latency metrics, which compresses large raw differences into small score differences. A 38% TTFT improvement may appear as only an 8% fitness score difference. Always examine raw metrics alongside fitness scores.</p>"},{"location":"getting-started/tutorial/#step-7-validate-against-your-slo","title":"Step 7: Validate Against Your SLO","text":"<p>Your SLO: TTFT p99 &lt; 500ms at 500 req/s.</p> <p>From the simulations above, find the minimum instance count where <code>ttft_p99_ms &lt; 500</code>. That's your capacity plan. Add 20-30% headroom for traffic spikes (real deployments see bursty traffic that exceeds the Poisson assumption).</p>"},{"location":"getting-started/tutorial/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Compute capacity first \u2014 estimate <code>1/step_time</code> per instance from beta coefficients</li> <li>Saturation is non-linear \u2014 TTFT degrades super-linearly as you approach capacity. Scaling from 4\u21928 instances produces a 7x improvement, not 2x (queue growth rate drops faster than linear)</li> <li>Check the bottleneck type \u2014 preemption count, scheduling delay, and raw TTFT tell you whether to add instances, add memory, or tune batch size</li> <li>Routing matters at scale \u2014 routing policy choice can change TTFT p99 by 3x at high request rates</li> <li>Deterministic replay \u2014 use <code>--seed</code> to get identical results for A/B comparisons</li> </ol>"},{"location":"getting-started/tutorial/#whats-next","title":"What's Next","text":"<ul> <li>Routing Policies \u2014 deep dive into scorer composition and signal freshness</li> <li>KV Cache &amp; Memory \u2014 tune KV blocks, prefix caching, and chunked prefill</li> <li>Metrics &amp; Results \u2014 understand all output fields and common patterns</li> <li>Hypothesis Experimentation \u2014 run rigorous experiments with the <code>/hypothesis-experiment</code> skill</li> </ul>"},{"location":"guide/","title":"User Guide","text":"<p>Task-oriented guides for using BLIS effectively. Each guide covers a specific feature with practical CLI examples and expected output.</p>"},{"location":"guide/#guides","title":"Guides","text":"Guide When to Use Routing Policies Choosing and configuring how requests are distributed across instances Admission Control Rate-limiting and traffic shaping at the cluster gateway Scheduling &amp; Priority Controlling request processing order within each instance Latency Models Choosing between blackbox (data-driven) and roofline (analytical) step time estimation KV Cache &amp; Memory Tuning GPU/CPU memory allocation, prefix caching, and chunked prefill Workload Specifications Defining multi-client traffic patterns with YAML Cluster Simulation Running multi-instance simulations with the full pipeline Metrics &amp; Results Understanding JSON output, metrics, anomaly counters, and fitness scores Hypothesis Experimentation Running rigorous, reproducible experiments with the <code>/hypothesis-experiment</code> skill Skills &amp; Plugins Claude Code skills, plugin marketplaces, and workflow tooling"},{"location":"guide/#reading-paths","title":"Reading Paths","text":"<p>Capacity planning: Quick Start \u2192 Tutorial \u2192 Cluster Simulation \u2192 Metrics &amp; Results</p> <p>Routing optimization: Routing Policies \u2192 Cluster Simulation \u2192 Metrics &amp; Results</p> <p>Memory tuning: KV Cache &amp; Memory \u2192 Metrics &amp; Results</p> <p>New model evaluation: Latency Models \u2192 Workload Specifications \u2192 Metrics &amp; Results</p> <p>Research: Hypothesis Experimentation \u2192 Metrics &amp; Results</p>"},{"location":"guide/admission/","title":"Admission Control","text":"<p>Admission control is the first gate in the cluster pipeline. It decides whether to accept or reject incoming requests before they reach the routing stage. Admission only applies in cluster mode (<code>--num-instances</code> &gt; 1) -- single-instance simulations skip directly to the wait queue.</p> <pre><code># Rate-limit a 4-instance cluster with token bucket admission\n./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 4 --rate 500 --num-requests 2000 \\\n  --admission-policy token-bucket \\\n  --token-bucket-capacity 10000 --token-bucket-refill-rate 1000\n</code></pre>"},{"location":"guide/admission/#available-policies","title":"Available Policies","text":"Policy Flag Value Behavior Always-admit <code>--admission-policy always-admit</code> (default) Accepts all requests unconditionally. No filtering. Token-bucket <code>--admission-policy token-bucket</code> Rate-limiting. Each request consumes tokens equal to its input token count. Tokens refill at a constant rate. Rejects when the bucket is empty. Reject-all <code>--admission-policy reject-all</code> Rejects all requests unconditionally. Pathological template for testing."},{"location":"guide/admission/#token-bucket-mechanics","title":"Token Bucket Mechanics","text":"<p>The token bucket policy controls throughput by treating each request's input token count as a cost:</p> Flag Description Default <code>--token-bucket-capacity</code> Maximum number of tokens the bucket can hold 10000 <code>--token-bucket-refill-rate</code> Tokens added per second of simulation time 1000 <p>How it works:</p> <ol> <li>Bucket starts full. At initialization, the bucket holds <code>capacity</code> tokens.</li> <li>Refill is continuous. On each admission decision, the bucket refills proportionally to elapsed simulation time: <code>refill = elapsed_microseconds * refill_rate / 1,000,000</code>.</li> <li>Cost per request = number of input tokens. A request with 512 input tokens costs 512 tokens from the bucket.</li> <li>Admission check. If <code>current_tokens &gt;= cost</code>, the request is admitted and the cost is subtracted. Otherwise the request is rejected with reason <code>\"insufficient tokens\"</code>.</li> <li>Capacity cap. Tokens never accumulate beyond <code>capacity</code>, even after long idle periods.</li> </ol> <p>Sizing the bucket</p> <p>With <code>--token-bucket-capacity 10000 --token-bucket-refill-rate 1000</code> and requests averaging 512 input tokens, the sustained admission rate is roughly <code>1000 / 512 ~ 1.95 req/s</code>. The bucket's capacity of 10000 tokens allows a burst of up to <code>10000 / 512 ~ 19</code> requests before rate-limiting kicks in.</p> <p>Rejected requests are counted in the output anomaly counters (<code>Rejected Requests</code>) and in the full pipeline conservation formula (<code>num_requests == injected_requests + rejected_requests</code>), but they never enter the routing stage or any instance queue.</p>"},{"location":"guide/admission/#when-to-use-admission-control","title":"When to Use Admission Control","text":"<ul> <li>Overload protection. When the arrival rate significantly exceeds service capacity, unbounded queues grow without limit. Admission shedding keeps queue depth manageable.</li> <li>Cost control. Limit total token throughput to match a token budget or downstream rate limit.</li> <li>Graceful degradation. Shed excess load to protect latency for admitted requests. Under extreme overload, routing distributes load and scheduling orders within instances, but neither can reduce total queue depth \u2014 admission is the lever that can. Note: the current admission policies are SLO-class-blind \u2014 all requests pay the same token cost regardless of <code>slo_class</code>. SLO-aware shedding (rejecting <code>batch</code> before <code>critical</code>) is not yet implemented.</li> <li>Testing rejection paths. The <code>reject-all</code> policy verifies that rejection counting, trace recording, and conservation invariants hold when no requests are admitted.</li> </ul> <p>Admission is the third lever</p> <p>Routing distributes load across instances. Scheduling orders requests within each instance. But when total arrival rate exceeds total service capacity, neither routing nor scheduling can reduce the queue -- they can only redistribute it. Admission control is the mechanism that actually reduces inbound volume.</p>"},{"location":"guide/admission/#pipeline-latency","title":"Pipeline Latency","text":"<p>The <code>--admission-latency</code> and <code>--routing-latency</code> flags model real network and processing overhead between gateway and backend (gRPC hops, service mesh serialization, queue dispatch). These are pipeline concerns that affect both admission and routing stages. See Cluster Simulation for details on configuring pipeline latency.</p>"},{"location":"guide/admission/#further-reading","title":"Further Reading","text":"<ul> <li>Cluster Simulation -- full pipeline overview</li> <li>Routing Policies -- the next stage after admission</li> <li>Cluster Architecture -- architectural details</li> </ul>"},{"location":"guide/cluster/","title":"Cluster Simulation","text":"<p>This guide covers running multi-instance BLIS simulations \u2014 the full pipeline from request arrival through admission, routing, scheduling, and metrics aggregation.</p> <pre><code># Quick example: 4-instance cluster with tracing\n./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 4 --rate 100 --num-requests 500 \\\n  --trace-level decisions --summarize-trace\n</code></pre>"},{"location":"guide/cluster/#single-instance-vs-cluster-mode","title":"Single-Instance vs Cluster Mode","text":"Setting Behavior <code>--num-instances 1</code> (default) Single-instance: requests go directly to the wait queue, no admission or routing <code>--num-instances N</code> (N &gt; 1) Cluster mode: requests pass through admission \u2192 routing \u2192 per-instance queues"},{"location":"guide/cluster/#the-pipeline","title":"The Pipeline","text":"<pre><code>Request \u2192 Admission \u2192 Routing \u2192 Instance WaitQueue \u2192 Batch Formation \u2192 Step \u2192 Completion\n                                                          \u2193\n                                                    KV Allocation + Latency Estimation\n</code></pre> <p>Each stage is configurable:</p> Stage Controls Key Flags Admission Whether to accept the request <code>--admission-policy</code>, <code>--token-bucket-capacity</code> Routing Which instance receives it <code>--routing-policy</code>, <code>--routing-scorers</code> Scheduling What order within the instance <code>--scheduler</code>, <code>--priority-policy</code> Batch Formation Which requests form the next batch <code>--max-num-running-reqs</code>, <code>--max-num-scheduled-tokens</code>"},{"location":"guide/cluster/#tensor-parallelism","title":"Tensor Parallelism","text":"<p>The <code>--tp</code> flag sets the tensor parallelism degree for all instances. TP affects both latency (FLOPs split across GPUs) and memory (KV blocks split across GPUs):</p> <pre><code># TP=2: 2 GPUs per instance\n./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 4 --tp 2 --rate 100 --num-requests 500\n\n# TP=4: 4 GPUs per instance (lower latency, fewer KV blocks per GPU)\n./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 2 --tp 4 --rate 100 --num-requests 500\n</code></pre> <p>Homogeneous instances</p> <p>All instances share the same SimConfig (model, GPU, TP, KV blocks). BLIS does not currently model heterogeneous fleets (mixed GPU types or TP configurations).</p>"},{"location":"guide/cluster/#scaling-and-saturation","title":"Scaling and Saturation","text":"<p>Instance scaling produces super-linear TTFT improvement near saturation. Doubling from 4\u21928 instances at near-capacity (rate=500) improves TTFT p99 by 7.4x, not 2x.</p> <p>This happens because the per-instance queue growth rate <code>excess = \u03bb/k - \u03bc</code> drops faster than linearly:</p> <pre><code>4 instances: excess = 500/4 - 57.4 = 67.6 req/s per instance \u2192 rapid queue growth\n8 instances: excess = 500/8 - 57.4 = 5.1 req/s per instance  \u2192 minimal queueing\n</code></pre> <p>At sub-saturation (rate=100): scaling effect vanishes (1.06x).</p>"},{"location":"guide/cluster/#admission-control","title":"Admission Control","text":"<p>For rate-limiting and traffic shaping policies, see the Admission Control page.</p>"},{"location":"guide/cluster/#admission-and-routing-latency","title":"Admission and Routing Latency","text":"<p>Model real network/processing overhead between gateway and backend:</p> <pre><code>--admission-latency 1000   # 1ms admission decision overhead\n--routing-latency 500      # 0.5ms routing decision overhead\n</code></pre> <p>These add simulated delays to the admission and routing pipeline, modeling gRPC overhead, service mesh hops, and queue serialization in production deployments.</p>"},{"location":"guide/cluster/#decision-tracing","title":"Decision Tracing","text":"<p>Log every routing decision for offline analysis:</p> <pre><code>./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 4 --rate 100 --num-requests 500 \\\n  --trace-level decisions --summarize-trace --counterfactual-k 3\n</code></pre> <p>The trace summary shows: - Target Distribution \u2014 how many requests went to each instance - Mean/Max Regret \u2014 how much better an alternative routing decision could have been</p> <p>Counterfactual regret for weighted policies</p> <p>For score-based policies (weighted, least-loaded), counterfactual regret is structurally zero \u2014 the chosen instance is always the highest-scoring one. Regret is only meaningful for non-score-based policies like round-robin.</p>"},{"location":"guide/cluster/#event-ordering","title":"Event Ordering","text":"<p>The cluster uses <code>(timestamp, priority, seqID)</code> ordering for deterministic event processing:</p> <ul> <li>Cluster events at time T process before instance events at time T</li> <li>Same-time instance ties broken by lowest instance index</li> <li>This ensures determinism (INV-6) but means results differ from a simple M/M/k queueing model</li> </ul>"},{"location":"guide/cluster/#work-conserving-property","title":"Work-Conserving Property","text":"<p>BLIS is work-conserving (INV-8): it never idles while requests wait. After every step completion, if the WaitQ has requests, a new StepEvent is immediately scheduled. Real systems may have scheduling delays not modeled here.</p>"},{"location":"guide/cluster/#further-reading","title":"Further Reading","text":"<ul> <li>Cluster Architecture \u2014 internal mechanics of the shared-clock event loop</li> <li>Routing Policies \u2014 scorer composition and signal freshness</li> <li>Metrics &amp; Results \u2014 understanding trace summaries and per-SLO metrics</li> </ul>"},{"location":"guide/experimentation/","title":"Hypothesis-Driven Experimentation","text":"<p>This guide covers using BLIS as a platform for rigorous, reproducible experiments. Because BLIS is a deterministic discrete-event simulator (same seed \u2192 byte-identical output), you can run controlled experiments that are impossible with real hardware.</p> <pre><code># Quick example: compare chunked prefill vs no chunked prefill\n./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --rate 100 --num-requests 500 --long-prefill-token-threshold 0\n\n./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --rate 100 --num-requests 500 --long-prefill-token-threshold 256\n</code></pre>"},{"location":"guide/experimentation/#why-experiment-with-a-simulator","title":"Why Experiment with a Simulator?","text":"<p>Real GPU benchmarks suffer from noise: wall-clock jitter, OS scheduling, GPU thermal throttling, network variability. With BLIS:</p> <ul> <li>Deterministic replay \u2014 change exactly one variable, attribute all output differences to that change</li> <li>No hardware cost \u2014 run thousands of configurations on a laptop</li> <li>Controlled conditions \u2014 isolate the effect of a single parameter while holding everything else constant</li> <li>Reproducible \u2014 share your seed, workload spec, and CLI flags; anyone can reproduce your results</li> </ul>"},{"location":"guide/experimentation/#capacity-planning-validation","title":"Capacity Planning Validation","text":"<p>The most common experiment workflow for platform engineers:</p> <ol> <li>Define your deployment: model, GPU, TP, instance count</li> <li>Define your workload: arrival rate, token distributions (from production logs if available)</li> <li>Define your SLO: TTFT p99 &lt; 200ms, E2E p99 &lt; 5s, etc.</li> <li>Run the simulation with these parameters</li> <li>Interpret: Does the simulated TTFT p99 meet your SLO? If not, add instances or tune routing.</li> </ol> <pre><code># Example: Will 8 instances of LLaMA 3.1 8B handle 400 req/s at TTFT p99 &lt; 500ms?\n./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 8 --rate 400 --num-requests 2000 \\\n  --routing-policy weighted --seed 42\n</code></pre>"},{"location":"guide/experimentation/#the-hypothesis-experiment-skill","title":"The <code>/hypothesis-experiment</code> Skill","text":"<p>For structured hypothesis-driven research, BLIS includes a guided experimentation workflow via the <code>/hypothesis-experiment</code> Claude Code skill:</p> <pre><code>/hypothesis-experiment\n</code></pre> <p>This skill guides you through:</p> <ol> <li>Formulate \u2014 state a testable prediction (e.g., \"chunked prefill reduces short-request TTFT p99 by &gt; 30%\")</li> <li>Classify \u2014 identify the hypothesis family (scheduler invariants, performance-regime, etc.)</li> <li>Design \u2014 specify parameters, controls, success criteria</li> <li>Implement \u2014 create <code>run.sh</code> (experiment script) and <code>analyze.py</code> (analysis script)</li> <li>Run \u2014 execute the experiment</li> <li>Analyze \u2014 parse results, compute statistics</li> <li>Document \u2014 write FINDINGS.md with conclusions and evidence</li> </ol>"},{"location":"guide/experimentation/#the-experiment-harness","title":"The Experiment Harness","text":"<p>All experiments use a shared harness (<code>hypotheses/lib/</code>) for consistency:</p> <pre><code>source hypotheses/lib/harness.sh\n\n# Run a simulation with standard setup\nblis_run 60 results/baseline.json \\\n  --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 4 --rate 100 --num-requests 500\n</code></pre> <p>The harness provides: - <code>blis_run()</code> \u2014 wrapper around the simulation binary - <code>setup_experiment()</code> \u2014 create output directories - <code>preflight_kv_check()</code> \u2014 verify KV configuration - <code>hypotheses/lib/analyze_helpers.py</code> \u2014 common analysis functions (<code>parse_blis_output()</code>, etc.)</p>"},{"location":"guide/experimentation/#case-studies","title":"Case Studies","text":"<p>Completed experiments demonstrate the power of hypothesis-driven analysis:</p> Experiment Finding Impact H7 (Horizontal Scaling) TTFT p99 scales 7.4x (not 2x) when doubling instances near saturation Super-linear benefit from queue growth rate reduction H27 (Chunked Prefill) <code>--long-prefill-token-threshold=256</code> reduces short-request TTFT p99 by 52% But ITL is unaffected \u2014 chunked prefill benefits scheduling, not decode H29 (Snapshot Staleness) <code>--snapshot-refresh-interval</code> 100ms degrades TTFT p99 by +354% for kv-utilization scorer Safe zone &lt; 5ms; composite scorer mitigates ~99% H20 (Heavy-Tailed) ParetoLogNormal produces fewer preemptions than Gaussian despite similar means Distribution median, not mean, drives KV pressure <p>All findings are documented in <code>hypotheses/*/FINDINGS.md</code>.</p>"},{"location":"guide/experimentation/#convergence-review","title":"Convergence Review","text":"<p>Experiments go through a multi-perspective review process to ensure rigor:</p> <ol> <li>Design Review (5 perspectives) \u2014 validates hypothesis quality and experiment design</li> <li>Code Review (5 perspectives) \u2014 checks run.sh/analyze.py for correctness</li> <li>FINDINGS Review (10 perspectives) \u2014 validates conclusions against evidence</li> </ol> <p>The <code>/convergence-review</code> skill automates this process. Zero CRITICAL + zero IMPORTANT findings = converged.</p>"},{"location":"guide/experimentation/#further-reading","title":"Further Reading","text":"<ul> <li>Hypothesis Process \u2014 full 10-step process for contributors</li> <li>Experiment Standards \u2014 rigor requirements (ED-1 through ED-6, RCV-1 through RCV-6)</li> <li>Metrics &amp; Results \u2014 understanding the metrics your experiments produce</li> </ul>"},{"location":"guide/kv-cache/","title":"KV Cache &amp; Memory Management","text":"<p>This guide covers KV cache allocation, prefix caching, tiered GPU+CPU offload, and chunked prefill \u2014 the memory subsystem that determines how many requests can run concurrently.</p> <pre><code># Quick example: simulate with reduced KV blocks to observe preemptions\n./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --total-kv-blocks 5000 --rate 50 --num-requests 200\n</code></pre>"},{"location":"guide/kv-cache/#block-allocation-model","title":"Block Allocation Model","text":"<p>KV cache is allocated in blocks of <code>--block-size-in-tokens</code> tokens (default: 16). Each request consumes <code>ceil(token_count / block_size)</code> blocks. Blocks are reference-counted and can be shared across requests via prefix caching.</p> Flag Default Description <code>--total-kv-blocks</code> Per-model* Total GPU-tier KV blocks <code>--block-size-in-tokens</code> 16 Tokens per block <p>*The CLI default is 1,000,000 but <code>defaults.yaml</code> overrides this per model. For LLaMA 3.1 8B / H100 / TP=2: 132,139 blocks.</p> <p>Block size affects prefix cache granularity</p> <p>Prefix caching uses block-aligned hashing (<code>hash.ComputeBlockHashes</code>). Smaller block sizes increase cache hit granularity but also increase allocation overhead. Choose block size relative to your typical prefix lengths.</p>"},{"location":"guide/kv-cache/#prefix-caching","title":"Prefix Caching","text":"<p>When requests share common prefixes (e.g., system prompts in RAG), BLIS can reuse KV cache blocks from prior computations. This reduces prefill tokens and improves TTFT.</p> <p>Prefix caching is automatic when using the <code>prefix-affinity</code> scorer with <code>weighted</code> routing:</p> <pre><code>./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 4 --routing-policy weighted \\\n  --routing-scorers \"prefix-affinity:3,queue-depth:1\" \\\n  --prefix-tokens 512 --rate 100 --num-requests 500\n</code></pre>"},{"location":"guide/kv-cache/#minimum-kv-block-requirements","title":"Minimum KV Block Requirements","text":"<p>DroppedUnservable rejection</p> <p>When <code>ceil(inputTokens / blockSize) &gt; TotalCapacity()</code>, BLIS drops the request as unservable \u2014 it physically cannot fit in memory. This mirrors vLLM's pre-engine rejection path.</p> <p>Compute the minimum blocks needed for your workload:</p> <pre><code>min_blocks = ceil(max_input_tokens / block_size)\n</code></pre> <p>For a workload with max 7,000 input tokens and block size 16: <code>ceil(7000/16) = 438</code> blocks minimum. Below this, requests are dropped. Below ~2x this threshold, cascading preemptions cause severe throughput degradation.</p>"},{"location":"guide/kv-cache/#tiered-caching-gpu-cpu-offload","title":"Tiered Caching (GPU + CPU Offload)","text":"<p>BLIS models tiered KV cache with GPU\u2192CPU offloading:</p> <pre><code>./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --kv-cpu-blocks 50000 \\\n  --kv-offload-threshold 0.9 \\\n  --kv-transfer-bandwidth 100.0 \\\n  --rate 100 --num-requests 500\n</code></pre> Flag Default Description <code>--kv-cpu-blocks</code> 0 CPU-tier blocks (0 = disabled) <code>--kv-offload-threshold</code> 0.9 GPU utilization fraction above which blocks offload to CPU <code>--kv-transfer-bandwidth</code> 100.0 GPU\u2192CPU transfer rate in blocks/tick <code>--kv-transfer-base-latency</code> 0 Fixed per-transfer latency in ticks"},{"location":"guide/kv-cache/#chunked-prefill","title":"Chunked Prefill","text":"<p>Long prefill sequences can cause head-of-line (HOL) blocking \u2014 a 2,048-token prefill takes ~43ms, blocking shorter requests from starting.</p> <p>Chunked prefill splits long prefills into smaller chunks:</p> <pre><code>./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --long-prefill-token-threshold 256 \\\n  --rate 100 --num-requests 500\n</code></pre> <p>Chunked prefill benefits TTFT, not ITL</p> <p>With <code>--long-prefill-token-threshold=256</code>, short-request TTFT p99 improves by ~52% in bimodal workloads. But ITL is unaffected (&lt;0.5%) because ~255 of ~256 ITL samples per request are decode-only steps. The benefit is in scheduling new requests, not in token generation speed.</p>"},{"location":"guide/kv-cache/#batch-formation-parameters","title":"Batch Formation Parameters","text":"<p>KV cache pressure is directly coupled to batch formation:</p> Flag Default Description <code>--max-num-running-reqs</code> 256 Maximum requests in the running batch <code>--max-num-scheduled-tokens</code> 2048 Token budget per step <p>These are the primary capacity knobs \u2014 in vLLM terms, <code>max_num_seqs</code> and <code>max_num_batched_tokens</code>. Reducing them decreases KV cache pressure but also reduces throughput.</p>"},{"location":"guide/kv-cache/#identifying-the-kv-pressure-cliff","title":"Identifying the KV Pressure Cliff","text":"<p>Preemption rates spike non-linearly as KV blocks decrease past a threshold. The threshold depends on your workload's median token count (not mean or tail):</p> <pre><code># Sweep KV blocks to find the cliff\nfor blocks in 100000 50000 20000 10000 5000 3000; do\n  echo \"=== blocks=$blocks ===\"\n  ./blis run --model meta-llama/llama-3.1-8b-instruct \\\n    --total-kv-blocks $blocks --rate 50 --num-requests 200 2&gt;/dev/null \\\n    | grep -E \"preemption_count|completed_requests\"\ndone\n</code></pre> <p>Distribution median drives KV pressure</p> <p>ParetoLogNormal distributions produce fewer preemptions than Gaussian despite similar means, because the Pareto component's median (~79 tokens) is much lower than Gaussian's median (~256 tokens). Short requests cycle faster, creating \"breathing room\" in the KV cache.</p>"},{"location":"guide/kv-cache/#further-reading","title":"Further Reading","text":"<ul> <li>Core Engine: KV Cache \u2014 internal mechanics</li> <li>Configuration Reference \u2014 all KV cache flags</li> <li>Metrics &amp; Results \u2014 understanding preemption rate, cache hit rate, KV thrashing</li> </ul>"},{"location":"guide/latency-models/","title":"Latency Models","text":"<p>The <code>LatencyModel</code> interface determines how BLIS estimates GPU step time for each batch iteration. BLIS ships two backends -- blackbox (data-driven) and roofline (analytical) -- and the pluggable architecture supports adding custom backends.</p> <pre><code># Blackbox mode (default) \u2014 uses pre-trained coefficients\n./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 4 --rate 100 --num-requests 500\n\n# Roofline mode \u2014 analytical estimation from model architecture\n./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --roofline --hardware H100 --tp 2 \\\n  --num-instances 4 --rate 100 --num-requests 500\n</code></pre>"},{"location":"guide/latency-models/#blackbox-mode-default","title":"Blackbox Mode (Default)","text":"<p>Blackbox mode uses trained regression coefficients from <code>defaults.yaml</code>, fit offline via Bayesian optimization against real vLLM measurements.</p> <p>Beta coefficients <code>[beta0, beta1, beta2]</code> estimate GPU step time:</p> <pre><code>StepTime = beta0 + beta1 * cache_miss_tokens + beta2 * decode_tokens\n</code></pre> <ul> <li><code>beta0</code> -- fixed per-step overhead (microseconds)</li> <li><code>beta1</code> -- cost per prefill token (cache miss)</li> <li><code>beta2</code> -- cost per decode token</li> </ul> <p>Alpha coefficients <code>[alpha0, alpha1, alpha2]</code> estimate CPU-side overhead:</p> <pre><code>QueueingTime           = alpha0 + alpha1 * input_length\nOutputTokenProcessingTime = alpha2\n</code></pre> <p>Pre-trained coefficient sets exist in <code>defaults.yaml</code> for common model/GPU/TP combinations (e.g., <code>meta-llama/llama-3.1-8b-instruct</code> on H100 with TP=2).</p> <p>Alpha overhead is non-blocking</p> <p>Alpha coefficients model CPU post-processing (tokenization, output serialization) that runs concurrently with GPU execution. Alpha time inflates TTFT and ITL metrics but does not block step scheduling -- the next batch step is scheduled at <code>now + stepTime</code> regardless of alpha overhead. This matches real vLLM's asynchronous post-processing pipeline.</p>"},{"location":"guide/latency-models/#roofline-mode-analytical","title":"Roofline Mode (Analytical)","text":"<p>Roofline mode computes step time analytically from model architecture (FLOPs, parameter count) and hardware specifications (compute throughput, memory bandwidth). It does not require pre-trained coefficients, making it suitable for new models.</p>"},{"location":"guide/latency-models/#the-roofline-flag","title":"The <code>--roofline</code> Flag","text":"<p>The simplest way to use roofline mode:</p> <pre><code>./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --roofline --hardware H100 --tp 2\n</code></pre> <p>This auto-resolves both required inputs:</p> <ol> <li>Model config -- checks <code>model_configs/</code> for a cached <code>config.json</code>, fetches from HuggingFace on miss</li> <li>Hardware config -- uses the bundled <code>hardware_config.json</code></li> </ol> <p>For gated models (e.g., LLaMA), set <code>HF_TOKEN</code>:</p> <pre><code>export HF_TOKEN=your_token_here\n./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --roofline --hardware H100 --tp 2\n</code></pre>"},{"location":"guide/latency-models/#manual-configuration","title":"Manual Configuration","text":"<p>For full control, provide configs explicitly:</p> <pre><code>./blis run --model my-custom-model \\\n  --model-config-folder ./my-model-configs/ \\\n  --hardware-config ./my-hardware-config.json \\\n  --hardware H100 --tp 4\n</code></pre>"},{"location":"guide/latency-models/#adding-support-for-new-models","title":"Adding Support for New Models","text":"<p>Any model with a HuggingFace <code>config.json</code> can use roofline mode:</p> <ol> <li>Download <code>config.json</code> from HuggingFace</li> <li>Place it in <code>model_configs/&lt;model-name&gt;/config.json</code></li> <li>Run with <code>--roofline --hardware &lt;GPU&gt; --tp &lt;N&gt;</code></li> </ol> <p>Or let BLIS fetch it automatically with <code>--roofline</code>.</p>"},{"location":"guide/latency-models/#tensor-parallelism-and-roofline","title":"Tensor Parallelism and Roofline","text":"<p>The <code>--tp</code> flag divides FLOPs and memory bandwidth across TP ranks:</p> <ul> <li>Higher TP reduces per-GPU step time (more parallelism)</li> <li>Higher TP reduces KV blocks per GPU (memory split across ranks)</li> </ul> <p>When choosing between TP and replication (more instances): TP reduces per-request latency, replication increases throughput. For capacity planning, simulate both configurations.</p>"},{"location":"guide/latency-models/#when-to-use-which","title":"When to Use Which","text":"Aspect Blackbox (default) Roofline When to use Model has pre-trained coefficients in <code>defaults.yaml</code> New model, no coefficients available, quick estimation Data required <code>defaults.yaml</code> entry for model/GPU/TP HuggingFace <code>config.json</code> + <code>hardware_config.json</code> Accuracy Higher tail accuracy -- trained on real vLLM measurements Good mean accuracy -- analytical estimation Alpha overhead Full alpha modeling (queueing + output processing) Alpha from coefficients; step time is analytical <p>Roofline for model fit, blackbox for SLO validation</p> <p>Use roofline for \"can this model serve at this rate?\" analysis -- mean latency rankings are equivalent across modes (H19 experiment confirmed weighted &lt; RR = LL ordering is identical). Use blackbox with trained coefficients for tail latency (p99) estimation under load, where alpha overhead (~4.3ms/req) materially affects scheduling timelines and p99 results diverge between modes.</p>"},{"location":"guide/latency-models/#pluggable-architecture","title":"Pluggable Architecture","text":"<p>The <code>LatencyModel</code> interface (defined in <code>sim/latency_model.go</code>) has five methods:</p> Method Purpose <code>StepTime(batch)</code> Duration of one batch step given the running batch <code>QueueingTime(req)</code> Arrival-to-queue delay for a request <code>OutputTokenProcessingTime()</code> Per-token post-processing time <code>SchedulingProcessingTime()</code> Scheduling overhead per request <code>PreemptionProcessingTime()</code> Preemption overhead per eviction <p>All time estimates are in microseconds (ticks).</p> <p>New backends register via the <code>NewLatencyModelFunc</code> variable in <code>sim/latency_model.go</code>. The <code>sim/latency/register.go</code> file uses <code>init()</code> to wire the factory, breaking the import cycle between <code>sim/</code> (interface owner) and <code>sim/latency/</code> (implementation). To add a custom backend, implement the five methods and register your factory via <code>init()</code> in a sub-package. See Extension Recipes for a step-by-step guide.</p>"},{"location":"guide/latency-models/#further-reading","title":"Further Reading","text":"<ul> <li>Roofline Estimation -- the mathematical model behind roofline step time calculation</li> <li>Configuration Reference -- all roofline-related CLI flags</li> </ul>"},{"location":"guide/results/","title":"Metrics &amp; Results","text":"<p>This guide covers how to read BLIS output \u2014 from the primary JSON metrics to anomaly counters, KV cache diagnostics, per-SLO breakdowns, fitness scores, and trace summaries.</p> <pre><code># Quick example: run with all diagnostic output enabled\n./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 4 --rate 200 --num-requests 1000 \\\n  --trace-level decisions --summarize-trace \\\n  --fitness-weights \"p99_ttft:3,mean_e2e:1,throughput:2\"\n</code></pre>"},{"location":"guide/results/#primary-metrics-json-output","title":"Primary Metrics (JSON Output)","text":"<p>The JSON output on stdout contains:</p> Field Unit Description <code>ttft_mean_ms</code> ms Mean Time To First Token \u2014 average responsiveness <code>ttft_p90_ms</code> ms 90th percentile TTFT <code>ttft_p99_ms</code> ms 99th percentile TTFT \u2014 tail latency <code>e2e_mean_ms</code> ms Mean End-to-End latency \u2014 average total request time <code>e2e_p99_ms</code> ms 99th percentile E2E <code>itl_mean_ms</code> ms Mean Inter-Token Latency \u2014 streaming smoothness <code>scheduling_delay_p99_ms</code> ms 99th percentile scheduling delay \u2014 queue wait time <code>responses_per_sec</code> req/s Throughput <code>completed_requests</code> count Requests that finished before horizon"},{"location":"guide/results/#scheduling-delay","title":"Scheduling Delay","text":"<p>Scheduling delay isolates the WaitQ wait time from compute time. High scheduling delay + low preemptions = queue saturation (add instances). Low scheduling delay + high TTFT = compute saturation (reduce batch size or use chunked prefill).</p> <p>Per-request units gotcha</p> <p>With <code>--results-path</code>, per-request <code>scheduling_delay_ms</code> is in ticks (microseconds) despite the field name. The aggregate <code>scheduling_delay_p99_ms</code> IS in milliseconds (divided by 1000). Always check units when comparing per-request to aggregate metrics.</p>"},{"location":"guide/results/#anomaly-counters","title":"Anomaly Counters","text":"<p>When anomalies are detected, BLIS prints <code>=== Anomaly Counters ===</code>:</p> Counter What It Means Action Priority Inversions A lower-priority request was scheduled before a higher-priority one Check scheduler choice \u2014 use <code>priority-fcfs</code> for SLO workloads HOL Blocking Events A long prefill blocked shorter requests Enable chunked prefill: <code>--long-prefill-token-threshold 256</code> Rejected Requests Admission policy rejected the request Check token bucket capacity or admission policy Dropped Unservable Request needs more KV blocks than exist Increase <code>--total-kv-blocks</code> or reduce max input tokens"},{"location":"guide/results/#kv-cache-metrics","title":"KV Cache Metrics","text":"<p>When KV cache activity is nonzero, BLIS prints <code>=== KV Cache Metrics ===</code>:</p> Metric Meaning Concern Threshold Preemption Rate Fraction of requests that were preempted (KV evicted) &gt; 5% indicates KV pressure Cache Hit Rate Fraction of blocks served from prefix cache Higher is better \u2014 indicates prefix reuse KV Thrashing Rate Repeated preemption-reallocation cycles &gt; 0 indicates severe memory pressure"},{"location":"guide/results/#per-slo-class-metrics","title":"Per-SLO-Class Metrics","text":"<p>When multiple SLO classes are present in the workload, BLIS prints per-class TTFT and E2E distributions. This lets you verify that <code>critical</code> requests meet SLOs even when <code>batch</code> traffic is heavy.</p>"},{"location":"guide/results/#fitness-evaluation","title":"Fitness Evaluation","text":"<p>For automated multi-configuration comparison:</p> <pre><code>--fitness-weights \"p99_ttft:3,mean_e2e:1,throughput:2\"\n</code></pre> <p>Valid metric keys: <code>throughput</code>, <code>tokens_per_sec</code>, <code>p99_ttft</code>, <code>p50_ttft</code>, <code>mean_ttft</code>, <code>p99_e2e</code>, <code>p50_e2e</code>, <code>mean_e2e</code>.</p>"},{"location":"guide/results/#how-normalization-works","title":"How Normalization Works","text":"<ul> <li>Latency metrics: <code>1 / (1 + value/1000)</code> \u2014 lower latency \u2192 higher score. Reference: 1000 ticks = 1ms</li> <li>Throughput metrics: <code>value / (value + reference)</code> \u2014 higher throughput \u2192 higher score. References: RPS=100, TPS=10,000</li> </ul> <p>Normalization compresses large differences</p> <p>The <code>1/(1+x/1000)</code> function compresses large raw differences into small score differences. A 38% TTFT p99 improvement (39,000\u219264,000 ticks) maps to only 2-8% fitness score difference. Always examine raw metrics alongside fitness scores for meaningful comparison.</p>"},{"location":"guide/results/#common-patterns","title":"Common Patterns","text":""},{"location":"guide/results/#saturation-curves","title":"Saturation Curves","text":"<p>As arrival rate increases past per-instance service capacity (\u03bc \u2248 1/step_time), TTFT p99 grows super-linearly. The queue growth rate <code>excess = \u03bb/k - \u03bc</code> determines how quickly latency degrades.</p>"},{"location":"guide/results/#tail-latency-spikes","title":"Tail Latency Spikes","text":"<p>P99 diverges from mean sharply near saturation. A workload at 90% capacity may show 2x mean TTFT but 10x P99 TTFT.</p>"},{"location":"guide/results/#snapshot-staleness-effects","title":"Snapshot Staleness Effects","text":"<p>With <code>kv-utilization</code> scorer alone at <code>--snapshot-refresh-interval 100ms</code>: +354% TTFT degradation. The default composite scorer mitigates ~99% of this effect.</p>"},{"location":"guide/results/#policy-equivalence-at-low-load","title":"Policy Equivalence at Low Load","text":"<p>All routing policies produce equivalent results (within 5%) at low utilization. Differentiation requires moderate-to-high load where queueing dynamics dominate.</p>"},{"location":"guide/results/#alpha-overhead","title":"Alpha Overhead","text":"<p>BLIS models non-GPU overhead (tokenization, API serialization) as <code>alpha</code> coefficients. Alpha queueing time (alpha0 + alpha1 \u00d7 inputLen) delays request enqueue, creating an event gap, but does not occupy the GPU. Alpha output processing time (alpha2) adds to TTFT/E2E metrics but does not affect step scheduling. This means:</p> <ul> <li>Simulated E2E &gt; theoretical M/M/k E2E (especially at high load)</li> <li>The divergence is 28-71% at \u03c1 \u2265 0.5 but only 0.3-3.3% at \u03c1 \u2264 0.3</li> <li>To compare with theoretical models, use <code>rho_eff = lambda \u00d7 step_total</code> not <code>lambda \u00d7 E2E_total</code></li> </ul>"},{"location":"guide/results/#per-request-results","title":"Per-Request Results","text":"<p>For detailed analysis, save per-request data:</p> <pre><code>./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --rate 100 --num-requests 500 --results-path results.json\n</code></pre> <p>Each request record includes TTFT, E2E, scheduling delay, and completion status.</p>"},{"location":"guide/results/#further-reading","title":"Further Reading","text":"<ul> <li>Configuration Reference \u2014 fitness weight syntax</li> <li>Tutorial: Capacity Planning \u2014 applying results to capacity decisions</li> </ul>"},{"location":"guide/routing/","title":"Routing Policies","text":"<p>This guide covers how BLIS distributes incoming requests across instances in cluster mode. For single-instance simulation, routing is not applicable. For instance-level request ordering, see Scheduling &amp; Priority.</p> <pre><code># Quick example: compare round-robin vs weighted routing\n./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 4 --rate 100 --num-requests 500 \\\n  --routing-policy weighted --trace-level decisions --summarize-trace\n</code></pre>"},{"location":"guide/routing/#available-policies","title":"Available Policies","text":"Policy Flag Value Strategy Round-robin <code>round-robin</code> Cyclic assignment \u2014 request N goes to instance N % k Least-loaded <code>least-loaded</code> Send to the instance with lowest <code>EffectiveLoad</code> Weighted <code>weighted</code> Composable multi-scorer pipeline (default: llm-d parity) Always-busiest <code>always-busiest</code> Pathological template \u2014 sends to the most loaded instance (for testing)"},{"location":"guide/routing/#weighted-scoring-composable-pipeline","title":"Weighted Scoring (Composable Pipeline)","text":"<p>The <code>weighted</code> routing policy is the most flexible. It combines multiple scoring dimensions, each evaluating instances on a <code>[0, 1]</code> scale:</p> <pre><code>--routing-policy weighted --routing-scorers \"prefix-affinity:3,queue-depth:2,kv-utilization:2\"\n</code></pre>"},{"location":"guide/routing/#available-scorers","title":"Available Scorers","text":"Scorer What It Measures llm-d Equivalent <code>prefix-affinity</code> Proportional prefix match ratio via router-side block hash cache prefix-scorer <code>queue-depth</code> Effective load: <code>QueueDepth + BatchSize + InFlightRequests</code> (min-max normalized) queue-scorer <code>kv-utilization</code> Inverse KV utilization: <code>1 - KVUtilization</code> kv-cache-utilization-scorer <code>load-balance</code> Inverse transform: <code>1 / (1 + effectiveLoad)</code> BLIS-native (no llm-d equivalent) <p>Prefix-affinity is a scorer, not a standalone policy</p> <p>The <code>prefix-affinity</code> scorer operates within the <code>weighted</code> routing pipeline, composed with load-balancing scorers. It uses a router-side <code>PrefixCacheIndex</code> with proportional block hash matching and LRU eviction. Always pair it with at least one load-aware scorer (queue-depth or kv-utilization) to prevent cold-start pile-on.</p>"},{"location":"guide/routing/#default-profile","title":"Default Profile","text":"<p>When <code>--routing-scorers</code> is not specified, the default profile is:</p> <pre><code>prefix-affinity:3, queue-depth:2, kv-utilization:2\n</code></pre> <p>This matches the llm-d Endpoint Picker scoring pipeline. Weights are relative \u2014 only ratios matter. <code>[3, 2, 2]</code> behaves identically to <code>[0.43, 0.29, 0.29]</code>.</p>"},{"location":"guide/routing/#signal-freshness","title":"Signal Freshness","text":"<p>Canonical source: Signal freshness tiers are specified in <code>docs/contributing/standards/invariants.md</code> (INV-7). The descriptions below provide additional user-facing context; <code>invariants.md</code> is authoritative if they diverge.</p> <p>Not all routing signals are equally fresh</p> <p>In production inference serving systems (e.g., llm-d), the router is a separate process from the inference engines. Some signals are maintained at the router level, while others require periodic reporting from instances. BLIS models this asymmetry.</p>"},{"location":"guide/routing/#why-signals-have-different-staleness","title":"Why Signals Have Different Staleness","text":"<p>Different signals originate from different places in the system:</p> <ul> <li>Router-local signals are maintained by the router itself \u2014 they're always current because the router controls them directly.</li> <li>Instance-internal signals live on the inference engine and must be communicated to the router \u2014 they're inherently stale by the reporting interval.</li> </ul> <p>BLIS models three signal freshness tiers:</p> Tier Signals Source Freshness Router-local InFlightRequests, prefix cache index Router increments InFlightRequests at dispatch, decrements at completion; prefix cache updated after each routing decision Always fresh \u2014 router owns this state Instance-reported (Immediate/Periodic) QueueDepth, BatchSize, KVUtilization, FreeKVBlocks, CacheHitRate Instance-internal state (scheduler queue, running batch, KV cache) When <code>--snapshot-refresh-interval=0</code> (default): Immediate (read from instance at routing time). When <code>&gt;0</code>: all Prometheus-sourced signals share the same Periodic refresh interval, matching real vLLM's single <code>/metrics</code> endpoint. <p>DES semantics of 'Immediate' mode</p> <p>\"Immediate\" means \"re-read from the instance object at query time\" \u2014 NOT \"perfectly synchronized with the simulation clock.\" At the same clock tick, cluster events are processed before instance events (determinism rule). So a routing decision at time T sees QueueDepth that hasn't yet processed instance events at time T. This is a determinism mechanism (INV-6), not a freshness guarantee.</p>"},{"location":"guide/routing/#staleness-impact","title":"Staleness Impact","text":"<p>At 5,000 req/s with 4 instances, ~45 routing decisions occur between KV utilization updates (~9ms step time). If using <code>kv-utilization:1</code> alone, all 45 decisions see the same stale utilization \u2014 severe load imbalance (3x worse TTFT p99).</p> <p>Safe zone for <code>--snapshot-refresh-interval</code></p> <p>Below 5ms (~1 step time): no degradation. At 10ms: 14% TTFT p99 increase. At 100ms: +354%. The default composite profile (<code>prefix-affinity:3, queue-depth:2, kv-utilization:2</code>) is inherently resilient \u2014 queue-depth's Immediate signal corrects stale KV signals, mitigating ~99% of the effect.</p>"},{"location":"guide/routing/#when-to-use-which-policy","title":"When to Use Which Policy","text":"Workload Recommended Policy Why Uniform traffic, no prefix sharing <code>least-loaded</code> or <code>weighted</code> with <code>queue-depth:1</code> Load balance is the only signal that matters RAG with shared system prompts <code>weighted</code> with <code>prefix-affinity:3,queue-depth:1</code> Prefix affinity maximizes KV cache reuse Mixed SLO classes <code>weighted</code> default + priority scheduling Routing distributes load; scheduling prioritizes critical requests Low traffic (&lt; 10 req/s) Any All policies produce equivalent results within 5%"},{"location":"guide/routing/#example-comparing-policies","title":"Example: Comparing Policies","text":"<p>BLIS includes a routing comparison script:</p> <pre><code>chmod +x examples/routing-comparison.sh\n./examples/routing-comparison.sh\n</code></pre> <p>This runs 5 configurations and shows TTFT p99, target distribution, and throughput for each. See <code>examples/routing-comparison.sh</code> for the full script.</p>"},{"location":"guide/routing/#further-reading","title":"Further Reading","text":"<ul> <li>Scheduling &amp; Priority \u2014 instance-level request ordering</li> <li>Admission Control \u2014 the gate before routing</li> <li>Cluster Architecture \u2014 how the routing pipeline works internally</li> <li>Configuration Reference \u2014 all routing flags</li> <li>Metrics &amp; Results \u2014 understanding trace summaries and regret analysis</li> </ul>"},{"location":"guide/scheduling/","title":"Scheduling &amp; Priority","text":"<p>Routing decides which instance receives a request. Scheduling decides what order requests are processed within an instance. These are independent policy axes -- you can combine any routing policy with any scheduler and any priority policy.</p> <pre><code># Priority-FCFS scheduling with age-based priority in a 4-instance cluster\n./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 4 --rate 100 --num-requests 500 \\\n  --scheduler priority-fcfs --priority-policy slo-based\n</code></pre>"},{"location":"guide/scheduling/#available-schedulers","title":"Available Schedulers","text":"<p>Each scheduler implements the <code>InstanceScheduler</code> interface: a single <code>OrderQueue</code> method called every step to reorder the wait queue before batch formation.</p> Scheduler Flag Value Strategy Notes FCFS <code>--scheduler fcfs</code> First-Come-First-Served. No reordering -- requests are processed in arrival order. Default. Fair and predictable. Priority-FCFS <code>--scheduler priority-fcfs</code> Sort by priority descending, then by arrival time ascending within the same priority. Ties broken by request ID for determinism. Requires a non-constant priority policy to be useful. With <code>--priority-policy constant</code>, all scores are equal and this degrades to FCFS. SJF <code>--scheduler sjf</code> Shortest Job First. Sort by input token count ascending, then by arrival time, then by ID. Optimizes TTFT for short requests but can starve long ones under sustained load. Ignores priority scores entirely. Reverse-priority <code>--scheduler reverse-priority</code> Lowest priority first. Sort by priority ascending, then by arrival time, then by ID. Pathological template for testing only. Causes priority inversions by design. <p>All schedulers use <code>sort.SliceStable</code> for deterministic ordering (INV-6).</p>"},{"location":"guide/scheduling/#priority-policies","title":"Priority Policies","text":"<p>Each priority policy implements the <code>PriorityPolicy</code> interface: a single <code>Compute</code> method that returns a <code>float64</code> score for a request at a given clock tick. Higher scores mean higher priority.</p> Policy Flag Value Formula Notes Constant <code>--priority-policy constant</code> Returns <code>0.0</code> for all requests. Default. No differentiation -- all requests have equal priority. SLO-based <code>--priority-policy slo-based</code> <code>BaseScore + AgeWeight * (clock - arrival_time)</code> Favors older requests. With default <code>AgeWeight=1e-6</code>, a request waiting 1 second (1,000,000 ticks) gets +1.0 priority. Despite the name, does NOT currently use per-request SLO metadata (<code>Request.SLOClass</code> is available but unused by this policy). Inverted-SLO <code>--priority-policy inverted-slo</code> <code>BaseScore - AgeWeight * (clock - arrival_time)</code> Starves older requests -- newer requests get higher priority. Pathological template for testing only. <p>Priority scores are recomputed every step before the scheduler orders the queue. This means SLO-based priority naturally ages: a request that has been waiting longer will have a higher score at the next step.</p>"},{"location":"guide/scheduling/#how-scheduling-and-priority-interact","title":"How Scheduling and Priority Interact","text":"<p>The scheduler and priority policy are independent modules that compose at step time. The simulator calls them in sequence:</p> <ol> <li>Priority assignment: For each queued request, call <code>PriorityPolicy.Compute()</code> and store the result on <code>Request.Priority</code>.</li> <li>Queue reordering: Call <code>InstanceScheduler.OrderQueue()</code> on the wait queue.</li> <li>Batch formation: Dequeue requests from the front of the reordered queue into the running batch.</li> </ol> <p>This means certain combinations are degenerate:</p> Combination Effective Behavior <code>priority-fcfs</code> + <code>constant</code> FCFS (all priority scores are 0.0, so the descending sort changes nothing) <code>sjf</code> + any priority policy SJF (priority scores are computed but ignored -- SJF sorts by input token count) <code>fcfs</code> + <code>slo-based</code> FCFS (priority scores are computed but FCFS does not reorder) <code>priority-fcfs</code> + <code>slo-based</code> The useful combination: older requests float to the top of the queue"},{"location":"guide/scheduling/#preemption-and-re-enqueueing","title":"Preemption and Re-enqueueing","text":"<p>BLIS models vLLM's two-queue architecture (WaitQ + RunningBatch), simplifying vLLM's three-queue model (waiting / running / swapped). When a request is preempted from the running batch due to KV cache pressure:</p> <ul> <li>The request is placed at the front of the WaitQ (not the back).</li> <li>Its progress is reset to zero (recompute mode, matching vLLM's recompute preemption).</li> <li>On the next step, the scheduler reorders the full queue including the preempted request.</li> </ul> <p>This means preempted requests get implicit priority over fresh arrivals in FCFS mode. With priority-fcfs + slo-based, the preempted request's age still determines its position relative to other waiting requests.</p>"},{"location":"guide/scheduling/#when-to-use-which","title":"When to Use Which","text":"Workload Recommended Configuration Why Uniform traffic, no SLO differentiation <code>--scheduler fcfs</code> (default) No reordering needed. All requests are equivalent. Mixed SLO classes needing fairness <code>--scheduler priority-fcfs --priority-policy slo-based</code> Older requests float up, preventing starvation of any class. Latency-sensitive short requests <code>--scheduler sjf</code> Short prompts get processed first. Watch for starvation of long requests under sustained load. Low load (&lt; ~10 req/s) Any Batch sizes are small enough that all schedulers pick the same requests. At low load, all four schedulers produce equivalent results within ~5%. <p>SJF starvation risk</p> <p>Under sustained high load, SJF can indefinitely delay long-prompt requests as short ones keep arriving. BLIS does not currently implement aging or starvation guards for SJF. If your workload has a mix of short and long prompts at high utilization, prefer <code>priority-fcfs</code> + <code>slo-based</code> instead.</p>"},{"location":"guide/scheduling/#further-reading","title":"Further Reading","text":"<ul> <li>Routing Policies -- the upstream decision (which instance)</li> <li>Cluster Simulation -- the full request pipeline from arrival to completion</li> <li>Core Engine: Scheduling Policies -- implementation details and DES mechanics</li> </ul>"},{"location":"guide/skills-and-plugins/","title":"Skills &amp; Plugins","text":"<p>BLIS development uses Claude Code skills and plugins for experimentation, review, and development workflows. Skills are organized in layers -- from project-specific workflows checked into the repository to general-purpose community plugins installed per-user.</p> <pre><code># Quick example: invoke the convergence-review skill for a PR plan\n/convergence-review pr-plan\n\n# Or start a guided hypothesis experiment\n/hypothesis-experiment\n</code></pre>"},{"location":"guide/skills-and-plugins/#skill-layers","title":"Skill Layers","text":"<p>Skills and plugins are organized into five layers, from most specific to most general:</p> Layer Location Purpose Key Examples Project skills <code>.claude/skills/</code> BLIS-specific workflows, checked into the repo <code>convergence-review</code> (multi-perspective review), <code>hypothesis-experiment</code> (guided experimentation) BLIS SDLC plugins <code>sdlc-plugins</code> marketplace BLIS development lifecycle <code>research-ideas</code> (iterative research ideation), <code>hypothesis-test</code> (experiment scaffolding) Superpowers <code>superpowers-marketplace</code> Cross-project development skills <code>superpowers</code> (TDD, debugging, plans, worktrees, brainstorming), <code>episodic-memory</code> (conversation search), <code>elements-of-style</code> (writing quality) Official plugins <code>claude-plugins-official</code> Anthropic official tools <code>commit-commands</code> (git workflow), <code>pr-review-toolkit</code> (PR review), <code>code-review</code>, <code>feature-dev</code>, <code>claude-md-management</code> Community plugins <code>awesome-claude-plugins</code> Community-contributed tools <code>audit-project</code> (multi-agent code review), <code>bug-fix</code>, <code>debugger</code>, <code>test-writer-fixer</code> <p>Project skills take precedence -- they encode BLIS-specific conventions (20 antipattern rules, 8 system invariants, convergence protocol) that generic tools do not know about.</p>"},{"location":"guide/skills-and-plugins/#marketplaces","title":"Marketplaces","text":"<p>Marketplaces are curated GitHub repositories that collect related plugins. Claude Code can install plugins directly from these repositories. Each marketplace has a different focus:</p> <ul> <li>claude-plugins-official -- Anthropic's official plugin collection. Maintained by the Claude Code team with stable, well-tested tools for common development tasks.<ul> <li>URL: https://github.com/anthropics/claude-plugins-official</li> </ul> </li> <li>superpowers-marketplace -- Community superpowers for enhanced development workflows. Provides structured approaches to TDD, debugging, worktree management, and brainstorming.<ul> <li>URL: https://github.com/obra/superpowers-marketplace</li> </ul> </li> <li>awesome-claude-plugins -- Community-contributed plugins covering diverse use cases. A broad collection including code review, testing, debugging, and documentation tools.<ul> <li>URL: https://github.com/ComposioHQ/awesome-claude-plugins</li> </ul> </li> <li>sdlc-plugins -- BLIS project's software development lifecycle plugins. Tailored for BLIS research workflows including hypothesis ideation and experiment scaffolding.<ul> <li>URL: https://github.com/inference-sim/sdlc-plugins</li> </ul> </li> </ul>"},{"location":"guide/skills-and-plugins/#installation","title":"Installation","text":"<p>Project skills (<code>.claude/skills/</code>) require no installation -- they are checked into the repository and automatically available when you open the project with Claude Code.</p> <p>For marketplace plugins, install them using the <code>/install-plugin</code> command inside Claude Code:</p> <pre><code>/install-plugin https://github.com/anthropics/claude-plugins-official/tree/main/commit-commands\n</code></pre> <p>Or install an entire marketplace to browse available plugins:</p> <pre><code>/install-plugin https://github.com/obra/superpowers-marketplace\n</code></pre> <p>Installed plugins persist in <code>~/.claude/plugins/</code> and are available across all projects on your machine.</p>"},{"location":"guide/skills-and-plugins/#which-skills-for-which-workflow","title":"Which Skills for Which Workflow","text":"<p>Each BLIS development workflow uses a different combination of skills. Required skills are essential for the workflow to function correctly; optional skills enhance but are not strictly necessary.</p> Workflow Required Skills Optional Skills PR Development <code>superpowers</code> (worktrees, writing-plans, executing-plans, verification), <code>commit-commands</code>, <code>convergence-review</code> <code>pr-review-toolkit</code>, <code>systematic-debugging</code> Hypothesis Experiments <code>convergence-review</code>, <code>hypothesis-experiment</code> <code>hypothesis-test</code> (from sdlc-plugins), <code>commit-commands</code> Design Process <code>convergence-review</code> <code>superpowers</code> (brainstorming) Research <code>research-ideas</code> (from sdlc-plugins) <code>episodic-memory</code>"},{"location":"guide/skills-and-plugins/#pr-development-example","title":"PR Development Example","text":"<p>A typical PR uses skills at multiple stages:</p> <ol> <li><code>/worktree</code> -- create an isolated working branch (from superpowers)</li> <li><code>/writing-plans</code> -- draft a micro plan with behavioral contracts (from superpowers)</li> <li><code>/convergence-review pr-plan</code> -- multi-perspective plan review (project skill)</li> <li><code>/executing-plans</code> -- implement the plan with TDD (from superpowers)</li> <li><code>/convergence-review pr-code</code> -- multi-perspective code review (project skill)</li> <li><code>/commit</code> -- stage, commit, and push (from commit-commands)</li> </ol>"},{"location":"guide/skills-and-plugins/#hypothesis-experiment-example","title":"Hypothesis Experiment Example","text":"<p>A hypothesis experiment uses the dedicated workflow:</p> <ol> <li><code>/hypothesis-experiment</code> -- guided Steps 0-10 (project skill)</li> <li><code>/convergence-review h-design</code> -- design review gate (project skill)</li> <li><code>/convergence-review h-code</code> -- code review gate (project skill)</li> <li><code>/convergence-review h-findings</code> -- findings review gate (project skill)</li> </ol>"},{"location":"guide/skills-and-plugins/#project-level-vs-user-level","title":"Project-Level vs User-Level","text":"<p>Skills and plugins live at two levels, each with different scope and persistence:</p> <p>Project-level (<code>.claude/skills/</code>) -- Checked into the repository. Automatically available to every developer who clones the project. These encode project-specific knowledge: BLIS convergence protocol parameters, review perspective definitions, experiment workflow steps. Changes go through normal PR review.</p> <p>User-level (<code>~/.claude/plugins/</code>) -- Installed per-user on each developer's machine. Must be installed individually. These provide general-purpose capabilities (git workflow, TDD scaffolding, debugging) that are useful across many projects. Not shared through the repository.</p> <p>The distinction matters for onboarding: project skills work immediately after <code>git clone</code>, while user-level plugins require each contributor to run their own installation steps.</p>"},{"location":"guide/skills-and-plugins/#further-reading","title":"Further Reading","text":"<ul> <li>PR Workflow -- skill usage in PR development</li> <li>Hypothesis Experiments -- skill usage in experimentation</li> <li>Convergence Protocol -- the review protocol that <code>convergence-review</code> implements</li> <li>Extension Recipes -- step-by-step guides that skills help execute</li> </ul>"},{"location":"guide/workloads/","title":"Workload Specifications","text":"<p>This guide covers how to define the traffic patterns BLIS simulates \u2014 from simple CLI flags to complex multi-client YAML workload specs.</p> <pre><code># Quick example: workload-spec YAML\n./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 4 --workload-spec examples/multiturn-chat-demo.yaml\n</code></pre>"},{"location":"guide/workloads/#workload-modes","title":"Workload Modes","text":"<p>BLIS supports four modes, in order of precedence:</p> Mode Flag Best For Workload-spec YAML <code>--workload-spec &lt;path&gt;</code> Multi-client workloads with custom distributions CLI distribution <code>--rate</code>, <code>--num-requests</code>, <code>--prompt-tokens</code> Quick single-client experiments Named presets <code>--workload chatbot</code> Standard workload profiles CSV traces <code>--workload traces</code> Replaying recorded production traffic"},{"location":"guide/workloads/#modeling-real-workloads","title":"Modeling Real Workloads","text":"<p>This section maps common traffic patterns to YAML workload spec configurations. For schema details, see the Workload Spec Schema.</p>"},{"location":"guide/workloads/#interactive-chat","title":"Interactive Chat","text":"<p>User-facing chat applications need low latency, memoryless arrivals (users arrive independently), and moderate token variance around a central prompt length.</p> <pre><code>clients:\n  - id: \"chat-user\"\n    rate_fraction: 1.0\n    slo_class: \"critical\"           # Latency-sensitive \u2014 tracked separately in metrics\n    prefix_group: \"system-prompt\"   # Shared system prompt enables prefix caching\n    prefix_length: 512              # 512 tokens of shared context prepended to each request\n    arrival:\n      process: poisson              # Memoryless: users arrive independently of each other\n    input_distribution:\n      type: gaussian                # Moderate variance around a typical prompt length\n      params:\n        mean: 256\n        std_dev: 128\n        min: 2\n        max: 4096\n    output_distribution:\n      type: exponential             # Most replies short, occasional long answers\n      params:\n        mean: 128\n</code></pre> <p>Pair with prefix-affinity routing for cache reuse:</p> <pre><code>./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 4 --workload-spec chat.yaml \\\n  --routing-policy weighted --routing-scorers \"prefix-affinity:3,queue-depth:2\"\n</code></pre>"},{"location":"guide/workloads/#rag-with-shared-prefixes","title":"RAG with Shared Prefixes","text":"<p>Retrieval-augmented generation workloads share a common document context across requests. The <code>prefix_group</code> and <code>prefix_length</code> fields model this shared context, and prefix-affinity routing ensures requests with the same prefix hit cached KV blocks on the same instance.</p> <pre><code>clients:\n  - id: \"rag-query\"\n    rate_fraction: 1.0\n    slo_class: \"standard\"\n    prefix_group: \"doc-context\"     # All requests share the retrieved document context\n    prefix_length: 2048             # Large shared prefix (retrieved passages)\n    arrival:\n      process: poisson\n    input_distribution:\n      type: gaussian\n      params:\n        mean: 128                   # Short user queries appended after the prefix\n        std_dev: 64\n        min: 2\n        max: 512\n    output_distribution:\n      type: exponential             # Short answers mostly, occasional long explanations\n      params:\n        mean: 64\n</code></pre> <p>Run with aggressive prefix-affinity to maximize cache reuse:</p> <pre><code>./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --num-instances 4 --workload-spec rag.yaml \\\n  --routing-policy weighted --routing-scorers \"prefix-affinity:3,queue-depth:1\"\n</code></pre>"},{"location":"guide/workloads/#batch-offline-processing","title":"Batch / Offline Processing","text":"<p>Non-interactive workloads (summarization, data extraction) tolerate latency and typically have higher token counts. Use <code>batch</code> or <code>background</code> SLO classes so per-class metrics track them separately from latency-sensitive traffic.</p> <pre><code>clients:\n  - id: \"batch-summarize\"\n    rate_fraction: 1.0\n    slo_class: \"batch\"              # Latency-tolerant \u2014 won't pollute critical-class metrics\n    arrival:\n      process: gamma                # Bursty job queue patterns (jobs submitted in waves)\n      cv: 2.0                       # CV &gt; 1 produces bursts; CV = 1 is Poisson-equivalent\n    input_distribution:\n      type: gaussian\n      params:\n        mean: 4096                  # Long documents for summarization\n        std_dev: 1000\n        min: 100\n        max: 8192\n    output_distribution:\n      type: gaussian\n      params:\n        mean: 512\n        std_dev: 150\n        min: 10\n        max: 2048\n</code></pre>"},{"location":"guide/workloads/#bursty-traffic","title":"Bursty Traffic","text":"<p>For flash sales or traffic spikes, use Gamma arrivals with high CV or cohort spike patterns:</p> <pre><code># Option 1: Sustained burstiness via Gamma CV=3.5\nclients:\n  - id: \"bursty-client\"\n    rate_fraction: 1.0\n    slo_class: \"critical\"\n    arrival:\n      process: gamma\n      cv: 3.5                       # High CV produces sustained burst clusters\n    input_distribution:\n      type: exponential\n      params:\n        mean: 512\n    output_distribution:\n      type: exponential\n      params:\n        mean: 256\n</code></pre> <p>For time-bounded traffic spikes, use cohort <code>spike</code> patterns instead (see Cohort Dynamics below).</p> <p>DES impact of burstiness</p> <p>Gamma CV=3.5 produces 1.66x worse TTFT p99 at sub-saturation because burst events arrive before the prior burst drains. The effect is load-duration dependent: visible at moderate load, drowned by queue growth at high overload.</p>"},{"location":"guide/workloads/#multi-turn-conversations","title":"Multi-Turn Conversations","text":"<p>For multi-round chat with context accumulation (e.g., reasoning models), use the <code>reasoning</code> field:</p> <pre><code>clients:\n  - id: \"reasoning-session\"\n    rate_fraction: 1.0\n    slo_class: \"standard\"\n    arrival:\n      process: poisson\n    input_distribution:\n      type: gaussian\n      params:\n        mean: 256\n        std_dev: 128\n        min: 2\n        max: 2048\n    output_distribution:\n      type: gaussian\n      params:\n        mean: 256\n        std_dev: 128\n        min: 2\n        max: 2048\n    reasoning:\n      reason_ratio_distribution:    # Fraction of output that is \"reasoning\" tokens\n        type: constant\n        params:\n          value: 50                 # 50% reasoning ratio\n      multi_turn:\n        max_rounds: 4              # Up to 4 conversation rounds per session\n        think_time_us: 5000000     # 5 seconds between rounds (user think time)\n        context_growth: accumulate # Each round prepends full prior context\n</code></pre> <p>Context accumulation means round N sees all prior input+output tokens as prefix, creating growing KV cache pressure across rounds.</p>"},{"location":"guide/workloads/#cli-distribution-mode-default","title":"CLI Distribution Mode (Default)","text":"<p>The simplest way to generate traffic:</p> <pre><code>./blis run --model meta-llama/llama-3.1-8b-instruct \\\n  --rate 100 --num-requests 500 \\\n  --prompt-tokens 512 --prompt-tokens-stdev 256 \\\n  --output-tokens 256 --output-tokens-stdev 128\n</code></pre>"},{"location":"guide/workloads/#writing-a-workload-spec-yaml","title":"Writing a Workload-Spec YAML","text":"<p>For complex workloads, use a YAML spec:</p> <pre><code>version: \"2\"\nseed: 42\naggregate_rate: 100       # Total arrival rate (req/s)\nnum_requests: 1000\n\nclients:\n  - id: \"interactive\"\n    rate_fraction: 0.6    # 60% of traffic \u2014 models a dominant chat workload\n    slo_class: \"critical\" # Latency-sensitive: per-class metrics tracked separately\n    prefix_group: \"chat\"  # Shared system prompt \u2014 enables prefix cache reuse\n    prefix_length: 512    # 512-token system prompt prepended to each request\n    arrival:\n      process: poisson    # Memoryless: independent user arrivals (typical for web traffic)\n    input_distribution:\n      type: gaussian      # Moderate variance around a mean prompt length\n      params:\n        mean: 256\n        std_dev: 128\n        min: 2\n        max: 4096\n    output_distribution:\n      type: exponential   # Right-skewed: most replies short, occasional long ones\n      params:\n        mean: 128\n\n  - id: \"batch\"\n    rate_fraction: 0.4    # 40% of traffic \u2014 background processing share\n    slo_class: \"batch\"    # Latency-tolerant: won't pollute critical-class TTFT metrics\n    arrival:\n      process: gamma      # Bursty: jobs submitted in waves from a job queue\n      cv: 2.0             # CV &gt; 1 produces clustered arrivals (CV=1 \u2248 Poisson)\n    input_distribution:\n      type: gaussian\n      params:\n        mean: 1024        # Longer inputs typical for summarization/extraction\n        std_dev: 512\n        min: 2\n        max: 7000\n    output_distribution:\n      type: gaussian\n      params:\n        mean: 512\n        std_dev: 256\n        min: 2\n        max: 7000\n</code></pre>"},{"location":"guide/workloads/#arrival-processes","title":"Arrival Processes","text":"Process Behavior DES Impact Use When <code>poisson</code> Memoryless, exponentially distributed inter-arrival times Steady event stream Default; matches typical web traffic <code>gamma</code> Bursty (CV &gt; 1) or regular (CV &lt; 1) inter-arrivals Burst events create temporary overloads Modeling real traffic with bursts <code>weibull</code> Shape-controlled inter-arrival times Similar to gamma, different tail behavior Specific traffic shape matching <code>constant</code> Fixed inter-arrival time (deterministic) Perfectly regular event stream Controlled experiments, debugging <p>DES implication</p> <p>Arrival processes directly determine the timing of <code>ArrivalEvent</code> injections into the event queue. Gamma CV=3.5 produces 1.66x worse TTFT p99 at sub-saturation because burst events arrive before the prior burst drains.</p>"},{"location":"guide/workloads/#token-distributions","title":"Token Distributions","text":"Type Parameters Behavior <code>gaussian</code> <code>mean</code>, <code>std_dev</code>, <code>min</code>, <code>max</code> Normal distribution, clamped to range <code>exponential</code> <code>mean</code> Right-skewed, long tail <code>pareto_lognormal</code> <code>alpha</code>, <code>xm</code>, <code>mu</code>, <code>sigma</code>, <code>mix_weight</code> Heavy-tailed (Pareto-LogNormal mixture) <code>constant</code> <code>value</code> Fixed token count (useful for controlled experiments) <code>empirical</code> <code>params</code> Inline key-value map (token count \u2192 probability)"},{"location":"guide/workloads/#slo-classes","title":"SLO Classes","text":"<p>Requests can be tagged with SLO classes for per-class metric tracking:</p> Class Intended Use <code>critical</code> Latency-sensitive user-facing requests <code>standard</code> Normal priority <code>sheddable</code> Can be dropped under load <code>batch</code> Offline processing, latency-tolerant <code>background</code> Lowest priority"},{"location":"guide/workloads/#estimating-capacity-for-your-workload","title":"Estimating Capacity for Your Workload","text":"<p>CLI mode and YAML mode have different defaults</p> <p>CLI mode uses <code>--prompt-tokens 512, --output-tokens 512</code> by default (step time ~17.4ms, capacity ~57 req/s per instance). YAML workloads define their own distributions \u2014 a YAML with mean=256/128 has step time ~11.8ms, capacity ~85 req/s. Don't reuse capacity estimates across modes.</p>"},{"location":"guide/workloads/#multi-client-composition","title":"Multi-Client Composition","text":"<p>Use <code>blis compose</code> to merge multiple workload specs into a single spec:</p> <pre><code># Merge a chat workload and a batch workload into one combined spec\n./blis compose --from chat.yaml --from batch.yaml &gt; combined.yaml\n</code></pre> <p>The compose operation:</p> <ul> <li>Concatenates all client lists from each input spec</li> <li>Sums aggregate rates (e.g., 60 req/s + 40 req/s = 100 req/s total)</li> <li>Renormalizes <code>rate_fraction</code> values proportionally: each client's merged fraction = <code>original_fraction * (spec_rate / total_rate)</code>, preserving absolute request rates</li> </ul> <p>This lets you build complex mixed workloads from reusable single-purpose specs.</p>"},{"location":"guide/workloads/#cohort-dynamics","title":"Cohort Dynamics","text":"<p>Cohorts model populations of similar clients with time-varying traffic patterns. Instead of defining individual clients, you specify a population count and a traffic pattern. BLIS expands each cohort into individual <code>ClientSpec</code> entries.</p> <p>Three traffic patterns are available:</p> Pattern Behavior Use Case <code>diurnal</code> Sinusoidal rate modulation over 24 hours (peak_hour, peak_to_trough_ratio) Day/night traffic cycles <code>spike</code> Clients active only during <code>[start_time_us, start_time_us + duration_us)</code> Flash sales, traffic bursts <code>drain</code> Linear ramp-down to zero rate over <code>ramp_duration_us</code> Graceful shutdown, load shedding <pre><code>version: \"2\"\naggregate_rate: 200\nnum_requests: 5000\n\ncohorts:\n  - id: \"daytime-users\"\n    population: 50                  # Expands to 50 individual clients\n    slo_class: \"critical\"\n    rate_fraction: 0.7\n    arrival:\n      process: poisson\n    input_distribution:\n      type: gaussian\n      params: { mean: 256, std_dev: 100, min: 2, max: 2048 }\n    output_distribution:\n      type: exponential\n      params: { mean: 128 }\n    diurnal:\n      peak_hour: 14                 # Peak at 2 PM\n      peak_to_trough_ratio: 3.0    # 3x more traffic at peak vs trough\n\n  - id: \"flash-sale\"\n    population: 20\n    slo_class: \"standard\"\n    rate_fraction: 0.3\n    arrival:\n      process: gamma\n      cv: 2.5\n    input_distribution:\n      type: gaussian\n      params: { mean: 128, std_dev: 50, min: 2, max: 512 }\n    output_distribution:\n      type: exponential\n      params: { mean: 64 }\n    spike:\n      start_time_us: 10000000      # Spike starts at 10 seconds\n      duration_us: 5000000         # Lasts 5 seconds\n</code></pre>"},{"location":"guide/workloads/#advanced-features","title":"Advanced Features","text":""},{"location":"guide/workloads/#multimodal-requests","title":"Multimodal Requests","text":"<p>The <code>multimodal</code> field on a client generates requests with combined text, image, audio, and video tokens. Total input = text + (image tokens x image count) + (audio tokens x audio count) + (video tokens x video count).</p> <pre><code>clients:\n  - id: \"vision-model\"\n    # ... arrival, rate_fraction, etc.\n    multimodal:\n      text_distribution:\n        type: gaussian\n        params: { mean: 128, std_dev: 50, min: 2, max: 512 }\n      image_distribution:\n        type: constant\n        params: { value: 576 }        # Tokens per image (e.g., ViT patch count)\n      image_count_distribution:\n        type: constant\n        params: { value: 1 }          # One image per request\n</code></pre> <p>Audio and video follow the same pattern with <code>audio_distribution</code>/<code>audio_count_distribution</code> and <code>video_distribution</code>/<code>video_count_distribution</code>.</p>"},{"location":"guide/workloads/#reasoning-multi-turn-with-context-accumulation","title":"Reasoning (Multi-Turn with Context Accumulation)","text":"<p>The <code>reasoning</code> field generates multi-turn conversation sessions where each round can accumulate prior context. See the Multi-Turn Conversations section above for a full example. Key fields:</p> <ul> <li><code>reason_ratio_distribution</code>: fraction of output tokens that represent \"reasoning\" (sampled as integer percentage, divided by 100)</li> <li><code>multi_turn.max_rounds</code>: number of conversation rounds per session</li> <li><code>multi_turn.think_time_us</code>: inter-round delay (user think time, in microseconds)</li> <li><code>multi_turn.context_growth</code>: <code>\"accumulate\"</code> to prepend all prior input+output as context, or omit for independent rounds</li> </ul>"},{"location":"guide/workloads/#client-side-network-latency","title":"Client-Side Network Latency","text":"<p>The <code>network</code> field adds client-perspective latency to server-side metrics. Useful for modeling geographically distributed users:</p> <pre><code>clients:\n  - id: \"remote-user\"\n    # ... arrival, rate_fraction, etc.\n    network:\n      rtt_ms: 50                      # Round-trip time in milliseconds\n      bandwidth_mbps: 100             # Link bandwidth (affects upload/download delay)\n</code></pre> <p>Client TTFT = server TTFT + RTT + upload delay. Client E2E = server E2E + RTT + upload delay + download delay. Upload/download delays are computed from token counts (4 bytes per token ID).</p>"},{"location":"guide/workloads/#built-in-presets-and-examples","title":"Built-in Presets and Examples","text":""},{"location":"guide/workloads/#named-presets-from-defaultsyaml","title":"Named Presets from defaults.yaml","text":"<p>BLIS ships with preset workload profiles in <code>defaults.yaml</code>. Use them via the CLI or the convert command:</p> <pre><code># Run directly with a named preset\n./blis run --model meta-llama/llama-3.1-8b-instruct --workload chatbot\n\n# Convert a preset to a v2 WorkloadSpec YAML for customization\n./blis convert preset --name chatbot --rate 10 --num-requests 100 &gt; chatbot.yaml\n</code></pre> <p>Available presets from <code>defaults.yaml</code>:</p> Preset Prompt Mean Output Mean Description <code>chatbot</code> 256 256 Interactive chat with moderate token lengths <code>contentgen</code> 1024 1024 Content generation with balanced I/O <code>summarization</code> 4096 512 Long-document summarization (high input, moderate output) <code>multidoc</code> 10240 1536 Multi-document processing (very long inputs)"},{"location":"guide/workloads/#scenario-presets-programmatic","title":"Scenario Presets (Programmatic)","text":"<p>The <code>sim/workload/scenarios.go</code> module provides scenario functions for common workload patterns. These are used internally by the simulator and in hypothesis experiments:</p> Scenario Function Key Characteristics Bursty traffic <code>ScenarioBurstyTraffic</code> Gamma CV=3.5, exponential tokens, <code>batch</code> SLO Unfair tenants <code>ScenarioUnfairTenants</code> 90% low-priority batch + 10% high-priority critical Prefix-heavy <code>ScenarioPrefixHeavy</code> 80% shared-prefix + 20% unique, tests prefix caching Mixed SLO <code>ScenarioMixedSLO</code> Equal mix of critical/standard/batch classes"},{"location":"guide/workloads/#example-files","title":"Example Files","text":"<p>BLIS ships with example workload specs in <code>examples/</code>:</p> File Description <code>multiturn-chat-demo.yaml</code> Multi-turn chat with prefix-affinity routing <code>prefix-affinity-demo.yaml</code> Shared-prefix workload for cache testing <code>servegen-language.yaml</code> ServeGen-derived language workload <code>inference-perf-shared-prefix.yaml</code> inference-perf format compatibility"},{"location":"guide/workloads/#further-reading","title":"Further Reading","text":"<ul> <li>Workload Spec Schema \u2014 complete field reference</li> <li>Configuration Reference \u2014 all workload flags</li> </ul>"},{"location":"methodology/","title":"Methodology","text":"<p>Reusable research methodologies developed and validated through BLIS experiments. These approaches are domain-agnostic \u2014 while they were refined on LLM inference serving, the techniques apply to any system with configurable policies and a simulation or benchmark harness.</p>"},{"location":"methodology/#methodology-pages","title":"Methodology Pages","text":"Page Description Strategy Evolution Iterative policy discovery through simulation: hypothesis-bundle-driven methodology with multi-judge review, convergence-gated verification, Bayesian parameter optimization, and cumulative principle extraction Hypothesis Bundles in Practice Detailed examples of hypothesis bundles from PR #452 (scheduling) and PR #447 (routing), prediction error analysis, bundle sizing, and writing guidelines"},{"location":"methodology/#when-to-use-strategy-evolution","title":"When to Use Strategy Evolution","text":"<p>Strategy Evolution is the right approach when:</p> <ul> <li>Your system has multiple interacting policy layers (routing, scheduling, memory, admission) where interactions produce non-obvious emergent behaviors</li> <li>The optimal configuration cannot be derived analytically because layer interactions are too complex</li> <li>You have a deterministic simulator or benchmark that accepts parameterized configuration and produces machine-parseable metrics</li> <li>You need defensible results \u2014 not just \"it works\" but \"here's why it works, here's the evidence, and here are the principles\"</li> </ul>"},{"location":"methodology/#relationship-to-other-processes","title":"Relationship to Other Processes","text":"<p>Strategy Evolution integrates existing BLIS processes:</p> <ul> <li>Hypothesis Experiments \u2014 Each strategy iteration is formulated as a hypothesis bundle. The hypothesis experiment framework provides the per-arm workflow (experiment design standards, review gates, convergence protocol).</li> <li>Convergence Protocol \u2014 Three convergence-gated review stages per iteration: Design Review (5 perspectives), Code Review (5 perspectives), FINDINGS Review (10 perspectives).</li> <li>PR Workflow \u2014 Implementation of winning strategies follows the standard PR workflow.</li> </ul>"},{"location":"methodology/hypothesis-bundles/","title":"Hypothesis Bundles in Practice","text":"<p>This page provides detailed examples of hypothesis bundles drawn from real Strategy Evolution experiments. It accompanies the main Strategy Evolution methodology page.</p> <p>Experimental Extensions</p> <p>The examples below describe experimental strategies (SLO-tiered priority, SLO-gated admission, per-SLO prefill thresholds) that were implemented as custom extensions during Strategy Evolution experiments. These are not yet available as standard BLIS policy templates. The examples illustrate the methodology, not current BLIS capabilities. See the Experimental Configurations note for details.</p>"},{"location":"methodology/hypothesis-bundles/#what-is-a-hypothesis-bundle","title":"What is a Hypothesis Bundle?","text":"<p>A hypothesis bundle replaces the informal strategy description (\"try mechanism X with parameters Y\") with a falsification framework designed before implementation. It is a set of testable predictions that collectively define a strategy and its validation apparatus.</p> <p>Every bundle contains up to five canonical arm types. Compound mechanisms use all five; simpler iterations use subsets (see when to skip). Bundles may also include application-specific arms beyond the canonical five \u2014 for example, H-zero-sum (side-effect detection on a different metric) as shown in the scheduling example below.</p> <pre><code>flowchart LR\n    S[\"Selected Strategy\"]\n\n    S --&gt; HM[\"H-main&lt;br/&gt;Does the mechanism work?&lt;br/&gt;What effect size? Why?\"]\n    S --&gt; HA[\"H-ablation&lt;br/&gt;Which components matter?&lt;br/&gt;Are any redundant?\"]\n    S --&gt; HSA[\"H-super-additivity&lt;br/&gt;Do components interact?&lt;br/&gt;Is compound &gt; sum of parts?\"]\n    S --&gt; HC[\"H-control-negative&lt;br/&gt;Where should the effect vanish?&lt;br/&gt;Confirms mechanism specificity\"]\n    S --&gt; HR[\"H-robustness&lt;br/&gt;Where does the strategy break?&lt;br/&gt;Generalization boundaries\"]\n\n    style HM fill:#c8e6c9\n    style HA fill:#ffecb3\n    style HSA fill:#ffecb3\n    style HC fill:#e1bee7\n    style HR fill:#bbdefb</code></pre> <p>Every arm includes three elements:</p> <ol> <li>Quantitative prediction \u2014 a specific metric and threshold (e.g., \"&gt;30% TTFT P99 improvement\")</li> <li>Causal mechanism \u2014 WHY the prediction should hold (e.g., \"because priority ordering allows critical requests to bypass the queue\")</li> <li>Diagnostic clause \u2014 what a failure would indicate (e.g., \"if this fails, priority gaps are insufficient at this load\")</li> </ol>"},{"location":"methodology/hypothesis-bundles/#example-scheduling-track-iteration-1-pr-452","title":"Example: Scheduling Track, Iteration 1 (PR #452)","text":"<p>The scheduling track of Strategy Evolution (PR #452, draft) explored SLO-aware scheduling strategies over 11 iterations, achieving -73.7% critical TTFT P99. Here is how iteration 1 \u2014 the SLO-tiered priority strategy \u2014 would be formulated as a hypothesis bundle.</p>"},{"location":"methodology/hypothesis-bundles/#the-scheduling-strategy","title":"The scheduling strategy","text":"<p>SLO-tiered priority with base scores: critical=10, standard=5, sheddable=1. Piecewise-linear urgency thresholds prevent starvation. A priority bridge passes SLO hints from the router to the scheduler.</p>"},{"location":"methodology/hypothesis-bundles/#the-scheduling-bundle-designed-at-phase-2-before-implementation","title":"The scheduling bundle (designed at Phase 2, before implementation)","text":""},{"location":"methodology/hypothesis-bundles/#h-main-scheduling-mechanism-claim","title":"H-main \u2014 scheduling mechanism claim","text":"<p>\"SLO-tiered priority with base scores [critical=10, standard=5, sheddable=1] will reduce critical TTFT P99 by &gt;30% at rate=2000 req/s because priority ordering allows critical requests to be scheduled ahead of lower-priority requests when they contend in the wait queue.</p> <p>If this fails, it indicates priority gaps are insufficient to overcome queue depth at this load level.\"</p> <p>Experiment design:</p> <ul> <li>Treatment: SLO-tiered priority scheduler + priority bridge</li> <li>Control: FCFS scheduler (baseline)</li> <li>Workload: Orthogonal SLO tiers (identical request shapes, SLO class is only differentiator)</li> <li>Rate: 2000 req/s (near saturation)</li> <li>Seeds: 42, 123, 456</li> </ul>"},{"location":"methodology/hypothesis-bundles/#h-ablation-priority-scheduling-component-isolation","title":"H-ablation-priority \u2014 scheduling component isolation","text":"<p>\"Removing priority ordering (reverting to FCFS) while keeping the priority bridge and all other components will degrade critical TTFT P99 by &gt;20%, because priority scheduling is the primary mechanism.</p> <p>If degradation is &lt;5%, the improvement comes from the routing bridge, not the scheduler.\"</p> <p>Experiment design:</p> <ul> <li>Treatment: Full compound strategy</li> <li>Control: Same strategy but with FCFS scheduler (priority disabled)</li> <li>Vary: Scheduler only (one dimension \u2014 ED-1)</li> </ul>"},{"location":"methodology/hypothesis-bundles/#h-zero-sum-cluster-wide-side-effect-detection-application-specific-arm","title":"H-zero-sum \u2014 cluster-wide side-effect detection (application-specific arm)","text":"<p>Note: This arm replaces H-super-additivity for this strategy. The priority bridge is infrastructure (not an independent mechanism), so component interaction testing is less relevant than side-effect detection. H-zero-sum tests whether improving one class harms another \u2014 a question specific to priority-based strategies at saturation.</p> <p>\"The compound strategy at near-saturation (rate=2000) will NOT degrade cluster-wide TTFT P99 by more than 10%, because priority reordering should primarily benefit critical requests without significantly harming others.</p> <p>If cluster P99 degrades &gt;30%, scheduling is zero-sum at saturation \u2014 improving one class directly worsens another. A non-zero-sum lever (e.g., admission control) would be needed to break the compute floor.\"</p> <p>Experiment design:</p> <ul> <li>Treatment: Full compound strategy</li> <li>Metric: Both critical-class and cluster-wide TTFT P99</li> <li>Rate: Near-saturation (2000 req/s)</li> </ul>"},{"location":"methodology/hypothesis-bundles/#h-control-negative-scheduling-mechanism-specificity","title":"H-control-negative \u2014 scheduling mechanism specificity","text":"<p>\"Applying the strategy to a uniform workload (all requests same SLO class) will produce &lt;5% improvement, confirming the mechanism requires SLO differentiation.</p> <p>If improvement exceeds 5%, the mechanism has an SLO-independent component we haven't identified.\"</p> <p>Experiment design:</p> <ul> <li>Treatment: Full compound strategy</li> <li>Workload: All requests labeled <code>standard</code> (uniform SLO)</li> <li>Expected: Byte-identical or &lt;5% difference from baseline</li> </ul>"},{"location":"methodology/hypothesis-bundles/#h-robustness-scheduling-generalization-boundary","title":"H-robustness \u2014 scheduling generalization boundary","text":"<p>\"The strategy maintains &gt;40% critical TTFT improvement when KV blocks decrease from 132K to 3000, because the priority mechanism doesn't depend on KV abundance. Below 3000, degradation is expected as KV contention dominates scheduling order.</p> <p>If improvement collapses above 3000 blocks, KV state is confounding the priority mechanism.\"</p>"},{"location":"methodology/hypothesis-bundles/#what-actually-happened-and-what-hypothesis-bundles-would-have-caught-earlier","title":"What actually happened (and what hypothesis bundles would have caught earlier)","text":"<p>The actual iteration 1 results:</p> Metric Prediction Outcome Match? Critical TTFT P99 &gt;30% improvement -50.8% Confirmed (exceeded prediction) Cluster TTFT P99 &lt;10% degradation +62.4% Refuted (zero-sum!) Throughput &lt;5% change ~0% Confirmed <p>The H-zero-sum prediction was refuted \u2014 cluster P99 degraded 62.4%, far exceeding the predicted &lt;10%. The diagnostic clause (\"a non-zero-sum lever is needed\") pointed directly to admission control as the solution. Under the old methodology without hypothesis bundles, this zero-sum behavior was an informal surprise that took three iterations and human insight to diagnose. With the bundle, the diagnostic clause would have redirected effort to admission control in iteration 1's FINDINGS analysis.</p>"},{"location":"methodology/hypothesis-bundles/#example-routing-track-iteration-6-pr-447","title":"Example: Routing Track, Iteration 6 (PR #447)","text":"<p>The routing track of Strategy Evolution (PR #447, draft) explored routing scorer optimization over 19 iterations, discovering that KV-utilization as a routing scorer is counterproductive (RP-6). Here is how the iteration that discovered this finding would be formulated.</p>"},{"location":"methodology/hypothesis-bundles/#the-routing-strategy","title":"The routing strategy","text":"<p>Drop the KV-utilization scorer from the routing pipeline. Use <code>pa:3,qd:2</code> instead of <code>pa:3,qd:2,kv:2</code>.</p>"},{"location":"methodology/hypothesis-bundles/#key-routing-bundle-arms","title":"Key routing bundle arms","text":""},{"location":"methodology/hypothesis-bundles/#h-main-routing-mechanism-claim","title":"H-main \u2014 routing mechanism claim","text":"<p>\"Removing the KV-utilization scorer will improve or maintain TTFT P99 compared to the full <code>pa:3,qd:2,kv:2</code> configuration, because the <code>1-KVUtilization</code> formula routes away from instances with high occupancy \u2014 which are often the instances with the most valuable cached prefixes (in prefix-heavy workloads). Under memory pressure, this anti-correlation is especially harmful.</p> <p>If removing kv-util degrades performance, cache affinity and memory pressure are not correlated in this workload.\"</p>"},{"location":"methodology/hypothesis-bundles/#h-ablation-kv-routing-signal-isolation","title":"H-ablation-kv \u2014 routing signal isolation","text":"<p>\"Under KV pressure (reduced blocks), the <code>pa:3,qd:2</code> configuration will improve by &gt;10% over <code>pa:3,qd:2,kv:2</code>, because KV-utilization penalizes the instances with the best cache locality.</p> <p>If &lt;5% difference under pressure, KV-util's routing harm is negligible even in the regime where it's theoretically worst.\"</p>"},{"location":"methodology/hypothesis-bundles/#h-control-negative-routing-regime-where-kv-util-should-help","title":"H-control-negative \u2014 routing regime where kv-util should help","text":"<p>\"Under abundant KV blocks (default 132K), removing kv-util will produce &lt;5% difference, because with abundant memory the signal is degenerate (all instances have low KV utilization).</p> <p>If &gt;5% improvement even with abundant blocks, kv-util is harmful in ALL regimes, not just under pressure.\"</p>"},{"location":"methodology/hypothesis-bundles/#what-this-routing-bundle-would-have-revealed","title":"What this routing bundle would have revealed","text":"<p>The actual routing track results showed:</p> <ul> <li>Removing kv-util improved performance by 4% even with abundant blocks</li> <li>Under KV pressure, the improvement was 23-25%</li> <li>The H-control-negative was borderline (4% is below the 5% threshold, so formally the control passed). However, the direction of improvement \u2014 positive even with abundant blocks \u2014 suggested regime-independent harm that warranted investigation</li> </ul> <p>Combined with the KV-pressure results (23-25%), this led to principle RP-6: \"KV-utilization as a routing scorer is counterproductive under memory pressure.\" The consistent direction of improvement across regimes additionally suggested this should be a universal recommendation.</p>"},{"location":"methodology/hypothesis-bundles/#why-prediction-errors-matter","title":"Why Prediction Errors Matter","text":"<p>The hypothesis bundle's most powerful feature is not confirming that strategies work \u2014 it's diagnosing why predictions fail.</p> <pre><code>flowchart TD\n    Pred[\"Prediction&lt;br/&gt;(designed in Phase 2)\"]\n    Obs[\"Observation&lt;br/&gt;(measured in Phase 3)\"]\n\n    Pred --&gt; Comp{\"Prediction matches&lt;br/&gt;observation?\"}\n    Obs --&gt; Comp\n\n    Comp --&gt;|Yes| Conf[\"CONFIRMED&lt;br/&gt;Mechanism verified&lt;br/&gt;Principle extracted\"]\n    Comp --&gt;|No| Disc[\"DISCREPANCY&lt;br/&gt;Causal model is wrong\"]\n\n    Disc --&gt; Why[\"Trace through code&lt;br/&gt;(RCV-1 to RCV-3)&lt;br/&gt;WHY is it wrong?\"]\n    Why --&gt; Fix[\"New principle that&lt;br/&gt;constrains all future&lt;br/&gt;iterations\"]\n\n    style Disc fill:#ffcdd2\n    style Why fill:#fff3e0\n    style Fix fill:#c8e6c9</code></pre> <p>Three categories of prediction error, each with different learning value:</p> Error type What it reveals Example Direction wrong Fundamental misunderstanding of mechanism Predicted improvement, got degradation \u2192 mechanism is counterproductive Magnitude wrong Correct mechanism, wrong model of strength Predicted &lt;10% degradation, got +62% \u2192 underestimated zero-sum effect Regime wrong Mechanism works but in different conditions Predicted improvement at all loads, only works under saturation \u2192 effect is load-dependent <p>In the scheduling track, the key breakthroughs came from prediction errors:</p> <ol> <li> <p>Iteration 1: H-zero-sum predicted &lt;10% cluster degradation. Observed +62.4%. Error revealed S6 (scheduling is zero-sum at saturation) \u2192 redirected to admission control.</p> </li> <li> <p>Iteration 3: H-main predicted admission gating would break the \"compute floor\" established in iteration 1-opt. Confirmed: critical TTFT dropped from 132ms to 107ms. The prediction-vs-outcome framework made this a testable claim rather than an informal hope.</p> </li> <li> <p>Iteration 4: Ablation predicted disabling chunked prefill for the critical class would save overhead from eliminated chunk boundaries (principle S9: \"Disabling chunking for critical saves 14ms of beta0 overhead\"). Confirmed \u2014 critical TTFT dropped from 132ms to 90ms.</p> </li> </ol>"},{"location":"methodology/hypothesis-bundles/#bundle-size-guide","title":"Bundle Size Guide","text":"<p>The number of hypothesis arms scales with strategy complexity. Use this table as a planning guide:</p> Components Typical arms Breakdown 1 4-5 H-main, H-control-negative, 2-3 H-robustness 2 6-7 H-main, 2 H-ablation, H-super-additivity, H-control-negative, 1-2 H-robustness 3 7-9 H-main, 3 H-ablation, H-super-additivity, H-control-negative, 1-2 H-robustness 4 9-11 H-main, 4 H-ablation, H-super-additivity, H-control-negative, 2-3 H-robustness"},{"location":"methodology/hypothesis-bundles/#managing-bundle-cost","title":"Managing bundle cost","text":"<ol> <li> <p>Tiered review depth:</p> Arm type FINDINGS Review Rationale H-main Full 10-perspective Core claim \u2014 needs maximum scrutiny H-ablation, H-super-additivity 5-perspective Simpler experiments, fewer confounds H-control-negative (byte-identical) Spot-check that treatment was active, then automatic validation Verify the treatment was correctly applied; byte-identical output confirms specificity H-robustness 5-perspective Standard experimental rigor </li> <li> <p>Parallel execution. All arms are independent. Use the parallel execution mode from the hypothesis process. All arms in a bundle must use the same seed set for valid comparison.</p> </li> <li> <p>Fast-fail. If H-main is refuted (after FINDINGS review confirms the refutation is genuine, not an analyzer bug), skip remaining arms. If H-control-negative fails, stop and redesign. See fast-fail rules.</p> </li> </ol>"},{"location":"methodology/hypothesis-bundles/#when-to-skip-the-full-bundle","title":"When to skip the full bundle","text":"<p>Not every iteration needs every arm:</p> Iteration type Required arms Optional New compound mechanism (\u22652 components) H-main, all H-ablation, H-super-additivity, H-control-negative H-robustness Component removal/simplification H-main, H-control-negative, H-ablation for removed component H-robustness Single-component mechanism H-main, H-control-negative H-robustness Parameter-only change (same active components, only numeric values change) H-main only Proceed directly to Bayesian optimization Robustness sweep (post-confirmation) H-robustness arms only \u2014"},{"location":"methodology/hypothesis-bundles/#writing-good-hypothesis-arms","title":"Writing Good Hypothesis Arms","text":""},{"location":"methodology/hypothesis-bundles/#the-three-elements","title":"The three elements","text":"<p>Every hypothesis arm needs:</p> <ol> <li> <p>A quantitative prediction with a threshold. Not \"should improve\" but \"&gt;30% TTFT P99 improvement.\" The threshold makes the prediction falsifiable. A prediction is confirmed if the observed effect matches the predicted direction AND exceeds the stated threshold. It is partially confirmed if the direction is correct but magnitude falls short. It is refuted if the direction is wrong or the magnitude is negligible (&lt;5%).</p> </li> <li> <p>A causal mechanism. Not \"because it's better\" but \"because priority ordering allows critical requests to bypass the queue when they contend with lower-priority requests.\" The mechanism makes the prediction diagnostic.</p> </li> <li> <p>A diagnostic clause. Not \"if it fails, something is wrong\" but \"if this fails, priority gaps are insufficient to overcome queue depth at this load.\" The clause directs investigation when the prediction errors.</p> </li> </ol>"},{"location":"methodology/hypothesis-bundles/#common-mistakes","title":"Common mistakes","text":"Mistake Problem Fix Vague prediction \"Should improve TTFT\" Specify metric + direction + threshold: \"&gt;30% TTFT P99 improvement\" No mechanism \"Because it's a better strategy\" Explain the causal chain: \"because X causes Y which reduces Z\" No diagnostic \"If it fails, try something else\" Direct investigation: \"if it fails, it indicates X, investigate Y\" Prediction too conservative \"Will produce &gt;0% improvement\" Unfalsifiable \u2014 any positive change confirms. Use meaningful thresholds from first principles. Missing control Testing compound without isolating components Add H-ablation for each component and H-control-negative"},{"location":"methodology/principles/","title":"Discovered Principles","text":"<p>Draft \u2014 not in site navigation</p> <p>This page is intentionally excluded from the MkDocs navigation while the simulator stabilizes. The principles below were discovered in early Strategy Evolution experiments and may be revised as the simulator matures. Accessible by direct URL for reference.</p> <p>Principles extracted from 30 iterations of Strategy Evolution across two parallel experiment tracks: routing optimization (19 iterations) and scheduling optimization (11 iterations). Each principle is a concise, falsifiable statement grounded in experimental evidence.</p> <p>Simulation Context</p> <p>These principles were discovered within BLIS's discrete-event simulation model, which abstracts real inference serving systems. BLIS models vLLM-style recomputation-mode preemption (not swap-based), step-level batch formation, and queue-ordering scheduling. Principles involving preemption, chunked prefill overhead, or scheduling may not directly transfer to production systems with richer mechanisms (swap-based preemption, sub-step chunking, advanced memory management). Each principle's evidence column cites the specific experiment \u2014 consult the original findings for full context.</p>"},{"location":"methodology/principles/#routing-principles-from-19-iterations-1000-experiments","title":"Routing Principles (from 19 iterations, 1000+ experiments)","text":"# Principle Source Evidence RP-1 Orthogonal signals &gt; pre-combined signals \u2014 Independent PA+QD give the argmax more information than cost-benefit Iter 4 Cost-benefit scorer 29\u2013134% worse than <code>pa:3,qd:2,kv:2</code> across all rate points RP-2 Full N-way scan &gt; Power-of-2-Choices at moderate scale \u2014 At N\u226416 with cheap snapshot reads, seeing all instances finds better cache+load combinations. At large N, P2C's O(1) cost may dominate. Iter 1 HCAR (P2C) 16% worse than static-default; misses 6/8 instances RP-3 PA scorer self-corrects on cache miss \u2014 Returns 0 when no cache match exists, degenerating to load-only Iter 2 Dynamic weight switching produces byte-identical results to static-default RP-4 Uniform routing &gt; SLO-differentiated routing \u2014 Per-SLO profiles fragment cache affinity Iter 5 Adaptive+SLO priority 3\u20135% worse; fragments per-instance cache hit rate RP-5 Routing dominates scheduling at moderate load \u2014 Effective routing keeps queues short, leaving nothing for the scheduler to optimize Iter 3, 5 Priority scheduling had zero effect (byte-identical) when routing was effective RP-6 KV-utilization as a routing scorer is counterproductive under memory pressure \u2014 The <code>1-KVUtilization</code> formula routes away from instances with high occupancy, which are often the instances with the most valuable cached prefixes. Note: KV-utilization remains valuable as an admission/capacity signal; the finding is specific to its use as a routing preference scorer with approximate cache indexes. Iter 6, 8 Removing kv-util from routing scorer improved performance 4% AND made routing KV-pressure-immune (instance-level KV behavior unchanged) RP-7 The optimal strategy is regime-dependent \u2014 Normal KV: <code>pa:3,qd:2,kv:2</code>. Under pressure: <code>pa:3,qd:2</code>. At high load with admission: <code>pa:4,qd:3</code>. Iter 8, 10 Static default fails under KV pressure (23\u201325% worse than RR) <p>Reconciling with BLIS Defaults</p> <p>The current BLIS default is <code>prefix-affinity:3,queue-depth:2,kv-utilization:2</code> (llm-d parity). Strategy Evolution discovered that <code>pa:4,qd:3</code> (no kv-util) performs better under KV pressure and at high load with admission control. The default is maintained for compatibility with the llm-d ecosystem. Users running Strategy Evolution experiments should consider the regime-dependent recommendation in RP-7 rather than assuming the default is optimal for all scenarios.</p> # Principle Source Evidence RP-8 Approximate routing degrades under KV pressure \u2014 PrefixCacheIndex diverges from actual KV state Iter 6 Validated by llm-d blog's 57x finding on approximate vs precise routing RP-9 Admission control is the 3rd lever at high load \u2014 Neither routing nor scheduling can reduce total queue depth; admission can Iter 11 Compound strategy beats RR by 47% at rate=2000 (admission shedding 30%) RP-10 PA:QD ratio is the dominant parameter; empirical safety rule \u22641.33 for tested config \u2014 Disproportionate PA without QD causes cascade failure. The 1.33 threshold was measured at 8 instances, 2000 req/s, 2x overload; the safe ratio will vary with cluster size, arrival rate, and prefix group cardinality. Iter 13, 14 <code>pa:4,qd:2</code> (ratio 2.0) \u2192 3570ms cascade; <code>pa:4,qd:3</code> (ratio 1.33) \u2192 132ms optimal RP-11 Goodput &gt; P99 as primary optimization metric \u2014 Fair comparison when strategies have different completion rates Iter 14 GPT-4o review identified metric fairness issue RP-12 Staleness immunity comes from signal independence \u2014 PA reads synchronous PrefixCacheIndex, QD reads Immediate EffectiveLoad Iter 16 <code>pa:3,qd:2</code> produced identical 65.45ms across all staleness levels and KV pressures RP-13 Bursty arrivals amplify admission control benefit \u2014 Admission shedding during gamma bursts provides outsized relief Iter 18 Compound achieves 174ms (+65% vs RR's 496ms) under gamma CV=2.0 RP-14 Compound advantage scales inversely with cluster size \u2014 Smaller clusters see larger relative improvement Iter 19 N=4: +83.5%, N=8: +69.6%, N=16: +51.2%"},{"location":"methodology/principles/#scheduling-kv-cache-principles-from-11-iterations","title":"Scheduling &amp; KV Cache Principles (from 11 iterations)","text":"# Principle Source Evidence S1 Priority policy is the primary differentiator for orthogonal workloads \u2014 Routing controls WHICH instance; priority controls ORDER within each instance Iter 1 50.8% critical TTFT P99 improvement from priority alone S2 Signal quality &gt; mechanism complexity \u2014 KVUtilization is degenerate at abundant blocks; prefix-affinity is the only proven high-signal scorer Iter 1 Three independent reviewers rated priority more impactful than routing S3 Parameter count drives optimization difficulty \u2014 5\u20137 parameters is workable; 10+ risks local minima Iter 1-opt, 8-opt Bayesian search with 7 params converged in 30 calls; 10-param space did not S4 Super-additivity must be tested, not assumed \u2014 Pathological compounding (H24) does not guarantee the converse Iter 1 Ablation experiments essential for all compound strategies S5 Starvation must be quantified with concrete crossover times \u2014 \"The age weight will prevent it\" is not sufficient Iter 1 Piecewise-linear urgency with per-class thresholds; sheddable overtakes critical at ~1s S6 Scheduling is zero-sum at saturation; admission is not \u2014 In a work-conserving system at saturation, priority reordering that improves one class directly worsens another. Below saturation, this effect vanishes. Admission control is non-zero-sum: reducing total queue depth benefits all admitted requests. Iter 1\u20133 Cluster P99 degraded 62.4% despite 50.8% critical improvement (at near-saturation load) S7 Load-adaptive gap is irrelevant at sustained near-saturation \u2014 Load always exceeds threshold at 2000 req/s Iter 2 Load-regime adaptive strategy showed zero improvement over fixed-gap S8 Admission gating breaks the \"compute floor\" \u2014 Queue depth reduction benefits ALL tiers Iter 3 At t=20: critical TTFT 107ms, breaking through 132ms scheduling floor S9 Chunking hurts the chunked request but helps others \u2014 Disabling chunking for critical saves 14ms of beta0 overhead Iter 4, 5 Critical TTFT dropped from 132ms to 90ms with no throughput loss S10 Per-SLO prefill thresholds are a zero-cost lever \u2014 No throughput loss, no admission rejection Iter 4 Smarter allocation of prefill budget per SLO class S11 Strategy generalizes across all workload shapes \u2014 Non-orthogonal, asymmetric, multi-prefix all show 65\u201374% improvement Iter 7 Critical TTFT P99 always 88\u201396ms regardless of workload S12 Strategy is immune to KV pressure down to ~3000 blocks \u2014 Below that, critical degrades but still beats baseline by 50% Iter 7 No-chunk creates vulnerability under extreme KV pressure (large upfront allocation) S13 Multi-prefix workloads create implicit SLO-instance specialization \u2014 Prefix-affinity routes each SLO class to different instance clusters Iter 7 Critical gets \"fast lane\" instances; sheddable degradation worst (+84%) S14 Asymmetric workloads (80% sheddable) show minimal sheddable degradation \u2014 Few critical requests to jump ahead Iter 7 Only +5% sheddable degradation \u2014 best SLO tradeoff profile S15 SLO-aware KV preemption has no moderate regime in BLIS's recomputation model \u2014 Either zero preemptions (no effect) or livelock. This applies to recomputation-mode preemption (ProgressIndex reset to 0); real vLLM also supports swap-based preemption (KV blocks offloaded to CPU), which may exhibit a moderate regime that BLIS cannot observe. Iter 9 Negative finding: no preemption regime exists for shared-prefix workloads under recomputation preemption S16 No-chunk benefits all tiers on multi-turn workloads \u2014 Context accumulation amplifies prefill savings Iter 10, 11 Sheddable TTFT improved 49% on heavy multi-turn (5 rounds, 500ms think time)"},{"location":"methodology/principles/#how-principles-constrain-future-iterations","title":"How Principles Constrain Future Iterations","text":"<p>Principles function as hard constraints on subsequent iteration design:</p> <ul> <li>RP-1 (orthogonality) prevented building a combined cache-load scorer in iterations 5\u201319</li> <li>RP-6 (KV-util counterproductive) eliminated KV-utilization from all subsequent strategies</li> <li>S6 (scheduling is zero-sum) redirected effort from scheduler optimization to admission control</li> <li>RP-10 (PA:QD safety rule) prevented ratio violations in Bayesian search bounds</li> </ul> <p>When a new iteration proposes a mechanism that contradicts an existing principle, it must either:</p> <ol> <li>Provide experimental evidence that the principle doesn't hold in the new regime, or</li> <li>Redesign to work within the principle's constraints</li> </ol>"},{"location":"methodology/strategy-evolution/","title":"Strategy Evolution","text":"<p>A structured, iterative search methodology for discovering high-performing system configurations in complex, multi-dimensional policy spaces. The methodology structure is domain-agnostic; the specific instantiation described here was developed for LLM inference serving using BLIS.</p>"},{"location":"methodology/strategy-evolution/#overview","title":"Overview","text":"<p>In systems with multiple interacting policy layers \u2014 routing, scheduling, memory management, admission control \u2014 the optimal configuration cannot be derived analytically or guessed by experts. Interactions between layers produce non-obvious emergent behaviors: super-additive effects, signal cancellation, and regime-dependent dominance.</p> <p>Strategy Evolution discovers optimal configurations through disciplined experimentation: human-guided mechanism design combined with machine-guided parameter optimization, organized into iterative cycles with rigorous measurement and cumulative principle extraction.</p> <p>The central idea: a strategy is a hypothesis bundle. Every candidate mechanism is formulated as a set of testable predictions \u2014 a main hypothesis, ablation hypotheses, controls, and robustness checks \u2014 all designed before any code is written. Prediction errors, not just fitness scores, are the primary signal for learning.</p> <pre><code>flowchart TD\n    P1[\"Phase 1&lt;br/&gt;Problem Framing\"] --&gt; P2[\"Phase 2&lt;br/&gt;Hypothesis Bundle Design\"]\n    P2 --&gt; P3[\"Phase 3&lt;br/&gt;Implement &amp; Verify\"]\n    P3 --&gt; P4[\"Phase 4&lt;br/&gt;Bayesian Optimization\"]\n    P4 --&gt; P5[\"Phase 5&lt;br/&gt;Principles &amp; Iteration\"]\n    P5 --&gt;|Iterate| P2\n    P5 --&gt;|Converged| Done[\"Definitive Strategy&lt;br/&gt;+ Principles\"]\n\n    style P2 fill:#e1f5fe\n    style P3 fill:#e8f5e9\n    style P5 fill:#fff3e0</code></pre> <p>For detailed examples of hypothesis bundles drawn from PR #452 and PR #447, see Hypothesis Bundles in Practice.</p>"},{"location":"methodology/strategy-evolution/#phase-1-problem-framing","title":"Phase 1: Problem Framing","text":"<p>Write a precise problem statement (<code>problem.md</code>) that specifies:</p> <ul> <li>The baseline to beat \u2014 exact configuration parameters, measured across 3+ seeds</li> <li>The target workload \u2014 designed to prevent shortcutting (see below)</li> <li>Quantitative success criteria \u2014 e.g., \"&gt;15% TTFT P99 improvement, &gt;5% throughput improvement\"</li> <li>Hard constraints \u2014 must be implementable, defensible to domain experts, robust across seeds</li> <li>Prior knowledge inventory \u2014 experimental findings that narrow the design space</li> </ul> <p>Workload Design is Critical</p> <p>Design the workload to prevent strategies from gaming the metric. In our work, we used orthogonal SLO tiers \u2014 all tiers share identical request shapes, so strategies cannot use token-length as a proxy for priority. The SLO class metadata is the only differentiator.</p> <p>Artifacts produced: <code>problem.md</code>, <code>baseline.json</code></p>"},{"location":"methodology/strategy-evolution/#phase-2-hypothesis-bundle-design","title":"Phase 2: Hypothesis Bundle Design","text":"<p>Phase 2 is the heart of Strategy Evolution. Each iteration generates candidate strategies and formulates the winner as a hypothesis bundle \u2014 a set of testable predictions designed before any code is written.</p>"},{"location":"methodology/strategy-evolution/#step-2a-generate-and-review-candidates","title":"Step 2a: Generate and review candidates","text":"<p>Generate 2\u20133 candidate strategies. Each must be:</p> <ol> <li>A parameterized template \u2014 the mechanism defines what the strategy does; tunable parameters control how aggressively</li> <li>Self-critiqued \u2014 identify weaknesses before external review</li> <li>Reviewed by multiple independent judges \u2014 we used Claude Opus, GPT-4o, and Gemini 2.5 Flash</li> </ol> <pre><code>sequenceDiagram\n    participant A as Author\n    participant J1 as Judge 1 (Claude)\n    participant J2 as Judge 2 (GPT-4o)\n    participant J3 as Judge 3 (Gemini)\n\n    A-&gt;&gt;A: Generate 2-3 candidates + self-critique\n    par Multi-judge review\n        A-&gt;&gt;J1: Candidate designs\n        A-&gt;&gt;J2: Candidate designs\n        A-&gt;&gt;J3: Candidate designs\n        J1--&gt;&gt;A: Design flaws + improvements\n        J2--&gt;&gt;A: Design flaws + improvements\n        J3--&gt;&gt;A: Design flaws + improvements\n    end\n    A-&gt;&gt;A: Select winner by consensus</code></pre> <p>Multi-Judge Catches Real Design Flaws</p> <ul> <li>Claude Opus identified that a proposed cache score (<code>1 - KVUtilization</code>) was a free-capacity signal, not a cache-affinity signal \u2014 a subtle conflation that would have wasted an entire iteration</li> <li>GPT-4o caught a bang-bang oscillation problem in an online learning controller</li> <li>Gemini caught a numerical instability in hyperbolic starvation protection</li> </ul>"},{"location":"methodology/strategy-evolution/#step-2b-decompose-winner-into-hypothesis-bundle","title":"Step 2b: Decompose winner into hypothesis bundle","text":"<p>After selecting the winning strategy, decompose it into a hypothesis bundle \u2014 a set of testable, falsifiable predictions:</p> <pre><code>flowchart LR\n    S[\"Selected Strategy\"]\n\n    S --&gt; HM[\"H-main&lt;br/&gt;Mechanism claim +&lt;br/&gt;predicted effect size\"]\n    S --&gt; HA[\"H-ablation&lt;br/&gt;One per component:&lt;br/&gt;isolate contribution\"]\n    S --&gt; HSA[\"H-super-additivity&lt;br/&gt;Compound effect vs&lt;br/&gt;sum of parts\"]\n    S --&gt; HC[\"H-control-negative&lt;br/&gt;Condition where effect&lt;br/&gt;should vanish\"]\n    S --&gt; HR[\"H-robustness&lt;br/&gt;Generalization&lt;br/&gt;boundaries\"]\n\n    style HM fill:#c8e6c9\n    style HA fill:#ffecb3\n    style HSA fill:#ffecb3\n    style HC fill:#e1bee7\n    style HR fill:#bbdefb</code></pre> Arm What it tests Purpose H-main Mechanism's predicted effect + causal explanation Does the strategy work, and why? H-ablation-{component} Each component's individual contribution Which parts matter? Are any redundant? H-super-additivity Whether compound effect exceeds sum of parts Do components interact? H-control-negative Where the effect should vanish Confirms mechanism specificity H-robustness Generalization across workloads, resources, scale Where does the strategy break? <p>Each arm follows the hypothesis experiment workflow: experiment design standards (ED-1 through ED-6), convergence-gated review, formal controls, FINDINGS documentation. Every arm includes a diagnostic clause (\"if this fails, it indicates...\") that directs investigation when predictions don't match outcomes.</p> <p>Pre-commit ablation, don't bolt it on</p> <p>Ablation hypotheses are designed HERE, before any code is written. This prevents confirmation bias \u2014 you predict each component's contribution before seeing whether the compound strategy works. If you can't articulate what removing a component should do, you don't understand the mechanism well enough to implement it.</p> <p>For a complete worked example of a hypothesis bundle, see Hypothesis Bundles in Practice \u2014 Scheduling Example.</p>"},{"location":"methodology/strategy-evolution/#step-2c-design-review","title":"Step 2c: Design Review","text":"<p>Run the 5-perspective Design Review on the hypothesis bundle using the universal convergence protocol.</p> <pre><code>/convergence-review h-design\n</code></pre> <p>The review covers the full bundle \u2014 not just H-main, but all ablation, control, and robustness arms.</p>"},{"location":"methodology/strategy-evolution/#step-2d-human-approval-gate","title":"Step 2d: Human approval gate","text":"<p>Present the hypothesis bundle for human approval. The human reviews: H-main predictions, ablation predictions, control designs, and robustness boundaries.</p> <p>This is a hard gate. Do not proceed to implementation until the human approves.</p> <p>Artifacts produced: <code>research.md</code>, <code>iter&lt;N&gt;-bundle.yaml</code>, Design Review convergence log</p>"},{"location":"methodology/strategy-evolution/#phase-3-implement-and-verify","title":"Phase 3: Implement and Verify","text":"<p>Implement the strategy code AND the experiment code for the full hypothesis bundle, then execute, analyze, and review \u2014 all before moving to parameter optimization.</p> <pre><code>flowchart TD\n    A[\"3a: Implement strategy code\"]\n    B[\"3b: Implement experiment code\"]\n    C[\"3c: Code Review&lt;br/&gt;(5 perspectives)\"]\n    D[\"3d: Execute all arms&lt;br/&gt;(parallel, 3+ seeds)\"]\n    E[\"3e: Compare predictions&lt;br/&gt;to outcomes\"]\n    F[\"3f: Document FINDINGS.md\"]\n    G[\"3g: FINDINGS Review&lt;br/&gt;(10 perspectives)\"]\n    H[\"3h: Self-audit&lt;br/&gt;(6 dimensions)\"]\n    I[\"3i: Record in ledger\"]\n\n    A --&gt; B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G --&gt; H --&gt; I\n\n    style C fill:#fff3e0\n    style E fill:#e8f5e9\n    style G fill:#fff3e0</code></pre> <p>3a \u2014 Implement strategy. New policy code, CLI flags, or configuration.</p> <p>3b \u2014 Implement experiments. For each hypothesis arm, create <code>run.sh</code> (sources shared harness) and <code>analyze.py</code>. All arms share one <code>run.sh</code> \u2014 they are independent and can execute in parallel.</p> <p>3c \u2014 Code Review. 5-perspective review on all experiment code BEFORE running experiments (<code>/convergence-review h-code</code>).</p> <p>3d \u2014 Execute. Run all hypothesis arms across 3+ seeds.</p> <p>3e \u2014 Compare predictions to outcomes. For each arm, record:</p> <ul> <li>What was predicted (from the bundle design in Phase 2)</li> <li>What was observed (from the experiment)</li> <li>Whether the prediction was confirmed (direction correct AND exceeds threshold), partially confirmed (direction correct, magnitude falls short), or refuted (direction wrong or magnitude negligible &lt;5%)</li> <li>If refuted: what the discrepancy reveals about the causal model</li> </ul> <p>3f \u2014 Document FINDINGS. Write <code>FINDINGS.md</code> using the hypothesis template. The prediction-vs-outcome comparison is a required section.</p> <p>3g \u2014 FINDINGS Review. 10-perspective review using the convergence protocol (<code>/convergence-review h-findings</code>).</p> <p>3h \u2014 Self-audit. Six dimensions of critical self-review.</p> <p>3i \u2014 Record in ledger. One row per iteration, including prediction accuracy.</p> <p>The Ledger is the Single Source of Truth</p> <p>Never delete rows. Failed approaches are as valuable as successes. The ledger prevents revisiting failed approaches and makes the full exploration path auditable.</p> <p>Extended ledger format:</p> Iter Strategy TTFT P99 \u0394% Throughput \u0394% Key Mechanism Prediction Accuracy Status 0 Baseline \u2014 \u2014 FCFS + constant priority \u2014 Measured 1 SLO-Gated Priority -50.8% ~0% Priority cascade H-main confirmed; H-zero-sum refuted (zero-sum) Bundle verified <p>Artifacts produced: <code>ledger.md</code>, <code>iter&lt;N&gt;-FINDINGS.md</code>, convergence logs</p>"},{"location":"methodology/strategy-evolution/#phase-4-bayesian-parameter-optimization","title":"Phase 4: Bayesian Parameter Optimization","text":"<p>Once a mechanism's H-main is confirmed in Phase 3, optimize its parameters. Mechanisms whose H-main was refuted skip this phase \u2014 their prediction errors are analyzed for principles instead.</p> <pre><code>flowchart TD\n    A[\"Define parameter ranges&lt;br/&gt;in strategy YAML\"] --&gt; B[\"Bayesian optimizer&lt;br/&gt;selects next point\"]\n    B --&gt; C[\"Run simulator&lt;br/&gt;3 seeds x N params\"]\n    C --&gt; D{\"Constraint&lt;br/&gt;violation?\"}\n    D --&gt;|Yes| E[\"Add penalty&lt;br/&gt;to objective\"]\n    D --&gt;|No| F[\"Record metric\"]\n    E --&gt; G[\"Update GP&lt;br/&gt;surrogate model\"]\n    F --&gt; G\n    G --&gt;|Budget remaining| B\n    G --&gt;|Budget exhausted| H[\"Extract best&lt;br/&gt;parameters\"]</code></pre> <p>This separates mechanism design (human creativity + hypothesis testing) from parameter tuning (machine search). Every strategy gets the benefit of optimization, so comparisons are fair.</p> <p>Strategy YAML format:</p> <pre><code>name: \"slo-gated-priority-cascade\"\nparameters:\n  - name: \"base_critical\"\n    flag: \"--slo-base-critical\"\n    type: \"real\"\n    low: 5.0\n    high: 20.0\n  - name: \"age_weight\"\n    flag: \"--slo-age-weight\"\n    type: \"real\"\n    low: 0.000001\n    high: 0.0001\n    prior: \"log-uniform\"\nobjective:\n  metric: \"ttft_p99_ms\"\n  direction: \"min\"\nconstraints:\n  - metric: \"throughput_tps\"\n    direction: \"max\"\n    threshold: 15000\n    weight: 10\n</code></pre> <p>Budget: 30\u201350 evaluations \u00d7 3 seeds = 90\u2013150 simulator runs per strategy.</p> <p>Artifacts produced: <code>optimize.py</code>, <code>*-optimization-results.json</code></p>"},{"location":"methodology/strategy-evolution/#phase-5-principle-extraction-and-iteration","title":"Phase 5: Principle Extraction and Iteration","text":"<p>Extract principles from the iteration's results and decide whether to iterate.</p>"},{"location":"methodology/strategy-evolution/#principle-extraction","title":"Principle extraction","text":"<p>Distill findings into numbered principles \u2014 concise, falsifiable statements grounded in experimental evidence. Principles come from two sources:</p> <ol> <li>Confirmed predictions \u2014 the mechanism works as theorized (e.g., RP-1: \"Orthogonal signals &gt; pre-combined signals\")</li> <li>Prediction errors \u2014 the most valuable source. When a prediction fails, the discrepancy reveals something about the system that wasn't understood (e.g., S6: \"Scheduling is zero-sum at saturation\")</li> </ol> <pre><code>flowchart TD\n    subgraph \"Phase 3 outputs\"\n        C[\"Confirmed predictions\"]\n        R[\"Refuted predictions\"]\n    end\n\n    C --&gt; P1[\"New principle&lt;br/&gt;(mechanism verified)\"]\n    R --&gt; D[\"Discrepancy analysis\"]\n    D --&gt; P2[\"New principle&lt;br/&gt;(causal model corrected)\"]\n    D --&gt; RD[\"Redesign&lt;br/&gt;(next iteration)\"]\n\n    P1 --&gt; PC[\"Principles Catalog\"]\n    P2 --&gt; PC\n\n    PC --&gt;|Constrains| Next[\"Phase 2&lt;br/&gt;(next iteration)\"]\n\n    style R fill:#ffcdd2\n    style D fill:#fff3e0\n    style P2 fill:#c8e6c9</code></pre> <p>Principles function as hard constraints on subsequent iterations:</p> <ul> <li>RP-1 (orthogonality) prevented building a combined cache-load scorer in iterations 5\u201319</li> <li>RP-6 (KV-util counterproductive) eliminated KV-utilization from all subsequent strategies</li> <li>S6 (scheduling is zero-sum) redirected effort from scheduler optimization to admission control</li> <li>RP-10 (PA:QD safety rule) prevented ratio violations in Bayesian search bounds</li> </ul> <p>When a new iteration proposes a mechanism that contradicts an existing principle, it must either provide experimental evidence that the principle doesn't hold in the new regime, or redesign to work within the principle's constraints.</p>"},{"location":"methodology/strategy-evolution/#fast-fail-rules","title":"Fast-fail rules","text":"Condition Action H-main refuted H-main refutation still requires a minimum 5-perspective FINDINGS Review to confirm the refutation is genuine (not an analyzer bug). After verification, skip ablation arms. Record refutation + diagnostic. Iterate. H-main confirmed, single dominant component (&gt;80% of effect) Simplify strategy to that component. Iterate. H-control-negative fails (effect appears when it shouldn't) Mechanism is confounded. Redesign before continuing."},{"location":"methodology/strategy-evolution/#stopping-criterion","title":"Stopping criterion","text":"<p>Stop when multiple consecutive iterations produce null or marginal results \u2014 you have found the basin of the optimal strategy. The principles catalog is the durable output.</p>"},{"location":"methodology/strategy-evolution/#practical-considerations","title":"Practical Considerations","text":""},{"location":"methodology/strategy-evolution/#bundle-size-management","title":"Bundle size management","text":"<p>The number of hypothesis arms scales with strategy complexity (see the Bundle Size Guide for detailed breakdowns). Three mitigations keep this manageable:</p> <ol> <li> <p>Tiered review depth. H-main gets the full 10-perspective FINDINGS review. Ablation arms get a 5-perspective review. Byte-identical controls get a spot-check that the treatment was active, then automatic validation.</p> </li> <li> <p>Parallel execution. Hypothesis arms are independent. Use parallel execution mode. All arms must use the same seed set for valid comparison.</p> </li> <li> <p>Fast-fail. If H-main is refuted (after FINDINGS review confirms the refutation), skip remaining arms.</p> </li> </ol>"},{"location":"methodology/strategy-evolution/#when-to-use-the-full-bundle-vs-a-lighter-iteration","title":"When to use the full bundle vs. a lighter iteration","text":"Iteration type Required arms Optional arms New compound mechanism (\u22652 components) H-main, all H-ablation, H-super-additivity, H-control-negative H-robustness Component removal/simplification H-main, H-control-negative, H-ablation for removed component H-robustness Single-component mechanism H-main, H-control-negative H-robustness Parameter-only change (same active components, only numeric values change) H-main only Proceed directly to Bayesian optimization Robustness sweep (post-confirmation) H-robustness arms only \u2014"},{"location":"methodology/strategy-evolution/#how-strategy-evolution-connects-to-existing-workflows","title":"How Strategy Evolution connects to existing workflows","text":"Skill / Process Role in Strategy Evolution <code>/research-ideas</code> Generates candidate strategies (Step 2a) <code>/hypothesis-experiment</code> Guided Steps 0-10 experiment workflow (project skill) <code>/hypothesis-test</code> Experiment scaffolding: run.sh, analyze.py templates (sdlc-plugins) <code>/convergence-review</code> Gates Design Review, Code Review, FINDINGS Review <code>/brainstorming</code> Explores solution space before committing to candidates Hypothesis experiment process Defines the detailed per-arm workflow"},{"location":"methodology/strategy-evolution/#skills-and-tools-inventory","title":"Skills and Tools Inventory","text":"Skill Phase Purpose <code>/research-ideas</code> 2a Structured idea generation with iterative external LLM review <code>/brainstorming</code> 2a Explore solution space before committing to an approach <code>/review-plan</code> 2a Send candidate designs to external LLMs for technical review <code>/convergence-review</code> (h-design) 2c 5-perspective design review on hypothesis bundle <code>/hypothesis-experiment</code> 3b Guided Steps 0-10 experiment workflow (project skill) <code>/hypothesis-test</code> 3b Experiment scaffolding: run.sh, analyze.py templates (sdlc-plugins) <code>/test-driven-development</code> 3a TDD for new policy implementations <code>/convergence-review</code> (h-code) 3c 5-perspective code review on experiment code <code>/convergence-review</code> (h-findings) 3g 10-perspective findings review <code>/verification-before-completion</code> 3h Confirm results before claiming success <code>/dispatching-parallel-agents</code> 3d Parallel hypothesis arm execution <code>/commit-push-pr</code> 5 Clean git integration after validation <p>Where to Get These Skills</p> <p>These skills are Claude Code plugins. To install them:</p> <ul> <li><code>/brainstorming</code>, <code>/test-driven-development</code>, <code>/verification-before-completion</code>, <code>/dispatching-parallel-agents</code>, <code>/commit-push-pr</code>: Install the superpowers plugin \u2014 <code>claude plugins add superpowers</code></li> <li><code>/convergence-review</code>, <code>/hypothesis-experiment</code>: Project-local skills defined in this repository's <code>.claude/skills/</code> directory. Available automatically when Claude Code is run from the repo root.</li> <li><code>/hypothesis-test</code>: Install the sdlc-plugins plugin \u2014 <code>claude plugins add sdlc-plugins</code></li> <li><code>/research-ideas</code>, <code>/review-plan</code>: Install the research-ideas plugin \u2014 <code>claude plugins add research-ideas</code></li> </ul> <p>Non-skill tools:</p> Tool Phase Purpose <code>optimize.py</code> + scikit-optimize 4 Bayesian optimization harness (<code>gp_minimize</code>) <code>strategy_template.yaml</code> 4 Parameterized strategy configuration format <code>ledger.md</code> All Single source of truth (1 row per iteration, with prediction accuracy) Git worktrees All Isolation per experiment track"},{"location":"methodology/strategy-evolution/#applying-to-a-new-problem","title":"Applying to a New Problem","text":"<ol> <li> <p>Write <code>problem.md</code> \u2014 baseline, workload, success criteria, constraints, prior knowledge. Design the workload to prevent shortcutting.</p> </li> <li> <p>Build the measurement harness \u2014 deterministic simulator or benchmark that accepts parameterized configuration, produces machine-parseable metrics, and runs fast enough for 100\u2013200 evaluations. For noisy real-system benchmarks, increase seed count and evaluation budget proportionally.</p> </li> <li> <p>Start the ledger \u2014 <code>ledger.md</code> with baseline row. One row per iteration with a prediction accuracy column. Never delete rows.</p> </li> <li> <p>Run the loop \u2014 for each iteration:</p> <ul> <li>Generate 2-3 candidates with multi-judge review (Phase 2a)</li> <li>Decompose winner into a hypothesis bundle with predictions, ablation, controls, and robustness checks (Phase 2b)</li> <li>Convergence-gated Design Review + human approval (Phase 2c-2d)</li> <li>Implement strategy + experiments, Code Review, execute all arms, compare predictions to outcomes, FINDINGS Review (Phase 3)</li> <li>Bayesian optimization for confirmed mechanisms (Phase 4)</li> <li>Extract principles from both confirmed predictions and prediction errors (Phase 5)</li> </ul> </li> <li> <p>Know when to stop \u2014 when multiple consecutive iterations produce null or marginal results, you have found the basin of the optimal strategy. The principles catalog is the durable output.</p> </li> </ol>"},{"location":"methodology/strategy-evolution/#results-how-two-tracks-converged","title":"Results: How Two Tracks Converged","text":"<p>Strategy Evolution was applied in parallel on two complementary problem spaces:</p> Dimension Scheduling Track (11 iters) Routing Track (19 iters) Primary lever Priority ordering + admission control Scorer weights + signal selection Best result 73.7% critical TTFT improvement 65% combined improvement (bursty) Key discovery Priority is zero-sum; admission is non-zero-sum KV-utilization scorer is counterproductive Winning strategy SLO-tiered priority + no-chunk prefill <code>pa:4,qd:3</code> + SLO-gated admission <p>Both tracks converged on SLO-gated admission control as the breakthrough \"third lever\" and on <code>prefix-affinity</code> + <code>queue-depth</code> as the optimal signal pair.</p> <pre><code>gantt\n    title Two-Track Convergence Timeline\n    dateFormat X\n    axisFormat %s\n    section Scheduling Track\n    Baseline + SLO priority     :s1, 0, 2\n    Bayesian optimization       :s2, 2, 3\n    Admission control discovery :crit, s3, 3, 5\n    Prefill threshold tuning    :s4, 5, 7\n    Robustness + multi-turn     :s5, 7, 11\n    section Routing Track\n    P2C + dynamic weights       :r1, 0, 3\n    Signal orthogonality        :r2, 3, 5\n    KV-util counterproductive   :crit, r3, 5, 8\n    Compound + admission        :crit, r4, 8, 14\n    Bayesian + scaling sweep    :r5, 14, 19\n    section Convergence\n    Both discover admission ctrl :milestone, 10, 10\n    Both drop kv-utilization     :milestone, 12, 12</code></pre> <p>Experimental Configurations</p> <p>The winning strategies described above were discovered during Strategy Evolution experiments using custom configurations. Some components (SLO-gated admission, SLO-tiered priority as compound strategies) are not yet available as standard BLIS policy templates. The current BLIS default (<code>pa:3,qd:2,kv:2</code>) is maintained for llm-d parity. The regime-dependent recommendation (normal KV: <code>pa:3,qd:2,kv:2</code>; under pressure: <code>pa:3,qd:2</code>; high load with admission: <code>pa:4,qd:3</code>) will be documented in the principles catalog (currently in draft).</p>"},{"location":"reference/","title":"Reference","text":"<p>Lookup documentation for BLIS configuration, supported models, and workload specification schemas.</p> Page Description Configuration All CLI flags, configuration precedence, defaults.yaml behavior Supported Models Pre-trained models with blackbox coefficients, roofline-compatible models Workload Spec Schema Complete YAML schema for multi-client workload specifications"},{"location":"reference/configuration/","title":"Configuration Reference","text":"<p>This page documents all CLI flags, configuration files, and their interactions. For architectural context on what these settings control, see Cluster Architecture and Core Engine.</p>"},{"location":"reference/configuration/#configuration-precedence","title":"Configuration Precedence","text":"<p>BLIS uses a layered configuration system where more specific sources override more general ones:</p> <pre><code>CLI flags (highest priority \u2014 explicit user input)\n    \u2193 overrides\nYAML files (policy-config, workload-spec, defaults.yaml)\n    \u2193 overrides\nHardcoded defaults (lowest priority)\n</code></pre> <p>CLI flags only override YAML values when explicitly set. BLIS checks whether each flag was provided by the user (not just whether it has a non-default value), so default flag values do not accidentally override YAML configuration.</p>"},{"location":"reference/configuration/#simulation-control","title":"Simulation Control","text":"<p>Top-level settings that control the simulation run.</p> Flag Type Default Description <code>--seed</code> int64 42 Random seed for deterministic simulation. Same seed produces byte-identical stdout. <code>--horizon</code> int64 MaxInt64 Simulation time limit in ticks (microseconds). Simulation stops when clock exceeds horizon or all requests complete. <code>--log</code> string \"warn\" Log verbosity: trace, debug, info, warn, error, fatal, panic. Logs go to stderr. <code>--results-path</code> string \"\" File path to save per-request results JSON. Empty = stdout only."},{"location":"reference/configuration/#kv-cache-configuration","title":"KV Cache Configuration","text":"<p>Controls GPU and CPU memory simulation for key-value cache blocks. Maps to <code>KVCacheConfig</code>.</p> Flag Type Default Description <code>--total-kv-blocks</code> int64 1000000* Total GPU-tier KV blocks. <code>--block-size-in-tokens</code> int64 16 Tokens per KV block. <code>--kv-cpu-blocks</code> int64 0 CPU-tier blocks. 0 disables tiered caching. <code>--kv-offload-threshold</code> float64 0.9 GPU utilization fraction above which blocks are offloaded to CPU. Range [0, 1]. <code>--kv-transfer-bandwidth</code> float64 100.0 GPU-CPU transfer rate in blocks/tick. Required &gt; 0 when CPU blocks &gt; 0. <code>--kv-transfer-base-latency</code> int64 0 Fixed per-transfer latency in ticks. <p>* The CLI default is 1,000,000 but <code>defaults.yaml</code> overrides this per model when coefficients are loaded. For example, <code>llama-3.1-8b/H100/TP=2</code> uses 132,139 blocks. The override only applies if the user did not explicitly set <code>--total-kv-blocks</code>.</p>"},{"location":"reference/configuration/#batch-formation","title":"Batch Formation","text":"<p>Controls how requests are selected for the running batch. Maps to <code>BatchConfig</code>.</p> Flag Type Default Description <code>--max-num-running-reqs</code> int64 256 Maximum requests in the running batch simultaneously. <code>--max-num-scheduled-tokens</code> int64 2048 Maximum total new tokens across all running requests per step (token budget). <code>--long-prefill-token-threshold</code> int64 0 Prefill length threshold for chunked prefill. 0 = disabled (all prefill in one step)."},{"location":"reference/configuration/#latency-model","title":"Latency Model","text":""},{"location":"reference/configuration/#regression-coefficients","title":"Regression Coefficients","text":"<p>Trained coefficients for the blackbox latency model. Maps to <code>LatencyCoeffs</code>.</p> Flag Type Default Description <code>--alpha-coeffs</code> float64 slice [0, 0, 0] Alpha coefficients [alpha0, alpha1, alpha2]. Models non-GPU overhead. <code>--beta-coeffs</code> float64 slice [0, 0, 0] Beta coefficients [beta0, beta1, beta2]. Models GPU step time. <p>When both alpha and beta coefficients are all zeros, BLIS automatically loads pre-trained coefficients from <code>defaults.yaml</code> based on the model, GPU, and TP configuration.</p>"},{"location":"reference/configuration/#model-and-hardware-selection","title":"Model and Hardware Selection","text":"<p>Maps to <code>ModelHardwareConfig</code>.</p> Flag Type Default Description <code>--model</code> string (required) LLM model name (e.g., <code>meta-llama/llama-3.1-8b-instruct</code>). <code>--hardware</code> string \"\" GPU type (e.g., <code>H100</code>, <code>A100</code>). If empty, loaded from <code>defaults.yaml</code>. <code>--tp</code> int 0 Tensor parallelism degree. If 0, loaded from <code>defaults.yaml</code>. <code>--vllm-version</code> string \"\" vLLM version string. If empty, loaded from <code>defaults.yaml</code>."},{"location":"reference/configuration/#roofline-mode","title":"Roofline Mode","text":"<p>For analytical step time estimation without trained coefficients.</p> Flag Type Default Description <code>--roofline</code> bool false Enable roofline mode with auto-fetch. Requires <code>--hardware</code> and <code>--tp</code>. Auto-resolves model config from <code>model_configs/</code> or HuggingFace, and hardware config from bundled <code>hardware_config.json</code>. Set <code>HF_TOKEN</code> env var for gated models. <code>--model-config-folder</code> string \"\" Path to folder containing HuggingFace <code>config.json</code>. Overrides <code>--roofline</code> auto-resolution. <code>--hardware-config</code> string \"\" Path to <code>hardware_config.json</code> with GPU specifications. Overrides <code>--roofline</code> auto-resolution. <p>See Roofline Estimation for details on the analytical model.</p>"},{"location":"reference/configuration/#latency-mode-selection","title":"Latency Mode Selection","text":"<p>The latency model mode is selected based on available configuration:</p> <ol> <li>Blackbox mode (default): If coefficients are provided via CLI flags or loaded from <code>defaults.yaml</code></li> <li>Explicit roofline mode: If <code>--roofline</code> is set with <code>--hardware</code> and <code>--tp</code>. Model config is auto-resolved: <code>model_configs/</code> (local) \u2192 HuggingFace fetch \u2192 error. Alpha coefficients and <code>total_kv_blocks</code> are loaded from <code>defaults.yaml</code> when available. Beta coefficients are replaced by analytical roofline computation.</li> <li>Implicit roofline mode: If all coefficients are zero and all four of <code>--model-config-folder</code>, <code>--hardware-config</code>, <code>--hardware</code>, and <code>--tp</code> are provided</li> <li>Error: If no coefficients can be resolved and roofline inputs are incomplete</li> </ol>"},{"location":"reference/configuration/#cluster-configuration","title":"Cluster Configuration","text":"<p>With <code>--num-instances 1</code> (the default), BLIS runs a single-instance simulation \u2014 requests go directly to the wait queue with no admission or routing layer. With <code>--num-instances N</code> (N &gt; 1), the cluster simulation activates: requests pass through the admission and routing pipeline before reaching per-instance wait queues. See Cluster Architecture for the multi-instance pipeline and Core Engine for single-instance internals.</p> Flag Type Default Description <code>--num-instances</code> int 1 Number of inference instances. 1 = single-instance mode; &gt; 1 = cluster mode with admission and routing."},{"location":"reference/configuration/#admission-policy","title":"Admission Policy","text":"<p>Controls which requests enter the routing pipeline. See Cluster Architecture: Admission.</p> Flag Type Default Description <code>--admission-policy</code> string \"always-admit\" Policy name: <code>always-admit</code>, <code>token-bucket</code>, <code>reject-all</code>. <code>--admission-latency</code> int64 0 Admission decision latency in microseconds. <code>--token-bucket-capacity</code> float64 10000 Token bucket maximum capacity. Required &gt; 0 when using <code>token-bucket</code>. <code>--token-bucket-refill-rate</code> float64 1000 Token bucket refill rate in tokens/second. Required &gt; 0 when using <code>token-bucket</code>."},{"location":"reference/configuration/#routing-policy","title":"Routing Policy","text":"<p>Controls how admitted requests are assigned to instances. See Cluster Architecture: Routing.</p> Flag Type Default Description <code>--routing-policy</code> string \"round-robin\" Policy name: <code>round-robin</code>, <code>least-loaded</code>, <code>weighted</code>, <code>always-busiest</code>. <code>--routing-latency</code> int64 0 Routing decision latency in microseconds. <code>--routing-scorers</code> string \"\" Scorer configuration for <code>weighted</code> policy. Format: <code>name:weight,name:weight,...</code> <code>--snapshot-refresh-interval</code> int64 0 Prometheus snapshot refresh interval for all instance metrics (QueueDepth, BatchSize, KVUtilization) in microseconds. 0 = immediate."},{"location":"reference/configuration/#scorer-configuration","title":"Scorer Configuration","text":"<p>When using <code>--routing-policy weighted</code>, the <code>--routing-scorers</code> flag configures which scorers are used and their relative weights:</p> <pre><code>--routing-scorers \"prefix-affinity:3,queue-depth:2,kv-utilization:2\"\n</code></pre> <p>Available scorers: <code>prefix-affinity</code>, <code>queue-depth</code>, <code>kv-utilization</code>, <code>load-balance</code>.</p> <p>Default (when <code>--routing-scorers</code> is empty): <code>prefix-affinity:3, queue-depth:2, kv-utilization:2</code> (llm-d parity).</p> <p>See Cluster Architecture: Scorer Composition for details on each scorer.</p>"},{"location":"reference/configuration/#scheduling-and-priority","title":"Scheduling and Priority","text":"<p>Per-instance policies that control request ordering within the wait queue. Maps to <code>PolicyConfig</code>.</p> Flag Type Default Description <code>--scheduler</code> string \"fcfs\" Scheduler: <code>fcfs</code>, <code>priority-fcfs</code>, <code>sjf</code>, <code>reverse-priority</code>. <code>--priority-policy</code> string \"constant\" Priority policy: <code>constant</code>, <code>slo-based</code>, <code>inverted-slo</code>. <p>See Core Engine: Scheduling for policy details.</p>"},{"location":"reference/configuration/#workload-configuration","title":"Workload Configuration","text":""},{"location":"reference/configuration/#workload-modes","title":"Workload Modes","text":"<p>BLIS supports four workload specification modes, in order of precedence:</p> Mode Trigger Description Workload-spec YAML <code>--workload-spec &lt;path&gt;</code> Multi-client workload with per-client distributions. Highest priority. CLI distribution <code>--workload distribution</code> (default) Single-client Gaussian distribution controlled by CLI flags. Preset <code>--workload &lt;name&gt;</code> Named preset from <code>defaults.yaml</code>: <code>chatbot</code>, <code>contentgen</code>, <code>summarization</code>, <code>multidoc</code>. CSV traces <code>--workload traces</code> Replay recorded traces from a CSV file."},{"location":"reference/configuration/#distribution-mode-flags","title":"Distribution Mode Flags","text":"<p>Used when <code>--workload distribution</code> (the default) and no <code>--workload-spec</code> is set.</p> Flag Type Default Description <code>--rate</code> float64 1.0 Request arrival rate in requests/second. <code>--num-requests</code> int 100 Total number of requests to generate. <code>--prompt-tokens</code> int 512 Mean prompt (input) token count. <code>--prompt-tokens-stdev</code> int 256 Standard deviation of prompt tokens. <code>--prompt-tokens-min</code> int 2 Minimum prompt token count. <code>--prompt-tokens-max</code> int 7000 Maximum prompt token count. <code>--output-tokens</code> int 512 Mean output token count. <code>--output-tokens-stdev</code> int 256 Standard deviation of output tokens. <code>--output-tokens-min</code> int 2 Minimum output token count. <code>--output-tokens-max</code> int 7000 Maximum output token count. <code>--prefix-tokens</code> int 0 Prefix token count for prefix caching simulation. Additive to prompt tokens."},{"location":"reference/configuration/#workload-spec-yaml","title":"Workload-Spec YAML","text":"<p>The <code>--workload-spec</code> flag loads a YAML file defining multi-client workloads:</p> <pre><code>aggregate_rate: 100       # Total arrival rate in requests/second\nnum_requests: 1000\nseed: 42\nhorizon: 1000000000       # Ticks (microseconds)\n\nclients:\n  - id: \"interactive\"\n    rate_fraction: 0.6    # 60% of aggregate rate\n    prefix_group: \"chat\"\n    prefix_length: 512\n    arrival:\n      process: \"poisson\"\n    input_distribution:\n      type: \"gaussian\"\n      params:\n        mean: 256\n        std_dev: 128\n        min: 2\n        max: 4096\n    output_distribution:\n      type: \"exponential\"\n      params:\n        mean: 128\n\n  - id: \"batch\"\n    rate_fraction: 0.4\n    arrival:\n      process: \"gamma\"\n      cv: 2.0\n    input_distribution:\n      type: \"gaussian\"\n      params:\n        mean: 1024\n        std_dev: 512\n        min: 2\n        max: 7000\n    output_distribution:\n      type: \"gaussian\"\n      params:\n        mean: 512\n        std_dev: 256\n        min: 2\n        max: 7000\n</code></pre> <p>Supported arrival processes: <code>poisson</code>, <code>gamma</code> (with <code>cv</code> parameter), <code>weibull</code> (with <code>cv</code> parameter), <code>constant</code>.</p> <p>Supported token distributions: <code>gaussian</code>, <code>exponential</code>, <code>pareto_lognormal</code>, <code>constant</code>, <code>empirical</code>.</p> <p>When <code>--workload-spec</code> is set, CLI <code>--seed</code>, <code>--horizon</code>, and <code>--num-requests</code> still override the YAML values if explicitly provided.</p>"},{"location":"reference/configuration/#trace-files","title":"Trace Files","text":"Flag Type Default Description <code>--workload-spec</code> string \"\" Path to workload-spec YAML. <code>--workload-traces-filepath</code> string \"\" Path to CSV trace file (required when <code>--workload traces</code>). <code>--defaults-filepath</code> string \"defaults.yaml\" Path to <code>defaults.yaml</code>."},{"location":"reference/configuration/#policy-bundle","title":"Policy Bundle","text":"<p>The <code>--policy-config</code> flag loads admission, routing, priority, and scheduling configuration from a single YAML file:</p> <pre><code>admission:\n  policy: \"always-admit\"\n  token_bucket_capacity: 10000.0\n  token_bucket_refill_rate: 1000.0\n\nrouting:\n  policy: \"weighted\"\n  scorers:\n    - name: \"prefix-affinity\"\n      weight: 3.0\n    - name: \"queue-depth\"\n      weight: 2.0\n    - name: \"kv-utilization\"\n      weight: 2.0\n\npriority:\n  policy: \"constant\"\n\nscheduler: \"fcfs\"\n</code></pre> <p>CLI flags override policy bundle values when explicitly set. For example, <code>--routing-policy least-loaded</code> overrides the bundle's <code>routing.policy</code> setting.</p>"},{"location":"reference/configuration/#decision-tracing","title":"Decision Tracing","text":"Flag Type Default Description <code>--trace-level</code> string \"none\" Trace verbosity: <code>none</code> or <code>decisions</code>. <code>--counterfactual-k</code> int 0 Number of counterfactual candidates per routing decision. Requires <code>--trace-level decisions</code>. <code>--summarize-trace</code> bool false Print trace summary after simulation. Requires <code>--trace-level decisions</code>. <p>See Cluster Architecture: Counterfactual Regret.</p>"},{"location":"reference/configuration/#fitness-evaluation","title":"Fitness Evaluation","text":"Flag Type Default Description <code>--fitness-weights</code> string \"\" Fitness function weights. Format: <code>metric:weight,metric:weight,...</code> <p>When configured, BLIS computes a single fitness score from aggregated metrics. Latency metrics are normalized via <code>1/(1 + value/1000)</code> where <code>value</code> is in ticks (microseconds) and 1000 = 1ms reference (lower is better); throughput metrics via <code>value/(value + reference)</code> where <code>referenceRPS = 100.0</code> and <code>referenceTPS = 10000.0</code> (higher is better). Useful for automated policy comparison across multiple simulation runs.</p>"},{"location":"reference/configuration/#defaultsyaml","title":"defaults.yaml","text":"<p>The <code>defaults.yaml</code> file serves as a model registry and workload preset store:</p> <pre><code># Section 1: Hardware/TP mappings (keyed by model ID)\ndefaults:\n  meta-llama/llama-3.1-8b-instruct:\n    GPU: H100\n    tensor_parallelism: 2\n    vllm_version: vllm/vllm-openai:v0.8.4\n    hf_repo: meta-llama/Llama-3.1-8B-Instruct\n\n# Section 2: Workload presets\nworkloads:\n  chatbot:\n    prompt_tokens: 256\n    prompt_tokens_stdev: 100\n    output_tokens: 256\n    output_tokens_stdev: 100\n    # ... min/max bounds\n\n# Section 3: Trained coefficients (keyed by model+GPU+TP)\nmodels:\n  - id: meta-llama/llama-3.1-8b-instruct\n    GPU: H100\n    tensor_parallelism: 2\n    vllm_version: vllm/vllm-openai:v0.8.4\n    alpha_coeffs: [1601.35, 3.51, 1805.54]\n    beta_coeffs: [6910.42, 17.67, 2.84]\n    total_kv_blocks: 132139\n</code></pre>"},{"location":"reference/configuration/#resolution-process","title":"Resolution Process","text":"<p>When BLIS starts:</p> <ol> <li>If <code>--roofline</code> is set:</li> <li>Auto-resolve model config: check <code>model_configs/</code> for existing <code>config.json</code>, fetch from HuggingFace on miss (set <code>HF_TOKEN</code> for gated models)</li> <li>Auto-resolve hardware config from bundled <code>hardware_config.json</code></li> <li>Load alpha coefficients and <code>total_kv_blocks</code> from <code>defaults.yaml</code> (beta coefficients are replaced by roofline computation)</li> <li><code>--model-config-folder</code> and <code>--hardware-config</code> override auto-resolution when explicitly set</li> <li>If <code>--alpha-coeffs</code> and <code>--beta-coeffs</code> are both all-zero and no roofline config is provided:</li> <li>Look up the model in <code>defaults.yaml</code> using <code>--model</code>, <code>--hardware</code>, <code>--tp</code>, <code>--vllm-version</code></li> <li>Load alpha/beta coefficients and <code>total_kv_blocks</code> from the matching entry</li> <li>Override <code>--total-kv-blocks</code> only if the user did not explicitly set it</li> <li>If coefficients are still all-zero but <code>--model-config-folder</code> and <code>--hardware-config</code> are provided:</li> <li>Enable roofline mode (implicit activation)</li> <li>If coefficients were explicitly provided via CLI:</li> <li>Use them directly, no <code>defaults.yaml</code> lookup</li> </ol>"},{"location":"reference/configuration/#coefficient-calibration","title":"Coefficient Calibration","text":"<p>BLIS uses a data-driven calibration strategy to ensure simulation accuracy. This process runs once per environment configuration (model, GPU, TP degree, vLLM version):</p> <ol> <li>Initialization: Define baseline estimates for alpha and beta coefficients as starting points for optimization</li> <li>Profiling: Execute training workloads on a live vLLM instance to collect ground-truth mean and P90 metrics for TTFT, ITL, and E2E</li> <li>Optimization: Run BLIS iteratively using Blackbox Bayesian Optimization to minimize the multi-objective loss:</li> </ol> <p>$$\\text{Loss} = \\sum_{m \\in {\\text{TTFT, ITL, E2E}}} \\left( |GT_{\\text{mean},m} - Sim_{\\text{mean},m}| + |GT_{\\text{p90},m} - Sim_{\\text{p90},m}| \\right)$$</p> <ol> <li>Artifact generation: Optimal alpha/beta coefficients are stored in <code>defaults.yaml</code> for production use</li> </ol> <p>For environments where live profiling is not feasible, the Roofline model provides analytical step time estimation without any training data.</p>"},{"location":"reference/configuration/#cli-flag-summary-by-sub-config","title":"CLI Flag Summary by Sub-Config","text":"Sub-Config Flags KVCacheConfig <code>--total-kv-blocks</code>, <code>--block-size-in-tokens</code>, <code>--kv-cpu-blocks</code>, <code>--kv-offload-threshold</code>, <code>--kv-transfer-bandwidth</code>, <code>--kv-transfer-base-latency</code> BatchConfig <code>--max-num-running-reqs</code>, <code>--max-num-scheduled-tokens</code>, <code>--long-prefill-token-threshold</code> LatencyCoeffs <code>--alpha-coeffs</code>, <code>--beta-coeffs</code> ModelHardwareConfig <code>--model</code>, <code>--hardware</code>, <code>--tp</code>, <code>--vllm-version</code>, <code>--roofline</code>, <code>--model-config-folder</code>, <code>--hardware-config</code> PolicyConfig <code>--scheduler</code>, <code>--priority-policy</code> WorkloadConfig <code>--workload</code>, <code>--workload-spec</code>, <code>--workload-traces-filepath</code>, <code>--defaults-filepath</code>, <code>--rate</code>, <code>--num-requests</code>, <code>--prompt-tokens*</code>, <code>--output-tokens*</code>, <code>--prefix-tokens</code> DeploymentConfig <code>--num-instances</code>, <code>--admission-policy</code>, <code>--admission-latency</code>, <code>--token-bucket-capacity</code>, <code>--token-bucket-refill-rate</code>, <code>--routing-policy</code>, <code>--routing-latency</code>, <code>--routing-scorers</code>, <code>--snapshot-refresh-interval</code>, <code>--trace-level</code>, <code>--counterfactual-k</code> Top-level <code>--seed</code>, <code>--horizon</code>, <code>--log</code>, <code>--results-path</code>, <code>--policy-config</code>, <code>--fitness-weights</code>, <code>--summarize-trace</code>"},{"location":"reference/models/","title":"Supported Models","text":"<p>All models below have pre-trained alpha/beta coefficients in <code>defaults.yaml</code> for blackbox mode. Models with a HuggingFace <code>config.json</code> in <code>model_configs/</code> additionally support roofline mode.</p>"},{"location":"reference/models/#dense-models","title":"Dense Models","text":"Model Sizes Meta LLaMA 3.1 8B Meta LLaMA 3.3 70B IBM Granite 3.1 8B CodeLlama 34B Microsoft Phi-4 14B Mistral Small (2501) 24B Mistral Small 3.1 (2503) 24B NVIDIA LLaMA 3.1 Nemotron 70B OpenAI GPT-OSS 20B, 120B Qwen 2.5 7B"},{"location":"reference/models/#moe-models","title":"MoE Models","text":"Model Architecture LLaMA 4 Maverick (FP8) 17B, 128 experts LLaMA 4 Scout 17B, 16 experts Mixtral 8x7B"},{"location":"reference/models/#quantized-variants","title":"Quantized Variants","text":"<p>Red Hat AI (<code>redhatai/</code>) provides FP8, W4A16, and W8A8 quantized variants for many of the above models, including LLaMA 3.1/3.3/4, Mistral Small 3.1, Phi-4, Qwen 2.5, and SmolLM3 3B (FP8 only). See <code>defaults.yaml</code> for the full list.</p>"},{"location":"reference/models/#roofline-only-models","title":"Roofline-Only Models","text":"<p>Any model with a HuggingFace <code>config.json</code> can use roofline mode via <code>--roofline</code> or <code>--model-config-folder</code>. The <code>--roofline</code> flag auto-fetches configs from HuggingFace on first use, caching them in <code>model_configs/</code>. Tested architectures include Qwen 2.5 1.5B/3B, Qwen 3 14B, and LLaMA 2 7B/70B.</p>"},{"location":"reference/models/#adding-a-new-model","title":"Adding a New Model","text":"<p>To add blackbox support for a new model:</p> <ol> <li>Calibrate alpha/beta coefficients using live vLLM profiling (see Configuration: Coefficient Calibration)</li> <li>Add the entry to <code>defaults.yaml</code></li> </ol> <p>To add roofline support:</p> <ol> <li>Download the model's <code>config.json</code> from HuggingFace</li> <li>Place in <code>model_configs/&lt;model-name&gt;/config.json</code></li> <li>Run with <code>--roofline --hardware &lt;GPU&gt; --tp &lt;N&gt;</code></li> </ol> <p>Or let BLIS auto-fetch it with <code>--roofline</code>.</p>"},{"location":"reference/workload-spec/","title":"Workload Spec Schema","text":"<p>Complete YAML schema reference for BLIS workload specifications (<code>--workload-spec</code>). For a guide-level introduction, see Workload Specifications.</p>"},{"location":"reference/workload-spec/#top-level-fields","title":"Top-Level Fields","text":"Field Type Required Description <code>version</code> string No Schema version (<code>\"2\"</code> recommended; <code>\"1\"</code> auto-upgraded) <code>seed</code> int64 No RNG seed (overridden by CLI <code>--seed</code> if set) <code>category</code> string No <code>language</code>, <code>multimodal</code>, <code>reasoning</code>, or empty <code>aggregate_rate</code> float64 Yes Total arrival rate in requests/second <code>num_requests</code> int64 No Total requests to generate (0 = unlimited, use horizon) <code>horizon</code> int64 No Simulation time limit in ticks (overridden by CLI <code>--horizon</code> if set) <code>clients</code> list Yes* Client specifications (see below) <code>cohorts</code> list No Cohort specifications with population dynamics (diurnal, spike, drain patterns) <code>servegen_data</code> object No Native ServeGen data file loading <code>inference_perf</code> object No inference-perf format compatibility <p>*At least one <code>client</code>, <code>cohort</code>, or <code>servegen_data</code> is required.</p>"},{"location":"reference/workload-spec/#client-specification","title":"Client Specification","text":"<p>Each entry in the <code>clients</code> list defines a traffic source:</p> Field Type Required Description <code>id</code> string No Client identifier (for metrics grouping) <code>tenant_id</code> string No Tenant identifier <code>slo_class</code> string No SLO tier: <code>critical</code>, <code>standard</code>, <code>sheddable</code>, <code>batch</code>, <code>background</code>, or empty <code>model</code> string No Model name override (for multi-model workloads) <code>rate_fraction</code> float64 Yes Fraction of <code>aggregate_rate</code> for this client (must be positive) <code>arrival</code> object Yes Arrival process configuration <code>input_distribution</code> object Yes Input token length distribution <code>output_distribution</code> object Yes Output token length distribution <code>prefix_group</code> string No Prefix group name (requests in same group share prefixes) <code>prefix_length</code> int No Shared prefix token count (additive to input_distribution) <code>streaming</code> bool No Whether to simulate streaming output <code>network</code> object No Client-side network characteristics <code>lifecycle</code> object No Activity window configuration <code>multimodal</code> object No Multimodal token generation <code>reasoning</code> object No Reasoning multi-turn behavior"},{"location":"reference/workload-spec/#arrival-process","title":"Arrival Process","text":"Field Type Values Description <code>process</code> string <code>poisson</code>, <code>gamma</code>, <code>weibull</code>, <code>constant</code> Inter-arrival time distribution <code>cv</code> *float64 Required for <code>gamma</code> and <code>weibull</code> Coefficient of variation (burstiness). CV &gt; 1 = bursty, CV &lt; 1 = regular"},{"location":"reference/workload-spec/#distribution-specification","title":"Distribution Specification","text":"<p>Used for <code>input_distribution</code> and <code>output_distribution</code>:</p> Field Type Description <code>type</code> string <code>gaussian</code>, <code>exponential</code>, <code>pareto_lognormal</code>, <code>constant</code>, <code>empirical</code> <code>params</code> map Type-specific parameters (see below) <code>file</code> string Reserved for future use (file-based loading not yet implemented). Use inline <code>params</code> instead."},{"location":"reference/workload-spec/#distribution-parameters","title":"Distribution Parameters","text":"Type Parameters <code>gaussian</code> <code>mean</code>, <code>std_dev</code>, <code>min</code>, <code>max</code> <code>exponential</code> <code>mean</code> <code>pareto_lognormal</code> <code>alpha</code>, <code>xm</code>, <code>mu</code>, <code>sigma</code>, <code>mix_weight</code> <code>constant</code> <code>value</code> <code>empirical</code> inline <code>params</code> map (key=token count, value=probability)"},{"location":"reference/workload-spec/#network-specification","title":"Network Specification","text":"Field Type Description <code>rtt_ms</code> float64 Round-trip time in milliseconds <code>bandwidth_mbps</code> float64 Bandwidth in Mbps"},{"location":"reference/workload-spec/#reasoning-specification","title":"Reasoning Specification","text":"Field Type Description <code>reason_ratio_distribution</code> DistSpec Distribution of reasoning-to-output ratio <code>multi_turn</code> object Multi-turn conversation configuration <code>multi_turn.max_rounds</code> int Maximum conversation rounds <code>multi_turn.think_time_us</code> int64 User think time between rounds (microseconds) <code>multi_turn.context_growth</code> string <code>accumulate</code> (prepend prior context)"},{"location":"reference/workload-spec/#cohort-specification","title":"Cohort Specification","text":"<p>Each entry in the <code>cohorts</code> list defines a population with lifecycle dynamics. Cohorts expand into individual clients with lifecycle windows derived from diurnal, spike, or drain patterns.</p> Field Type Required Description <code>id</code> string No Cohort identifier <code>population</code> int Yes Number of clients in this cohort (max 100,000) <code>tenant_id</code> string No Tenant identifier <code>slo_class</code> string No SLO tier: <code>critical</code>, <code>standard</code>, <code>sheddable</code>, <code>batch</code>, <code>background</code> <code>model</code> string No Model name override <code>arrival</code> object Yes Arrival process configuration (same as Client) <code>input_distribution</code> object Yes Input token length distribution <code>output_distribution</code> object Yes Output token length distribution <code>prefix_group</code> string No Prefix group name <code>streaming</code> bool No Whether to simulate streaming output <code>rate_fraction</code> float64 Yes Fraction of <code>aggregate_rate</code> for each client in this cohort <code>diurnal</code> object No Sinusoidal rate modulation (see below) <code>spike</code> object No Traffic spike configuration (see below) <code>drain</code> object No Linear ramp-down to zero (see below)"},{"location":"reference/workload-spec/#diurnal-pattern","title":"Diurnal Pattern","text":"Field Type Description <code>peak_hour</code> int Hour of peak traffic (0-23) <code>peak_to_trough_ratio</code> float64 Ratio of peak to trough rate (\u2265 1.0)"},{"location":"reference/workload-spec/#spike-pattern","title":"Spike Pattern","text":"Field Type Description <code>start_time_us</code> int64 Spike start time in microseconds <code>duration_us</code> int64 Spike duration in microseconds"},{"location":"reference/workload-spec/#drain-pattern","title":"Drain Pattern","text":"Field Type Description <code>start_time_us</code> int64 Drain start time in microseconds <code>ramp_duration_us</code> int64 Ramp-down duration in microseconds"},{"location":"reference/workload-spec/#lifecycle-specification","title":"Lifecycle Specification","text":"<p>Activity window configuration for clients (used in the <code>lifecycle</code> field of Client Specification). Cohort patterns (diurnal, spike, drain) are converted into lifecycle windows internally.</p> Field Type Description <code>windows</code> list List of active time windows"},{"location":"reference/workload-spec/#active-window","title":"Active Window","text":"Field Type Description <code>start_us</code> int64 Window start time in microseconds <code>end_us</code> int64 Window end time in microseconds"},{"location":"reference/workload-spec/#multimodal-specification","title":"Multimodal Specification","text":"<p>Configures multimodal request generation (used in the <code>multimodal</code> field of Client Specification). Each distribution follows the same Distribution Specification format.</p> Field Type Description <code>text_distribution</code> DistSpec Text token distribution <code>image_distribution</code> DistSpec Image token distribution <code>image_count_distribution</code> DistSpec Number of images per request <code>audio_distribution</code> DistSpec Audio token distribution <code>audio_count_distribution</code> DistSpec Number of audio segments per request <code>video_distribution</code> DistSpec Video token distribution <code>video_count_distribution</code> DistSpec Number of video segments per request"},{"location":"reference/workload-spec/#servegen-data-specification","title":"ServeGen Data Specification","text":"<p>Native ServeGen data file loading (used in the <code>servegen_data</code> top-level field):</p> Field Type Required Description <code>path</code> string Yes Path to ServeGen data directory (containing <code>chunk-*-trace.csv</code> and <code>dataset.json</code>) <code>span_start</code> int64 No Trace span start filter (microseconds) <code>span_end</code> int64 No Trace span end filter (microseconds)"},{"location":"reference/workload-spec/#inferenceperf-specification","title":"InferencePerf Specification","text":"<p>inference-perf format compatibility (used in the <code>inference_perf</code> top-level field):</p> Field Type Required Description <code>stages</code> list Yes Rate/duration stages for load patterns <code>shared_prefix</code> object Yes Shared prefix expansion configuration"},{"location":"reference/workload-spec/#stage","title":"Stage","text":"Field Type Description <code>rate</code> float64 Requests per second for this stage <code>duration</code> int64 Stage duration in seconds (note: unlike other time fields which use microseconds, this field uses seconds)"},{"location":"reference/workload-spec/#shared-prefix","title":"Shared Prefix","text":"Field Type Description <code>num_unique_system_prompts</code> int Number of unique system prompts <code>num_users_per_system_prompt</code> int Users per system prompt <code>system_prompt_len</code> int System prompt length in tokens <code>question_len</code> int Question length in tokens <code>output_len</code> int Output length in tokens <code>enable_multi_turn_chat</code> bool Enable multi-turn chat mode"},{"location":"reference/workload-spec/#complete-example","title":"Complete Example","text":"<pre><code>version: \"2\"\nseed: 42\ncategory: reasoning\naggregate_rate: 500.0\nnum_requests: 500\n\nclients:\n  - id: \"multi-turn-chat\"\n    tenant_id: \"chat-users\"\n    slo_class: \"standard\"\n    rate_fraction: 1.0\n    streaming: true\n    arrival:\n      process: poisson\n    input_distribution:\n      type: gaussian\n      params:\n        mean: 128\n        std_dev: 30\n        min: 32\n        max: 512\n    output_distribution:\n      type: gaussian\n      params:\n        mean: 64\n        std_dev: 20\n        min: 16\n        max: 256\n    reasoning:\n      reason_ratio_distribution:\n        type: gaussian\n        params:\n          mean: 0\n          std_dev: 0\n          min: 0\n          max: 0\n      multi_turn:\n        max_rounds: 5\n        think_time_us: 500000\n        context_growth: accumulate\n</code></pre>"},{"location":"reference/workload-spec/#validation","title":"Validation","text":"<p>BLIS validates workload specs with strict YAML parsing (<code>KnownFields(true)</code>) \u2014 typos in field names cause errors. Additional validation:</p> <ul> <li><code>aggregate_rate</code> must be positive</li> <li>Each client's <code>rate_fraction</code> must be positive</li> <li><code>arrival.process</code> must be one of the valid processes</li> <li><code>cv</code> for gamma/weibull must be finite and positive</li> <li>Weibull <code>cv</code> must be in [0.01, 10.4]</li> <li>Distribution types must be recognized</li> <li>All numeric params must be finite (no NaN or Inf)</li> <li>At least one <code>client</code>, <code>cohort</code>, or <code>servegen_data</code> is required</li> <li>Cohort <code>population</code> must be positive and \u2264 100,000</li> </ul>"}]}